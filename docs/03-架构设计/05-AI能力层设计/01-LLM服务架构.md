---
æ–‡æ¡£ç±»å‹: æ¶æ„æ–‡æ¡£
éœ€æ±‚ç¼–å·: DOC-2025-11-001
created_at: 2025-11-29
author: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
updated_at: 2025-11-30 00:25
updater: gemini-pro
status: å·²æ‰¹å‡†
---

# AIæ ‡ä¹¦æ™ºèƒ½åˆ›ä½œå¹³å° - LLMæœåŠ¡æ¶æ„

> **å®ç°**: Python FastAPIç‹¬ç«‹æœåŠ¡
> **ç«¯å£**: 8001
> **éƒ¨ç½²**: ç‹¬ç«‹äºJavaæœåŠ¡çš„Pythonå®¹å™¨

## ğŸ“‹ æ–‡æ¡£å¯¼èˆª

æœ¬æ–‡æ¡£æ˜¯AIèƒ½åŠ›å±‚è®¾è®¡çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä»–ç›¸å…³æ–‡æ¡£ï¼š

1. [00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md](./00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md) - AIèƒ½åŠ›çŸ©é˜µå’ŒæŠ€æœ¯æ ˆæ¦‚è§ˆ
2. **01-LLMæœåŠ¡æ¶æ„.md**ï¼ˆæœ¬æ–‡æ¡£ï¼‰- LLMå®¢æˆ·ç«¯ã€Promptç®¡ç†ã€ä»»åŠ¡é˜Ÿåˆ—
3. [02-å‘é‡æ£€ç´¢æœåŠ¡.md](./02-å‘é‡æ£€ç´¢æœåŠ¡.md) - å‘é‡åµŒå…¥æœåŠ¡ã€Elasticsearchã€Pineconeå­˜å‚¨
4. [03-çŸ¥è¯†å›¾è°±æœåŠ¡.md](./03-çŸ¥è¯†å›¾è°±æœåŠ¡.md) - çŸ¥è¯†å›¾è°±ç®¡ç†ã€æ™ºèƒ½åŒ¹é…å¼•æ“
5. [04-å·¥ä½œæµä¸ä¼˜åŒ–.md](./04-å·¥ä½œæµä¸ä¼˜åŒ–.md) - å·¥ä½œæµç¼–æ’ã€æˆæœ¬ä¼˜åŒ–ã€æ€§èƒ½ä¼˜åŒ–

## ğŸ¤– LLMæœåŠ¡æ¶æ„

### 1. LLMå®¢æˆ·ç«¯è®¾è®¡

```python
# app/services/ai/llm_client.py
# Python AIæœåŠ¡ - LLMå®¢æˆ·ç«¯
from typing import Optional, List, Dict, Any
from enum import Enum
import openai
import anthropic
from langchain.chat_models import ChatOpenAI, ChatAnthropic
from langchain.schema import SystemMessage, HumanMessage, AIMessage

class ModelProvider(str, Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    ZHIPU = "zhipu"
    BAIDU = "baidu"

class LLMClient:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯"""

    def __init__(self):
        self.providers = {
            ModelProvider.OPENAI: self._init_openai(),
            ModelProvider.ANTHROPIC: self._init_anthropic(),
            ModelProvider.ZHIPU: self._init_zhipu(),
        }

    def _init_openai(self) -> ChatOpenAI:
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        return ChatOpenAI(
            model_name="gpt-4-turbo-preview",
            temperature=0.7,
            openai_api_key=settings.OPENAI_API_KEY,
        )

    def _init_anthropic(self) -> ChatAnthropic:
        """åˆå§‹åŒ–Anthropicå®¢æˆ·ç«¯"""
        return ChatAnthropic(
            model="claude-3-opus-20240229",
            anthropic_api_key=settings.ANTHROPIC_API_KEY,
        )

    async def chat(
        self,
        messages: List[Dict[str, str]],
        provider: ModelProvider = ModelProvider.OPENAI,
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„èŠå¤©æ¥å£

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            provider: æ¨¡å‹æä¾›å•†
            model: å…·ä½“æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°

        Returns:
            ç”Ÿæˆç»“æœ
        """
        llm = self.providers[provider]

        if model:
            llm.model_name = model
        llm.temperature = temperature
        llm.max_tokens = max_tokens

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        formatted_messages = self._format_messages(messages)

        # è°ƒç”¨LLM
        response = await llm.agenerate([formatted_messages])

        return {
            "content": response.generations[0][0].text,
            "model": llm.model_name,
            "tokens_used": {
                "prompt": response.llm_output.get("token_usage", {}).get("prompt_tokens", 0),
                "completion": response.llm_output.get("token_usage", {}).get("completion_tokens", 0),
                "total": response.llm_output.get("token_usage", {}).get("total_tokens", 0),
            },
            "finish_reason": response.generations[0][0].generation_info.get("finish_reason"),
        }

    def _format_messages(self, messages: List[Dict[str, str]]) -> List:
        """æ ¼å¼åŒ–æ¶ˆæ¯"""
        formatted = []
        for msg in messages:
            role = msg["role"]
            content = msg["content"]

            if role == "system":
                formatted.append(SystemMessage(content=content))
            elif role == "user":
                formatted.append(HumanMessage(content=content))
            elif role == "assistant":
                formatted.append(AIMessage(content=content))

        return formatted
```

### 2. Promptç®¡ç†

```python
# app/services/ai/prompt_manager.py
from typing import Dict, Any, Optional
from jinja2 import Template
from app.models.ai_prompt import AIPrompt
from app.core.cache import cache

class PromptManager:
    """Promptæ¨¡æ¿ç®¡ç†å™¨"""

    def __init__(self):
        self.cache_ttl = 3600  # 1å°æ—¶ç¼“å­˜

    async def get_prompt(
        self,
        code: str,
        variables: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        è·å–å¹¶æ¸²æŸ“Prompt

        Args:
            code: Promptä»£ç 
            variables: å˜é‡å­—å…¸

        Returns:
            æ¸²æŸ“åçš„Prompt
        """
        # ä»ç¼“å­˜è·å–
        cache_key = f"prompt:{code}"
        prompt_template = await cache.get(cache_key)

        if not prompt_template:
            # ä»æ•°æ®åº“è·å–
            prompt = await AIPrompt.get_by_code(code)
            if not prompt:
                raise ValueError(f"Prompt not found: {code}")

            prompt_template = prompt.prompt_template
            # ç¼“å­˜
            await cache.set(cache_key, prompt_template, self.cache_ttl)

        # æ¸²æŸ“æ¨¡æ¿
        if variables:
            template = Template(prompt_template)
            return template.render(**variables)

        return prompt_template

    async def create_prompt(
        self,
        name: str,
        code: str,
        category: str,
        prompt_template: str,
        system_prompt: Optional[str] = None,
        variables: Optional[List[str]] = None,
        model_params: Optional[Dict[str, Any]] = None,
    ) -> AIPrompt:
        """åˆ›å»ºPromptæ¨¡æ¿"""
        prompt = await AIPrompt.create(
            name=name,
            code=code,
            category=category,
            prompt_template=prompt_template,
            system_prompt=system_prompt,
            variables=variables or [],
            model_params=model_params or {},
        )

        # æ¸…é™¤ç¼“å­˜
        await cache.delete(f"prompt:{code}")

        return prompt


# Promptç¤ºä¾‹
REQUIREMENT_ANALYSIS_PROMPT = """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ‹›æŠ•æ ‡ä¸“å®¶ï¼Œæ“…é•¿åˆ†ææ‹›æ ‡æ–‡ä»¶å¹¶æå–å…³é”®éœ€æ±‚ã€‚

è¯·åˆ†æä»¥ä¸‹æ‹›æ ‡æ–‡ä»¶å†…å®¹ï¼Œæå–å…³é”®éœ€æ±‚ä¿¡æ¯ï¼š

æ‹›æ ‡æ–‡ä»¶å†…å®¹ï¼š
{bidding_document_content}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š

1. é¡¹ç›®åŸºæœ¬ä¿¡æ¯
- é¡¹ç›®åç§°
- æ‹›æ ‡å•ä½
- é¡¹ç›®é¢„ç®—
- æäº¤æˆªæ­¢æ—¶é—´

2. æŠ€æœ¯éœ€æ±‚
- æ ¸å¿ƒåŠŸèƒ½éœ€æ±‚
- æŠ€æœ¯æŒ‡æ ‡è¦æ±‚
- æ€§èƒ½è¦æ±‚
- å®‰å…¨è¦æ±‚

3. å•†åŠ¡éœ€æ±‚
- èµ„è´¨è¦æ±‚
- ä¸šç»©è¦æ±‚
- äººå‘˜è¦æ±‚
- å…¶ä»–å•†åŠ¡æ¡ä»¶

4. è¯„åˆ†æ ‡å‡†
- æŠ€æœ¯è¯„åˆ†ï¼ˆæƒé‡å’Œæ ‡å‡†ï¼‰
- å•†åŠ¡è¯„åˆ†ï¼ˆæƒé‡å’Œæ ‡å‡†ï¼‰

5. é£é™©ç‚¹
- è¯†åˆ«æ½œåœ¨çš„é£é™©ç‚¹
- ç»™å‡ºé£é™©ç­‰çº§ï¼ˆé«˜/ä¸­/ä½ï¼‰

6. å»ºè®®
- æŠ•æ ‡å»ºè®®
- éœ€è¦é‡ç‚¹å…³æ³¨çš„æ–¹é¢
"""

CONTENT_GENERATION_PROMPT = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ ‡ä¹¦æ’°å†™ä¸“å®¶ï¼Œæ“…é•¿æ’°å†™æŠ€æœ¯æ–¹æ¡ˆå’Œå•†åŠ¡æ–¹æ¡ˆã€‚

é¡¹ç›®èƒŒæ™¯ï¼š
{project_background}

éœ€æ±‚å†…å®¹ï¼š
{requirements}

ä¼ä¸šèƒ½åŠ›ï¼š
{capabilities}

å†å²æ¡ˆä¾‹ï¼š
{cases}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œæ’°å†™{section_type}éƒ¨åˆ†çš„å†…å®¹ã€‚

è¦æ±‚ï¼š
1. å†…å®¹ä¸“ä¸šã€å‡†ç¡®ã€æœ‰è¯´æœåŠ›
2. çªå‡ºä¼ä¸šä¼˜åŠ¿å’Œç«äº‰åŠ›
3. ç´§å¯†è´´åˆæ‹›æ ‡éœ€æ±‚
4. ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†
5. å­—æ•°çº¦{word_count}å­—

è¯·ç›´æ¥è¾“å‡ºå†…å®¹ï¼Œä¸è¦åŒ…å«é¢å¤–çš„è§£é‡Šã€‚
"""
```

### 3. AIä»»åŠ¡é˜Ÿåˆ—

```python
# app/services/ai/task_queue.py
from celery import Task
from typing import Dict, Any
from app.tasks.celery_app import celery_app
from app.models.ai_task import AITask, TaskStatus
from app.services.ai.llm_client import LLMClient
from app.core.logging import logger

class AITaskQueue:
    """AIä»»åŠ¡é˜Ÿåˆ—ç®¡ç†"""

    @staticmethod
    @celery_app.task(bind=True, max_retries=3)
    async def process_ai_task(self: Task, task_id: str) -> Dict[str, Any]:
        """
        å¤„ç†AIä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            å¤„ç†ç»“æœ
        """
        # è·å–ä»»åŠ¡
        task = await AITask.get(task_id)
        if not task:
            raise ValueError(f"Task not found: {task_id}")

        try:
            # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
            await task.update(
                status=TaskStatus.RUNNING,
                started_at=datetime.utcnow()
            )

            # è·å–LLMå®¢æˆ·ç«¯
            llm_client = LLMClient()

            # æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œ
            if task.task_type == "parse":
                result = await self._process_parse_task(task, llm_client)
            elif task.task_type == "analyze":
                result = await self._process_analyze_task(task, llm_client)
            elif task.task_type == "match":
                result = await self._process_match_task(task, llm_client)
            elif task.task_type == "generate":
                result = await self._process_generate_task(task, llm_client)
            elif task.task_type == "review":
                result = await self._process_review_task(task, llm_client)
            else:
                raise ValueError(f"Unknown task type: {task.task_type}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºæˆåŠŸ
            await task.update(
                status=TaskStatus.SUCCESS,
                output_data=result,
                completed_at=datetime.utcnow(),
                duration_seconds=(datetime.utcnow() - task.started_at).total_seconds()
            )

            return result

        except Exception as e:
            logger.error(f"AI task failed: {task_id}, error: {str(e)}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            await task.update(
                status=TaskStatus.FAILED,
                error_message=str(e),
                completed_at=datetime.utcnow(),
                retry_count=task.retry_count + 1
            )

            # é‡è¯•
            if task.retry_count < task.max_retries:
                raise self.retry(exc=e, countdown=60 * (task.retry_count + 1))

            raise

    @staticmethod
    async def _process_generate_task(
        task: AITask,
        llm_client: LLMClient
    ) -> Dict[str, Any]:
        """å¤„ç†å†…å®¹ç”Ÿæˆä»»åŠ¡"""
        input_data = task.input_data

        # è·å–Prompt
        prompt_manager = PromptManager()
        prompt = await prompt_manager.get_prompt(
            code="content_generation",
            variables=input_data.get("context", {})
        )

        # è°ƒç”¨LLM
        response = await llm_client.chat(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ ‡ä¹¦æ’°å†™ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            model=input_data.get("model", "gpt-4-turbo-preview"),
            temperature=input_data.get("temperature", 0.7),
            max_tokens=input_data.get("max_tokens", 2000)
        )

        return {
            "content": response["content"],
            "tokens_used": response["tokens_used"],
            "model": response["model"]
        }
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **AIèƒ½åŠ›å±‚æ€»è§ˆ**: [00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md](./00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md)
- **å‘é‡æ£€ç´¢æœåŠ¡**: [02-å‘é‡æ£€ç´¢æœåŠ¡.md](./02-å‘é‡æ£€ç´¢æœåŠ¡.md)
- **çŸ¥è¯†å›¾è°±æœåŠ¡**: [03-çŸ¥è¯†å›¾è°±æœåŠ¡.md](./03-çŸ¥è¯†å›¾è°±æœåŠ¡.md)
- **å·¥ä½œæµä¸ä¼˜åŒ–**: [04-å·¥ä½œæµä¸ä¼˜åŒ–.md](./04-å·¥ä½œæµä¸ä¼˜åŒ–.md)

---

## ä¿®æ”¹å†å²

| æ—¥æœŸ | ç‰ˆæœ¬ | ä¿®æ”¹è€… | ä¿®æ”¹å†…å®¹æ¦‚è¦ |
|------|------|--------|-------------|
| 2025-11-30 00:25 | 1.1 | gemini-pro | YAMLå¤´éƒ¨æ—¶é—´æˆ³æ›´æ–°ã€‚ |
| 2025-11-29 | 1.0 | claude-sonnet-4-5 (claude-sonnet-4-5-20250929) | ä»05-AIèƒ½åŠ›å±‚è®¾è®¡.mdæ‹†åˆ†åˆ›å»ºLLMæœåŠ¡æ¶æ„æ–‡æ¡£ |

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´11æœˆ29æ—¥
**æ–‡æ¡£çŠ¶æ€**: âœ… å·²æ‰¹å‡†
