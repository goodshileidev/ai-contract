---
æ–‡æ¡£ç±»å‹: æ¶æ„æ–‡æ¡£
éœ€æ±‚ç¼–å·: DOC-2025-11-001
åˆ›å»ºæ—¥æœŸ: 2025-11-29
åˆ›å»ºè€…: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
æœ€åæ›´æ–°: 2025-11-29
æ›´æ–°è€…: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
çŠ¶æ€: å·²æ‰¹å‡†
---

# AIæ ‡ä¹¦æ™ºèƒ½åˆ›ä½œå¹³å° - AIèƒ½åŠ›æ¶æ„æ¦‚è¿°

> **AIæ¡†æ¶**: LlamaIndex 0.9+ (ä¸»åŠ›RAG) + LangChain 0.1+ (å¤‡ç”¨å·¥ä½œæµ)
> **å‘é‡æ•°æ®åº“**: Elasticsearch 8+ (ç»Ÿä¸€å‘é‡æ£€ç´¢)
> **å¤§æ¨¡å‹**: GPT-4/Claude + æ™ºè°±AI/ç™¾åº¦æ–‡å¿ƒ
> **æœåŠ¡**: Python FastAPI (ç«¯å£8001)

## ğŸ“‹ æ–‡æ¡£å¯¼èˆª

æœ¬æ–‡æ¡£æ˜¯æŠ€æœ¯æ¶æ„è®¾è®¡çš„ä¸€éƒ¨åˆ†,å…¶ä»–ç›¸å…³æ–‡æ¡£:

1. [00-æŠ€æœ¯æ¶æ„æ€»è§ˆ.md](./00-æŠ€æœ¯æ¶æ„æ€»è§ˆ.md) - æ•´ä½“æ¶æ„å’Œå‰ç«¯æŠ€æœ¯æ¶æ„
2. [01-åç«¯æœåŠ¡æ¶æ„.md](./01-åç«¯æœåŠ¡æ¶æ„.md) - Javaå’ŒPythonæœåŠ¡æ¶æ„è¯¦è§£
3. **02-AIèƒ½åŠ›æ¶æ„æ¦‚è¿°.md**(æœ¬æ–‡æ¡£)- AIæœåŠ¡æ¶æ„è®¾è®¡æ¦‚è¿°
4. [03-æ•°æ®å­˜å‚¨æ¶æ„.md](./03-æ•°æ®å­˜å‚¨æ¶æ„.md) - æ•°æ®å­˜å‚¨å’Œæ¶ˆæ¯é˜Ÿåˆ—æ¶æ„
5. [04-éƒ¨ç½²è¿ç»´æ¶æ„.md](./04-éƒ¨ç½²è¿ç»´æ¶æ„.md) - éƒ¨ç½²ã€å®‰å…¨å’Œç›‘æ§æ¶æ„

## ğŸ¤– AIèƒ½åŠ›æ¶æ„

### AIæœåŠ¡æ¶æ„è®¾è®¡
```typescript
// AIèƒ½åŠ›æ¶æ„
interface AIArchitecture {
  // LLMæœåŠ¡å±‚
  llmServices: {
    openai: OpenAIService;      // GPT-4/3.5
    anthropic: AnthropicService; // Claude
    zhipu: ZhipuAIService;      // æ™ºè°±AI
    baidu: BaiduAIService;      // ç™¾åº¦æ–‡å¿ƒ
  };

  // RAGæ¡†æ¶å±‚
  ragFrameworks: {
    llamaIndex: LlamaIndexService;  // ä¸»åŠ›RAGæ¡†æ¶
    langChain: LangChainService;    // å¤‡ç”¨å·¥ä½œæµæ¡†æ¶
  };

  // å‘é‡æ£€ç´¢å±‚
  vectorSearch: {
    elasticsearch: ElasticsearchService;  // ç»Ÿä¸€å‘é‡æ•°æ®åº“
  };

  // AIèƒ½åŠ›å±‚
  aiCapabilities: {
    documentAnalysis: DocumentAnalysisService;
    contentGeneration: ContentGenerationService;
    knowledgeMatching: KnowledgeMatchingService;
    qualityAssurance: QualityAssuranceService;
  };

  // å·¥ä½œæµç¼–æ’å±‚
  workflowEngine: {
    orchestrator: WorkflowOrchestrator;
    taskQueue: TaskQueue;
    stateManager: StateManager;
    errorHandler: ErrorHandler;
  };
}
```

### LlamaIndex RAGå®ç°
```python
# ä½¿ç”¨LlamaIndexæ„å»ºRAGç³»ç»Ÿ
from llama_index import (
    VectorStoreIndex,
    ServiceContext,
    StorageContext,
    Document
)
from llama_index.vector_stores import ElasticsearchStore
from llama_index.llms import OpenAI
from llama_index.embeddings import OpenAIEmbedding
from typing import List, Dict, Any

class BidGenerationRAG:
    """æ ‡ä¹¦ç”ŸæˆRAGç³»ç»Ÿ"""

    def __init__(self):
        # é…ç½®Elasticsearchå‘é‡å­˜å‚¨
        self.vector_store = ElasticsearchStore(
            index_name="bid_knowledge",
            es_url="http://elasticsearch:9200"
        )

        # é…ç½®LLMå’ŒåµŒå…¥æ¨¡å‹
        self.llm = OpenAI(model="gpt-4", temperature=0.1)
        self.embed_model = OpenAIEmbedding()

        # é…ç½®æœåŠ¡ä¸Šä¸‹æ–‡
        self.service_context = ServiceContext.from_defaults(
            llm=self.llm,
            embed_model=self.embed_model,
            chunk_size=512,
            chunk_overlap=50
        )

        # é…ç½®å­˜å‚¨ä¸Šä¸‹æ–‡
        self.storage_context = StorageContext.from_defaults(
            vector_store=self.vector_store
        )

        # åˆ›å»ºç´¢å¼•
        self.index = VectorStoreIndex.from_vector_store(
            vector_store=self.vector_store,
            service_context=self.service_context
        )

    async def analyze_tender(
        self,
        tender_doc: str,
        analysis_type: str = "comprehensive"
    ) -> Dict[str, Any]:
        """åˆ†ææ‹›æ ‡æ–‡æ¡£"""

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        query_engine = self.index.as_query_engine(
            similarity_top_k=10,
            response_mode="tree_summarize"
        )

        # æ„å»ºåˆ†ææç¤º
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹æ‹›æ ‡æ–‡æ¡£ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

        {tender_doc}

        åˆ†æç»´åº¦ï¼š
        1. é¡¹ç›®åŸºæœ¬ä¿¡æ¯
        2. æŠ€æœ¯è¦æ±‚
        3. å•†åŠ¡æ¡æ¬¾
        4. è¯„åˆ†æ ‡å‡†
        5. é£é™©å› ç´ 
        6. æŠ•æ ‡å»ºè®®
        """

        # æ‰§è¡ŒæŸ¥è¯¢
        response = await query_engine.aquery(prompt)

        return {
            "analysis": response.response,
            "source_nodes": [
                {
                    "text": node.node.text,
                    "score": node.score,
                    "metadata": node.node.metadata
                }
                for node in response.source_nodes
            ]
        }

    async def generate_content(
        self,
        requirements: Dict[str, Any],
        company_profile: Dict[str, Any],
        section: str
    ) -> str:
        """ç”Ÿæˆæ ‡ä¹¦å†…å®¹"""

        # æ£€ç´¢ç›¸å…³çŸ¥è¯†
        query_engine = self.index.as_query_engine(
            similarity_top_k=5
        )

        # æ„å»ºç”Ÿæˆæç¤º
        prompt = f"""
        åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆæ ‡ä¹¦{section}éƒ¨åˆ†ï¼š

        é¡¹ç›®è¦æ±‚ï¼š
        {requirements}

        å…¬å¸ä¿¡æ¯ï¼š
        {company_profile}

        è¯·ç”Ÿæˆä¸“ä¸šã€æœ‰è¯´æœåŠ›çš„å†…å®¹ã€‚
        """

        # æ‰§è¡ŒæŸ¥è¯¢ç”Ÿæˆ
        response = await query_engine.aquery(prompt)

        return response.response

    async def index_documents(self, documents: List[Dict[str, Any]]):
        """ç´¢å¼•æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""

        # è½¬æ¢ä¸ºLlamaIndex Documentå¯¹è±¡
        llama_docs = [
            Document(
                text=doc["content"],
                metadata=doc.get("metadata", {})
            )
            for doc in documents
        ]

        # åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex.from_documents(
            llama_docs,
            service_context=self.service_context,
            storage_context=self.storage_context
        )

        return index
```

### LangChainå·¥ä½œæµè®¾è®¡
```python
# æ ‡ä¹¦ç”Ÿæˆå·¥ä½œæµï¼ˆä½¿ç”¨LangChainä½œä¸ºå¤‡ç”¨ï¼‰
from langchain.schema import Document
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

class BidGenerationWorkflow:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4", temperature=0.1)
        self.tender_analyzer = TenderAnalysisChain()
        self.capability_matcher = CapabilityMatchingChain()
        self.content_generator = ContentGenerationChain()
        self.quality_checker = QualityAssuranceChain()

    async def generate_bid(self, tender_doc: Document,
                          company_profile: CompanyProfile) -> BidDocument:
        # 1. åˆ†ææ‹›æ ‡æ–‡ä»¶
        tender_analysis = await self.tender_analyzer.arun(tender_doc)

        # 2. åŒ¹é…ä¼ä¸šèƒ½åŠ›
        capability_match = await self.capability_matcher.arun(
            tender_analysis, company_profile
        )

        # 3. ç”Ÿæˆæ ‡ä¹¦å†…å®¹
        content_generation = await self.content_generator.arun(
            tender_analysis, capability_match
        )

        # 4. è´¨é‡æ£€æŸ¥
        quality_check = await self.quality_checker.arun(content_generation)

        return BidDocument(
            analysis=tender_analysis,
            match=capability_match,
            content=content_generation,
            quality=quality_check
        )
```

### Elasticsearchå‘é‡æ•°æ®åº“è®¾è®¡
```typescript
// Elasticsearchå‘é‡æ•°æ®åº“æ¶æ„
interface ElasticsearchSchema {
  // ä¼ä¸šèƒ½åŠ›å‘é‡ç´¢å¼•
  capabilities: {
    index: "bid_capabilities";
    mappings: {
      properties: {
        id: { type: "keyword" };
        type: { type: "keyword" }; // 'product', 'service', 'project', 'personnel'
        name: { type: "text" };
        description: { type: "text" };
        embedding: {
          type: "dense_vector";
          dims: 1536;  // OpenAI embeddingç»´åº¦
          similarity: "cosine";
        };
        metadata: {
          properties: {
            industry: { type: "keyword" };
            tags: { type: "keyword" };
            experience: { type: "integer" };
            certifications: { type: "keyword" };
          };
        };
      };
    };
  };

  // æ‹›æ ‡éœ€æ±‚å‘é‡ç´¢å¼•
  requirements: {
    index: "bid_requirements";
    mappings: {
      properties: {
        id: { type: "keyword" };
        source: { type: "keyword" };
        content: { type: "text" };
        embedding: {
          type: "dense_vector";
          dims: 1536;
          similarity: "cosine";
        };
        metadata: {
          properties: {
            category: { type: "keyword" };
            priority: { type: "keyword" };
            weight: { type: "float" };
          };
        };
      };
    };
  };

  // æ ‡ä¹¦å†…å®¹å‘é‡ç´¢å¼•
  bid_content: {
    index: "bid_documents";
    mappings: {
      properties: {
        id: { type: "keyword" };
        section: { type: "keyword" };
        content: { type: "text" };
        embedding: {
          type: "dense_vector";
          dims: 1536;
          similarity: "cosine";
        };
        metadata: {
          properties: {
            project_id: { type: "keyword" };
            template_id: { type: "keyword" };
            quality_score: { type: "float" };
            approval_status: { type: "keyword" };
          };
        };
      };
    };
  };
}
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **æŠ€æœ¯æ¶æ„æ€»è§ˆ**: [00-æŠ€æœ¯æ¶æ„æ€»è§ˆ.md](./00-æŠ€æœ¯æ¶æ„æ€»è§ˆ.md)
- **åç«¯æœåŠ¡æ¶æ„**: [01-åç«¯æœåŠ¡æ¶æ„.md](./01-åç«¯æœåŠ¡æ¶æ„.md)
- **AIèƒ½åŠ›å±‚è¯¦ç»†è®¾è®¡**: [@docs/03-æ¶æ„è®¾è®¡/05-AIèƒ½åŠ›å±‚è®¾è®¡/](../05-AIèƒ½åŠ›å±‚è®¾è®¡/)
- **æ•°æ®å­˜å‚¨æ¶æ„**: [03-æ•°æ®å­˜å‚¨æ¶æ„.md](./03-æ•°æ®å­˜å‚¨æ¶æ„.md)
- **éƒ¨ç½²è¿ç»´æ¶æ„**: [04-éƒ¨ç½²è¿ç»´æ¶æ„.md](./04-éƒ¨ç½²è¿ç»´æ¶æ„.md)

## ä¿®æ”¹å†å²

| æ—¥æœŸ | ç‰ˆæœ¬ | ä¿®æ”¹è€… | ä¿®æ”¹å†…å®¹æ¦‚è¦ |
|------|------|--------|-------------|
| 2025-11-29 | 1.0 | claude-sonnet-4-5 (claude-sonnet-4-5-20250929) | ä»02-æŠ€æœ¯æ¶æ„è®¾è®¡.mdæ‹†åˆ†åˆ›å»ºAIèƒ½åŠ›æ¶æ„æ¦‚è¿°æ–‡æ¡£ |

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´11æœˆ29æ—¥
**æ–‡æ¡£çŠ¶æ€**: âœ… å·²æ‰¹å‡†
