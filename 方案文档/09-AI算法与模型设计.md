# AIæ ‡ä¹¦æ™ºèƒ½åˆ›ä½œå¹³å° - AIç®—æ³•ä¸æ¨¡å‹è®¾è®¡

## ğŸ§  AIèƒ½åŠ›æ¶æ„æ€»è§ˆ

### AIç³»ç»Ÿæ¶æ„å›¾
```mermaid
graph TB
    subgraph "AIèƒ½åŠ›å±‚"
        A[æ–‡æ¡£è§£æå¼•æ“]
        B[å†…å®¹ç”Ÿæˆå¼•æ“]
        C[æ™ºèƒ½åŒ¹é…å¼•æ“]
        D[è´¨é‡è¯„ä¼°å¼•æ“]
        E[çŸ¥è¯†å›¾è°±å¼•æ“]
    end

    subgraph "æ¨¡å‹æœåŠ¡å±‚"
        F[LLMæ¨¡å‹é›†ç¾¤]
        G[å‘é‡åµŒå…¥æ¨¡å‹]
        H[åˆ†ç±»æ¨¡å‹]
        I[å›å½’æ¨¡å‹]
        J[æ¨èæ¨¡å‹]
    end

    subgraph "æ•°æ®å¤„ç†å±‚"
        K[æ–‡æœ¬é¢„å¤„ç†]
        L[ç‰¹å¾æå–]
        M[æ•°æ®æ¸…æ´—]
        N[å‘é‡åŒ–]
    end

    subgraph "çŸ¥è¯†ç®¡ç†å±‚"
        O[ä¼ä¸šçŸ¥è¯†å›¾è°±]
        P[è¡Œä¸šçŸ¥è¯†åº“]
        Q[æ¨¡æ¿çŸ¥è¯†åº“]
        R[å†å²æ¡ˆä¾‹åº“]
    end

    A --> K
    B --> L
    C --> M
    D --> N
    E --> O

    K --> F
    L --> G
    M --> H
    N --> I
    O --> J

    F --> O
    G --> P
    H --> Q
    I --> R
    J --> E
```

## ğŸ“„ æ‹›æ ‡æ–‡æ¡£æ™ºèƒ½è§£æç®—æ³•

### 1. æ–‡æ¡£ç»“æ„åŒ–è§£æç®—æ³•
```python
import re
import spacy
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

@dataclass
class DocumentSection:
    """æ–‡æ¡£ç»“æ„åŒ–æ•°æ®æ¨¡å‹"""
    section_type: str
    title: str
    content: str
    page_number: int
    subsections: List['DocumentSection']
    metadata: Dict[str, Any]

class DocumentStructureParser:
    """æ–‡æ¡£ç»“æ„åŒ–è§£æå™¨"""

    def __init__(self):
        self.nlp = spacy.load("zh_core_web_sm")
        self.section_patterns = {
            'project_info': [r'é¡¹ç›®æ¦‚å†µ', r'é¡¹ç›®ç®€ä»‹', r'é¡¹ç›®èƒŒæ™¯'],
            'technical_requirements': [r'æŠ€æœ¯è¦æ±‚', r'æŠ€æœ¯è§„æ ¼', r'æŠ€æœ¯å‚æ•°'],
            'commercial_terms': [r'å•†åŠ¡æ¡æ¬¾', r'åˆåŒæ¡æ¬¾', r'ä»˜æ¬¾æ–¹å¼'],
            'evaluation_criteria': [r'è¯„æ ‡åŠæ³•', r'è¯„åˆ†æ ‡å‡†', r'è¯„å®¡æ–¹æ³•'],
            'submission_requirements': [r'æŠ•æ ‡è¦æ±‚', r'æäº¤è¦æ±‚', r'æŠ•æ ‡é¡»çŸ¥']
        }
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=None,
            ngram_range=(1, 3)
        )

    async def parse_document_structure(self, raw_text: str) -> Dict[str, Any]:
        """
        è§£ææ–‡æ¡£ç»“æ„

        Args:
            raw_text: åŸå§‹æ–‡æ¡£æ–‡æœ¬

        Returns:
            ç»“æ„åŒ–è§£æç»“æœ
        """
        try:
            # 1. æ–‡æœ¬é¢„å¤„ç†
            cleaned_text = self._preprocess_text(raw_text)

            # 2. ç« èŠ‚è¯†åˆ«
            sections = await self._identify_sections(cleaned_text)

            # 3. å†…å®¹åˆ†ç±»
            classified_sections = await self._classify_sections(sections)

            # 4. å…³é”®ä¿¡æ¯æå–
            key_information = await self._extract_key_information(classified_sections)

            # 5. ç»“æ„éªŒè¯
            validation_result = self._validate_structure(classified_sections)

            return {
                'document_structure': classified_sections,
                'key_information': key_information,
                'validation_result': validation_result,
                'parsing_metadata': {
                    'total_sections': len(classified_sections),
                    'processing_time': 0,  # å®é™…è®¡ç®—
                    'confidence_score': validation_result['overall_confidence']
                }
            }

        except Exception as e:
            raise DocumentParsingError(f"æ–‡æ¡£ç»“æ„è§£æå¤±è´¥: {str(e)}")

    def _preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)

        # æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
        text = text.replace('ï¼Œ', ',').replace('ã€‚', '.')
        text = text.replace('ï¼š', ':').replace('ï¼›', ';')

        # ç§»é™¤é¡µçœ‰é¡µè„š
        text = re.sub(r'ç¬¬\d+é¡µ', '', text)
        text = re.sub(r'Page\s+\d+', '', text)

        return text.strip()

    async def _identify_sections(self, text: str) -> List[DocumentSection]:
        """è¯†åˆ«æ–‡æ¡£ç« èŠ‚"""
        sections = []
        lines = text.split('\n')
        current_section = None

        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue

            # æ£€æµ‹ç« èŠ‚æ ‡é¢˜
            if self._is_section_title(line):
                if current_section:
                    sections.append(current_section)

                current_section = DocumentSection(
                    section_type='unknown',
                    title=line,
                    content='',
                    page_number=self._estimate_page_number(i, len(lines)),
                    subsections=[],
                    metadata={'line_number': i}
                )
            elif current_section:
                # æ·»åŠ åˆ°å½“å‰ç« èŠ‚å†…å®¹
                current_section.content += line + '\n'

        if current_section:
            sections.append(current_section)

        return sections

    def _is_section_title(self, line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç« èŠ‚æ ‡é¢˜"""
        # è§„åˆ™1: åŒ…å«æ•°å­—ç¼–å·
        if re.match(r'^\d+[\.ã€].*', line):
            return True

        # è§„åˆ™2: é•¿åº¦é€‚ä¸­ä¸”å…¨å¤§å†™
        if len(line) < 50 and line.isupper():
            return True

        # è§„åˆ™3: åŒ…å«ç« èŠ‚å…³é”®è¯
        section_keywords = ['é¡¹ç›®', 'è¦æ±‚', 'æ ‡å‡†', 'åŠæ³•', 'æ¡æ¬¾', 'é¡»çŸ¥']
        if any(keyword in line for keyword in section_keywords):
            return True

        return False

    async def _classify_sections(self, sections: List[DocumentSection]) -> List[DocumentSection]:
        """å¯¹ç« èŠ‚è¿›è¡Œåˆ†ç±»"""
        for section in sections:
            section.section_type = await self._classify_section_type(section)

        return sections

    async def _classify_section_type(self, section: DocumentSection) -> str:
        """åˆ†ç±»å•ä¸ªç« èŠ‚ç±»å‹"""
        content = section.title + ' ' + section.content

        # ä½¿ç”¨å…³é”®è¯åŒ¹é…è¿›è¡Œåˆæ­¥åˆ†ç±»
        type_scores = {}
        for section_type, patterns in self.section_patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    score += 1
            type_scores[section_type] = score

        # ä½¿ç”¨TF-IDFè¿›è¡Œè¯­ä¹‰åˆ†ç±»
        if hasattr(self, '_section_tfidf_matrix'):
            content_vector = self.tfidf_vectorizer.transform([content])
            similarities = cosine_similarity(content_vector, self._section_tfidf_matrix)

            # ç»“åˆå…³é”®è¯åŒ¹é…å’Œè¯­ä¹‰ç›¸ä¼¼åº¦
            for i, section_type in enumerate(self.section_patterns.keys()):
                type_scores[section_type] += similarities[0][i] * 0.5

        # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»å‹
        if not type_scores or max(type_scores.values()) == 0:
            return 'other'

        return max(type_scores, key=type_scores.get)

    async def _extract_key_information(self, sections: List[DocumentSection]) -> Dict[str, Any]:
        """æå–å…³é”®ä¿¡æ¯"""
        key_info = {
            'project_info': {},
            'technical_requirements': [],
            'commercial_terms': {},
            'evaluation_criteria': {},
            'deadlines': [],
            'contacts': [],
            'budgets': []
        }

        for section in sections:
            if section.section_type == 'project_info':
                key_info['project_info'] = await self._extract_project_info(section)
            elif section.section_type == 'technical_requirements':
                key_info['technical_requirements'] = await self._extract_technical_requirements(section)
            elif section.section_type == 'commercial_terms':
                key_info['commercial_terms'] = await self._extract_commercial_terms(section)
            elif section.section_type == 'evaluation_criteria':
                key_info['evaluation_criteria'] = await self._extract_evaluation_criteria(section)

            # é€šç”¨ä¿¡æ¯æå–
            key_info['deadlines'].extend(await self._extract_deadlines(section))
            key_info['contacts'].extend(await self._extract_contacts(section))
            key_info['budgets'].extend(await self._extract_budgets(section))

        return key_info

    async def _extract_project_info(self, section: DocumentSection) -> Dict[str, Any]:
        """æå–é¡¹ç›®åŸºæœ¬ä¿¡æ¯"""
        doc = self.nlp(section.content)

        project_info = {}

        # é¡¹ç›®åç§°
        project_name_patterns = [
            r'é¡¹ç›®åç§°[ï¼š:]\s*([^\n]+)',
            r'é¡¹ç›®[ï¼š:]\s*([^\n]+)',
            r'å·¥ç¨‹åç§°[ï¼š:]\s*([^\n]+)'
        ]
        for pattern in project_name_patterns:
            match = re.search(pattern, section.content)
            if match:
                project_info['project_name'] = match.group(1).strip()
                break

        # é¡¹ç›®ç¼–å·
        project_number_patterns = [
            r'é¡¹ç›®ç¼–å·[ï¼š:]\s*([A-Za-z0-9\-]+)',
            r'æ‹›æ ‡ç¼–å·[ï¼š:]\s*([A-Za-z0-9\-]+)'
        ]
        for pattern in project_number_patterns:
            match = re.search(pattern, section.content)
            if match:
                project_info['project_number'] = match.group(1).strip()
                break

        # é‡‡è´­äºº
        procurer_patterns = [
            r'é‡‡è´­äºº[ï¼š:]\s*([^\n]+)',
            r'æ‹›æ ‡äºº[ï¼š:]\s*([^\n]+)'
        ]
        for pattern in procurer_patterns:
            match = re.search(pattern, section.content)
            if match:
                project_info['procurement_agency'] = match.group(1).strip()
                break

        return project_info

    async def _extract_technical_requirements(self, section: DocumentSection) -> List[Dict[str, Any]]:
        """æå–æŠ€æœ¯è¦æ±‚"""
        requirements = []

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æŠ€æœ¯è¦æ±‚æ¡ç›®
        requirement_patterns = [
            r'(\d+\..*?)(?=\d+\.|$)',  # æ•°å­—ç¼–å·
            r'ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰.*?(?=ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰|$)',  # ä¸­æ–‡ç¼–å·
            r'[â€¢Â·-]\s*(.*?)(?=[â€¢Â·-]|$)'  # é¡¹ç›®ç¬¦å·
        ]

        for pattern in requirement_patterns:
            matches = re.findall(pattern, section.content, re.DOTALL)
            for match in matches:
                requirement_text = match.strip()
                if len(requirement_text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„åŒ¹é…
                    # è¿›ä¸€æ­¥åˆ†æè¦æ±‚ç±»å‹
                    req_type = self._classify_requirement_type(requirement_text)
                    requirements.append({
                        'requirement': requirement_text,
                        'type': req_type,
                        'mandatory': self._is_mandatory_requirement(requirement_text),
                        'priority': self._assess_requirement_priority(requirement_text)
                    })

        return requirements

    def _classify_requirement_type(self, requirement: str) -> str:
        """åˆ†ç±»æŠ€æœ¯è¦æ±‚ç±»å‹"""
        type_keywords = {
            'functional': ['åŠŸèƒ½', 'å®ç°', 'æ”¯æŒ', 'æä¾›'],
            'performance': ['æ€§èƒ½', 'å“åº”æ—¶é—´', 'ååé‡', 'å¹¶å‘'],
            'security': ['å®‰å…¨', 'åŠ å¯†', 'è®¤è¯', 'æƒé™'],
            'compatibility': ['å…¼å®¹', 'é€‚é…', 'æ”¯æŒ', 'æ ‡å‡†'],
            'reliability': ['å¯é ', 'ç¨³å®š', 'å¯ç”¨', 'å®¹é”™']
        }

        scores = {}
        for req_type, keywords in type_keywords.items():
            score = sum(1 for keyword in keywords if keyword in requirement)
            scores[req_type] = score

        if not scores or max(scores.values()) == 0:
            return 'general'

        return max(scores, key=scores.get)

    def _is_mandatory_requirement(self, requirement: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¼ºåˆ¶æ€§è¦æ±‚"""
        mandatory_keywords = ['å¿…é¡»', 'åº”å½“', 'ä¸¥ç¦', 'ä¸å¾—', 'è¦æ±‚', 'è§„å®š']
        return any(keyword in requirement for keyword in mandatory_keywords)

    def _assess_requirement_priority(self, requirement: str) -> str:
        """è¯„ä¼°è¦æ±‚ä¼˜å…ˆçº§"""
        high_priority_keywords = ['å…³é”®', 'é‡è¦', 'æ ¸å¿ƒ', 'ä¸»è¦']
        low_priority_keywords = ['å»ºè®®', 'å¯é€‰', 'æ¨è', 'æœ€å¥½']

        if any(keyword in requirement for keyword in high_priority_keywords):
            return 'high'
        elif any(keyword in requirement for keyword in low_priority_keywords):
            return 'low'
        else:
            return 'medium'

    def _validate_structure(self, sections: List[DocumentSection]) -> Dict[str, Any]:
        """éªŒè¯æ–‡æ¡£ç»“æ„å®Œæ•´æ€§"""
        required_sections = ['project_info', 'technical_requirements', 'commercial_terms']
        found_sections = {s.section_type for s in sections}

        missing_sections = set(required_sections) - found_sections
        completeness_score = (len(required_sections) - len(missing_sections)) / len(required_sections)

        # è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°
        total_content_length = sum(len(s.content) for s in sections)
        avg_content_length = total_content_length / len(sections) if sections else 0

        quality_score = min(avg_content_length / 1000, 1.0)  # å‡è®¾1000å­—ç¬¦ä¸ºæ»¡åˆ†

        overall_confidence = (completeness_score + quality_score) / 2

        return {
            'is_complete': len(missing_sections) == 0,
            'missing_sections': list(missing_sections),
            'completeness_score': completeness_score,
            'quality_score': quality_score,
            'overall_confidence': overall_confidence,
            'recommendations': self._generate_validation_recommendations(missing_sections, quality_score)
        }

    def _generate_validation_recommendations(self, missing_sections: List[str], quality_score: float) -> List[str]:
        """ç”ŸæˆéªŒè¯å»ºè®®"""
        recommendations = []

        if missing_sections:
            recommendations.append(f"æ–‡æ¡£ç¼ºå°‘ä»¥ä¸‹é‡è¦ç« èŠ‚: {', '.join(missing_sections)}")

        if quality_score < 0.5:
            recommendations.append("æ–‡æ¡£å†…å®¹è¾ƒä¸ºç®€ç•¥ï¼Œå»ºè®®è¡¥å……æ›´å¤šè¯¦ç»†ä¿¡æ¯")

        return recommendations

class DocumentParsingError(Exception):
    """æ–‡æ¡£è§£æå¼‚å¸¸"""
    pass
```

### 2. æ™ºèƒ½éœ€æ±‚åˆ†æç®—æ³•
```python
from typing import List, Dict, Any, Tuple
import networkx as nx
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
import numpy as np

class RequirementAnalyzer:
    """æ™ºèƒ½éœ€æ±‚åˆ†æå™¨"""

    def __init__(self):
        self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.requirement_types = {
            'functional': {
                'keywords': ['åŠŸèƒ½', 'å®ç°', 'æ”¯æŒ', 'æä¾›', 'å¤„ç†'],
                'weight': 0.3
            },
            'non_functional': {
                'keywords': ['æ€§èƒ½', 'å®‰å…¨', 'å¯ç”¨æ€§', 'å…¼å®¹æ€§', 'å¯é æ€§'],
                'weight': 0.25
            },
            'constraint': {
                'keywords': ['çº¦æŸ', 'é™åˆ¶', 'å¿…é¡»', 'åº”å½“', 'ä¸å¾—'],
                'weight': 0.2
            },
            'assumption': {
                'keywords': ['å‡è®¾', 'å‰æ', 'æ¡ä»¶', 'ç¯å¢ƒ'],
                'weight': 0.15
            },
            'deliverable': {
                'keywords': ['äº¤ä»˜', 'äº§å‡º', 'æˆæœ', 'æŠ¥å‘Š'],
                'weight': 0.1
            }
        }

    async def analyze_requirements(self, requirements: List[str]) -> Dict[str, Any]:
        """
        åˆ†æéœ€æ±‚é›†åˆ

        Args:
            requirements: éœ€æ±‚æ–‡æœ¬åˆ—è¡¨

        Returns:
            éœ€æ±‚åˆ†æç»“æœ
        """
        try:
            # 1. éœ€æ±‚åˆ†ç±»
            classified_requirements = await self._classify_requirements(requirements)

            # 2. éœ€æ±‚ä¾èµ–å…³ç³»åˆ†æ
            dependency_graph = await self._analyze_dependencies(requirements)

            # 3. éœ€æ±‚ä¼˜å…ˆçº§åˆ†æ
            priority_analysis = await self._analyze_priorities(classified_requirements)

            # 4. éœ€æ±‚å¤æ‚åº¦è¯„ä¼°
            complexity_analysis = await self._assess_complexity(requirements)

            # 5. éœ€æ±‚é£é™©è¯„ä¼°
            risk_analysis = await self._assess_risks(requirements)

            # 6. éœ€æ±‚ä¸€è‡´æ€§æ£€æŸ¥
            consistency_check = await self._check_consistency(requirements)

            return {
                'classified_requirements': classified_requirements,
                'dependency_graph': dependency_graph,
                'priority_analysis': priority_analysis,
                'complexity_analysis': complexity_analysis,
                'risk_analysis': risk_analysis,
                'consistency_check': consistency_check,
                'analysis_summary': self._generate_analysis_summary(
                    classified_requirements, priority_analysis, complexity_analysis
                )
            }

        except Exception as e:
            raise RequirementAnalysisError(f"éœ€æ±‚åˆ†æå¤±è´¥: {str(e)}")

    async def _classify_requirements(self, requirements: List[str]) -> List[Dict[str, Any]]:
        """åˆ†ç±»éœ€æ±‚"""
        classified = []

        for i, req in enumerate(requirements):
            # è®¡ç®—æ¯ä¸ªç±»å‹çš„ç›¸ä¼¼åº¦åˆ†æ•°
            type_scores = {}

            for req_type, config in self.requirement_types.items():
                score = self._calculate_type_similarity(req, config['keywords'])
                type_scores[req_type] = score * config['weight']

            # ä½¿ç”¨è¯­ä¹‰æ¨¡å‹å¢å¼ºåˆ†ç±»
            embedding = self.sentence_model.encode([req])
            if hasattr(self, '_type_embeddings'):
                similarities = cosine_similarity(embedding, self._type_embeddings)

                for j, req_type in enumerate(self.requirement_types.keys()):
                    type_scores[req_type] += similarities[0][j] * 0.3

            # ç¡®å®šä¸»è¦ç±»å‹å’Œç½®ä¿¡åº¦
            primary_type = max(type_scores, key=type_scores.get)
            confidence = type_scores[primary_type]

            classified.append({
                'id': i,
                'text': req,
                'type': primary_type,
                'confidence': confidence,
                'all_scores': type_scores
            })

        return classified

    def _calculate_type_similarity(self, requirement: str, keywords: List[str]) -> float:
        """è®¡ç®—éœ€æ±‚ä¸ç±»å‹å…³é”®è¯çš„ç›¸ä¼¼åº¦"""
        if not keywords:
            return 0.0

        # ä½¿ç”¨TF-IDFè®¡ç®—ç›¸ä¼¼åº¦
        docs = [requirement] + keywords
        vectorizer = TfidfVectorizer().fit_transform(docs)

        # è®¡ç®—éœ€æ±‚ä¸æ¯ä¸ªå…³é”®è¯çš„ç›¸ä¼¼åº¦
        requirement_vec = vectorizer[0]
        keyword_vectors = vectorizer[1:]

        similarities = cosine_similarity(requirement_vec, keyword_vectors)

        # è¿”å›æœ€é«˜ç›¸ä¼¼åº¦
        return np.max(similarities)

    async def _analyze_dependencies(self, requirements: List[str]) -> Dict[str, Any]:
        """åˆ†æéœ€æ±‚ä¾èµ–å…³ç³»"""
        # æ„å»ºä¾èµ–å›¾
        G = nx.DiGraph()

        # æ·»åŠ èŠ‚ç‚¹
        for i, req in enumerate(requirements):
            G.add_node(i, text=req)

        # åˆ†æä¾èµ–å…³ç³»
        dependencies = {}

        for i, req in enumerate(requirements):
            dependencies[i] = []

            # ä½¿ç”¨å…³é”®è¯è¯†åˆ«ä¾èµ–
            dependency_keywords = ['åŸºäº', 'ä¾èµ–', 'å‰æ', 'éœ€è¦', 'åœ¨...åŸºç¡€ä¸Š']

            for j, other_req in enumerate(requirements):
                if i != j:
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¾èµ–å…³ç³»
                    dependency_score = self._calculate_dependency_score(req, other_req)

                    if dependency_score > 0.3:  # é˜ˆå€¼
                        G.add_edge(j, i, weight=dependency_score)
                        dependencies[i].append({
                            'depends_on': j,
                            'score': dependency_score,
                            'reason': self._explain_dependency(req, other_req)
                        })

        # è®¡ç®—å›¾çš„æŒ‡æ ‡
        try:
            # å…³é”®è·¯å¾„åˆ†æ
            critical_path = nx.dag_longest_path(G) if nx.is_directed_acyclic_graph(G) else []

            # ä¾èµ–æ·±åº¦
            dependency_depths = {}
            for node in G.nodes():
                try:
                    dependency_depths[node] = nx.shortest_path_length(G, source=node)
                except nx.NetworkXNoPath:
                    dependency_depths[node] = 0

            # ä¸­å¿ƒæ€§åˆ†æ
            centrality = nx.degree_centrality(G)
            betweenness = nx.betweenness_centrality(G)

        except Exception as e:
            critical_path = []
            dependency_depths = {}
            centrality = {}
            betweenness = {}

        return {
            'dependencies': dependencies,
            'critical_path': critical_path,
            'dependency_depths': dependency_depths,
            'centrality_metrics': {
                'degree_centrality': centrality,
                'betweenness_centrality': betweenness
            },
            'graph_stats': {
                'num_nodes': G.number_of_nodes(),
                'num_edges': G.number_of_edges(),
                'is_dag': nx.is_directed_acyclic_graph(G)
            }
        }

    def _calculate_dependency_score(self, req1: str, req2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªéœ€æ±‚ä¹‹é—´çš„ä¾èµ–åˆ†æ•°"""
        # ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦
        embedding1 = self.sentence_model.encode([req1])
        embedding2 = self.sentence_model.encode([req2])
        semantic_similarity = cosine_similarity(embedding1, embedding2)[0][0]

        # ä½¿ç”¨å…³é”®è¯åŒ¹é…
        dependency_indicators = ['åŸºäº', 'ä¾èµ–', 'å‚è€ƒ', 'éµå¾ª', 'ç¬¦åˆ']
        keyword_score = 0

        for indicator in dependency_indicators:
            if indicator in req1:
                # æ£€æŸ¥req2æ˜¯å¦åŒ…å«ç›¸å…³å†…å®¹
                if any(word in req2 for word in req1.split() if len(word) > 2):
                    keyword_score += 0.2

        return semantic_similarity * 0.7 + keyword_score * 0.3

    def _explain_dependency(self, req1: str, req2: str) -> str:
        """è§£é‡Šä¾èµ–å…³ç³»çš„åŸå› """
        # æ‰¾å‡ºå…±åŒçš„å…³é”®è¯
        words1 = set(req1.split())
        words2 = set(req2.split())
        common_words = words1.intersection(words2)

        if common_words:
            return f"å­˜åœ¨å…±åŒå…³é”®è¯: {', '.join(list(common_words)[:3])}"
        else:
            return "è¯­ä¹‰ç›¸ä¼¼åº¦è¾ƒé«˜"

    async def _analyze_priorities(self, classified_requirements: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æéœ€æ±‚ä¼˜å…ˆçº§"""
        priorities = {}

        for req in classified_requirements:
            req_id = req['id']

            # åŸºäºç±»å‹çš„åŸºç¡€ä¼˜å…ˆçº§
            type_priority_scores = {
                'functional': 0.8,
                'non_functional': 0.6,
                'constraint': 0.9,
                'assumption': 0.3,
                'deliverable': 0.7
            }

            base_priority = type_priority_scores.get(req['type'], 0.5)

            # åŸºäºå…³é”®è¯è°ƒæ•´ä¼˜å…ˆçº§
            high_priority_keywords = ['å…³é”®', 'é‡è¦', 'æ ¸å¿ƒ', 'ä¸»è¦', 'å¿…é¡»']
            low_priority_keywords = ['å»ºè®®', 'å¯é€‰', 'æ¨è', 'æœŸæœ›']

            keyword_adjustment = 0
            for keyword in high_priority_keywords:
                if keyword in req['text']:
                    keyword_adjustment += 0.1

            for keyword in low_priority_keywords:
                if keyword in req['text']:
                    keyword_adjustment -= 0.1

            # åŸºäºæ–‡æœ¬é•¿åº¦å’Œå¤æ‚åº¦è°ƒæ•´
            complexity_bonus = min(len(req['text']) / 500, 0.2)

            # è®¡ç®—æœ€ç»ˆä¼˜å…ˆçº§
            final_priority = min(max(base_priority + keyword_adjustment + complexity_bonus, 0), 1)

            # ç¡®å®šä¼˜å…ˆçº§ç­‰çº§
            if final_priority >= 0.8:
                priority_level = 'critical'
            elif final_priority >= 0.6:
                priority_level = 'high'
            elif final_priority >= 0.4:
                priority_level = 'medium'
            else:
                priority_level = 'low'

            priorities[req_id] = {
                'score': final_priority,
                'level': priority_level,
                'factors': {
                    'type_priority': base_priority,
                    'keyword_adjustment': keyword_adjustment,
                    'complexity_bonus': complexity_bonus
                }
            }

        return priorities

    async def _assess_complexity(self, requirements: List[str]) -> Dict[str, Any]:
        """è¯„ä¼°éœ€æ±‚å¤æ‚åº¦"""
        complexity_scores = {}

        for i, req in enumerate(requirements):
            # æ–‡æœ¬å¤æ‚åº¦æŒ‡æ ‡
            text_length = len(req)
            word_count = len(req.split())
            sentence_count = len(req.split('ã€‚'))

            # è¯æ±‡å¤æ‚åº¦
            unique_words = len(set(req.split()))
            vocabulary_richness = unique_words / word_count if word_count > 0 else 0

            # æŠ€æœ¯æœ¯è¯­å¯†åº¦
            technical_keywords = ['ç³»ç»Ÿ', 'æ•°æ®åº“', 'ç½‘ç»œ', 'å®‰å…¨', 'æ€§èƒ½', 'æ¥å£', 'æ¶æ„']
            technical_density = sum(1 for word in technical_keywords if word in req) / word_count

            # æ¡ä»¶å¤æ‚åº¦
            condition_words = ['å¦‚æœ', 'å½“', 'åœ¨...æƒ…å†µä¸‹', 'æ»¡è¶³...æ¡ä»¶']
            condition_density = sum(1 for word in condition_words if word in req) / sentence_count if sentence_count > 0 else 0

            # è®¡ç®—ç»¼åˆå¤æ‚åº¦åˆ†æ•°
            complexity_score = (
                min(text_length / 1000, 0.3) +  # æ–‡æœ¬é•¿åº¦ (æœ€å¤š0.3)
                min(vocabulary_richness, 0.2) +  # è¯æ±‡ä¸°å¯Œåº¦ (æœ€å¤š0.2)
                min(technical_density * 5, 0.3) +  # æŠ€æœ¯å¯†åº¦ (æœ€å¤š0.3)
                min(condition_density, 0.2)  # æ¡ä»¶å¤æ‚åº¦ (æœ€å¤š0.2)
            )

            # ç¡®å®šå¤æ‚åº¦ç­‰çº§
            if complexity_score >= 0.8:
                complexity_level = 'very_high'
            elif complexity_score >= 0.6:
                complexity_level = 'high'
            elif complexity_score >= 0.4:
                complexity_level = 'medium'
            else:
                complexity_level = 'low'

            complexity_scores[i] = {
                'score': complexity_score,
                'level': complexity_level,
                'metrics': {
                    'text_length': text_length,
                    'word_count': word_count,
                    'vocabulary_richness': vocabulary_richness,
                    'technical_density': technical_density,
                    'condition_density': condition_density
                }
            }

        return complexity_scores

    async def _assess_risks(self, requirements: List[str]) -> Dict[str, Any]:
        """è¯„ä¼°éœ€æ±‚é£é™©"""
        risk_assessments = {}

        risk_indicators = {
            'ambiguity': ['å¯èƒ½', 'å¤§æ¦‚', 'çº¦', 'å·¦å³', 'ä¼°è®¡'],
            'dependency': ['ä¾èµ–', 'åŸºäº', 'éœ€è¦', 'å‰æ'],
            'technical': ['æ–°æŠ€æœ¯', 'åˆ›æ–°', 'ç ”å‘', 'å¼€å‘'],
            'schedule': ['ç´§æ€¥', 'å°½å¿«', 'ç«‹å³', 'æŒ‰æ—¶'],
            'resource': ['å¤§é‡', 'é«˜å¼ºåº¦', 'é«˜æŠ€èƒ½', 'ä¸“ä¸š']
        }

        for i, req in enumerate(requirements):
            risk_scores = {}

            for risk_type, indicators in risk_indicators.items():
                score = sum(1 for indicator in indicators if indicator in req) / len(indicators)
                risk_scores[risk_type] = score

            # è®¡ç®—ç»¼åˆé£é™©åˆ†æ•°
            overall_risk = sum(risk_scores.values()) / len(risk_scores)

            # ç¡®å®šé£é™©ç­‰çº§
            if overall_risk >= 0.7:
                risk_level = 'high'
            elif overall_risk >= 0.4:
                risk_level = 'medium'
            else:
                risk_level = 'low'

            # ç”Ÿæˆé£é™©æè¿°
            risk_descriptions = []
            for risk_type, score in risk_scores.items():
                if score > 0.3:
                    risk_descriptions.append(f"{risk_type}: {score:.2f}")

            risk_assessments[i] = {
                'overall_risk': overall_risk,
                'risk_level': risk_level,
                'risk_scores': risk_scores,
                'risk_descriptions': risk_descriptions,
                'mitigation_suggestions': self._generate_mitigation_suggestions(risk_scores)
            }

        return risk_assessments

    def _generate_mitigation_suggestions(self, risk_scores: Dict[str, float]) -> List[str]:
        """ç”Ÿæˆé£é™©ç¼“è§£å»ºè®®"""
        suggestions = []

        if risk_scores.get('ambiguity', 0) > 0.3:
            suggestions.append("å»ºè®®è¿›ä¸€æ­¥æ¾„æ¸…éœ€æ±‚ï¼Œæ¶ˆé™¤æ¨¡ç³Šæ€§è¡¨è¿°")

        if risk_scores.get('dependency', 0) > 0.3:
            suggestions.append("å»ºè®®æ˜ç¡®ä¾èµ–å…³ç³»ï¼Œåˆ¶å®šç›¸åº”çš„é£é™©åº”å¯¹è®¡åˆ’")

        if risk_scores.get('technical', 0) > 0.3:
            suggestions.append("å»ºè®®è¿›è¡ŒæŠ€æœ¯å¯è¡Œæ€§è¯„ä¼°ï¼Œå‡†å¤‡æŠ€æœ¯å¤‡é€‰æ–¹æ¡ˆ")

        if risk_scores.get('schedule', 0) > 0.3:
            suggestions.append("å»ºè®®é‡æ–°è¯„ä¼°æ—¶é—´å®‰æ’ï¼Œè€ƒè™‘é€‚å½“çš„ç¼“å†²æ—¶é—´")

        if risk_scores.get('resource', 0) > 0.3:
            suggestions.append("å»ºè®®è¯„ä¼°èµ„æºéœ€æ±‚ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„äººåŠ›å’ŒæŠ€æœ¯æ”¯æŒ")

        return suggestions

    async def _check_consistency(self, requirements: List[str]) -> Dict[str, Any]:
        """æ£€æŸ¥éœ€æ±‚ä¸€è‡´æ€§"""
        consistency_issues = []

        # ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹çŸ›ç›¾
        embeddings = self.sentence_model.encode(requirements)
        similarities = cosine_similarity(embeddings)

        # æ£€æŸ¥ç›¸äº’çŸ›ç›¾çš„éœ€æ±‚
        contradiction_keywords = {
            'positive': ['æ”¯æŒ', 'å…è®¸', 'æä¾›', 'åŒ…å«'],
            'negative': ['ä¸æ”¯æŒ', 'ç¦æ­¢', 'æ’é™¤', 'ä¸å…è®¸']
        }

        for i in range(len(requirements)):
            for j in range(i + 1, len(requirements)):
                # æ£€æŸ¥è¯­ä¹‰ç›¸ä¼¼åº¦
                if similarities[i][j] > 0.7:  # é«˜ç›¸ä¼¼åº¦
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨çŸ›ç›¾
                    req1_has_positive = any(kw in requirements[i] for kw in contradiction_keywords['positive'])
                    req1_has_negative = any(kw in requirements[i] for kw in contradiction_keywords['negative'])
                    req2_has_positive = any(kw in requirements[j] for kw in contradiction_keywords['positive'])
                    req2_has_negative = any(kw in requirements[j] for kw in contradiction_keywords['negative'])

                    if (req1_has_positive and req2_has_negative) or (req1_has_negative and req2_has_positive):
                        consistency_issues.append({
                            'type': 'contradiction',
                            'requirement_1': i,
                            'requirement_2': j,
                            'similarity': similarities[i][j],
                            'description': f"éœ€æ±‚ {i+1} å’Œ {j+1} å¯èƒ½å­˜åœ¨çŸ›ç›¾"
                        })

                # æ£€æŸ¥é‡å¤
                elif similarities[i][j] > 0.9:
                    consistency_issues.append({
                        'type': 'duplication',
                        'requirement_1': i,
                        'requirement_2': j,
                        'similarity': similarities[i][j],
                        'description': f"éœ€æ±‚ {i+1} å’Œ {j+1} å†…å®¹é«˜åº¦é‡å¤"
                    })

        # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
        total_pairs = len(requirements) * (len(requirements) - 1) / 2
        issue_count = len(consistency_issues)
        consistency_score = max(1 - (issue_count / total_pairs), 0)

        return {
            'consistency_score': consistency_score,
            'issues': consistency_issues,
            'issue_count': issue_count,
            'total_pairs': total_pairs,
            'recommendations': self._generate_consistency_recommendations(consistency_issues)
        }

    def _generate_consistency_recommendations(self, issues: List[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆä¸€è‡´æ€§æ”¹è¿›å»ºè®®"""
        recommendations = []

        contradictions = [issue for issue in issues if issue['type'] == 'contradiction']
        duplications = [issue for issue in issues if issue['type'] == 'duplication']

        if contradictions:
            recommendations.append(f"å‘ç° {len(contradictions)} ä¸ªçŸ›ç›¾éœ€æ±‚ï¼Œå»ºè®®é‡æ–°å®¡æŸ¥ç›¸å…³éœ€æ±‚")

        if duplications:
            recommendations.append(f"å‘ç° {len(duplications)} ä¸ªé‡å¤éœ€æ±‚ï¼Œå»ºè®®åˆå¹¶æˆ–åˆ é™¤é‡å¤å†…å®¹")

        if not issues:
            recommendations.append("éœ€æ±‚ä¸€è‡´æ€§è‰¯å¥½ï¼Œæœªå‘ç°æ˜æ˜¾é—®é¢˜")

        return recommendations

    def _generate_analysis_summary(self, classified_requirements: List[Dict[str, Any]],
                                 priority_analysis: Dict[str, Any],
                                 complexity_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        # ç»Ÿè®¡å„ç±»å‹éœ€æ±‚æ•°é‡
        type_counts = {}
        for req in classified_requirements:
            req_type = req['type']
            type_counts[req_type] = type_counts.get(req_type, 0) + 1

        # ç»Ÿè®¡ä¼˜å…ˆçº§åˆ†å¸ƒ
        priority_counts = {}
        for req_id, priority_info in priority_analysis.items():
            level = priority_info['level']
            priority_counts[level] = priority_counts.get(level, 0) + 1

        # ç»Ÿè®¡å¤æ‚åº¦åˆ†å¸ƒ
        complexity_counts = {}
        for req_id, complexity_info in complexity_analysis.items():
            level = complexity_info['level']
            complexity_counts[level] = complexity_counts.get(level, 0) + 1

        return {
            'total_requirements': len(classified_requirements),
            'type_distribution': type_counts,
            'priority_distribution': priority_counts,
            'complexity_distribution': complexity_counts,
            'key_insights': self._generate_key_insights(type_counts, priority_counts, complexity_counts)
        }

    def _generate_key_insights(self, type_counts: Dict[str, int],
                             priority_counts: Dict[str, int],
                             complexity_counts: Dict[str, int]) -> List[str]:
        """ç”Ÿæˆå…³é”®æ´å¯Ÿ"""
        insights = []

        # éœ€æ±‚ç±»å‹æ´å¯Ÿ
        if 'functional' in type_counts:
            insights.append(f"åŠŸèƒ½éœ€æ±‚å æ¯”æœ€é«˜ ({type_counts['functional']} ä¸ª)ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨åŠŸèƒ½å®ç°")

        if 'constraint' in type_counts and type_counts['constraint'] > 0:
            insights.append(f"å‘ç° {type_counts['constraint']} ä¸ªçº¦æŸæ¡ä»¶ï¼Œéœ€è¦ä¸¥æ ¼éµå®ˆ")

        # ä¼˜å…ˆçº§æ´å¯Ÿ
        high_priority_total = priority_counts.get('critical', 0) + priority_counts.get('high', 0)
        if high_priority_total > 0:
            insights.append(f"é«˜ä¼˜å…ˆçº§éœ€æ±‚ {high_priority_total} ä¸ªï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†")

        # å¤æ‚åº¦æ´å¯Ÿ
        high_complexity_total = complexity_counts.get('very_high', 0) + complexity_counts.get('high', 0)
        if high_complexity_total > 0:
            insights.append(f"é«˜å¤æ‚åº¦éœ€æ±‚ {high_complexity_total} ä¸ªï¼Œå»ºè®®åˆ†é…æ›´å¤šèµ„æºå’Œæ—¶é—´")

        return insights

class RequirementAnalysisError(Exception):
    """éœ€æ±‚åˆ†æå¼‚å¸¸"""
    pass
```

## ğŸ¯ æ™ºèƒ½åŒ¹é…å¼•æ“ç®—æ³•

### 1. ä¼ä¸šèƒ½åŠ›åŒ¹é…ç®—æ³•
```python
import numpy as np
from typing import Dict, List, Any, Tuple
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import networkx as nx
from dataclasses import dataclass

@dataclass
class CapabilityMatch:
    """èƒ½åŠ›åŒ¹é…ç»“æœ"""
    capability_id: str
    capability_name: str
    match_score: float
    match_details: Dict[str, Any]
    gaps: List[str]
    strengths: List[str]

@dataclass
class RequirementMatch:
    """éœ€æ±‚åŒ¹é…ç»“æœ"""
    requirement_id: str
    requirement_text: str
    matched_capabilities: List[CapabilityMatch]
    overall_match_score: float
    coverage_percentage: float
    risk_assessment: Dict[str, Any]

class IntelligentMatchingEngine:
    """æ™ºèƒ½åŒ¹é…å¼•æ“"""

    def __init__(self):
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            stop_words=None
        )

        # åŒ¹é…æƒé‡é…ç½®
        self.match_weights = {
            'semantic_similarity': 0.4,
            'keyword_matching': 0.3,
            'experience_matching': 0.2,
            'certification_matching': 0.1
        }

        # èƒ½åŠ›ç±»å‹æƒé‡
        self.capability_type_weights = {
            'product': 0.3,
            'service': 0.3,
            'project': 0.2,
            'personnel': 0.1,
            'technology': 0.1
        }

    async def match_requirements_to_capabilities(self,
                                                requirements: List[str],
                                                company_capabilities: List[Dict[str, Any]],
                                                matching_criteria: List[str] = None) -> Dict[str, Any]:
        """
        å°†éœ€æ±‚åŒ¹é…åˆ°ä¼ä¸šèƒ½åŠ›

        Args:
            requirements: éœ€æ±‚åˆ—è¡¨
            company_capabilities: ä¼ä¸šèƒ½åŠ›åˆ—è¡¨
            matching_criteria: åŒ¹é…æ¡ä»¶

        Returns:
            åŒ¹é…ç»“æœ
        """
        try:
            # 1. é¢„å¤„ç†èƒ½åŠ›æ•°æ®
            processed_capabilities = await self._preprocess_capabilities(company_capabilities)

            # 2. è®¡ç®—éœ€æ±‚-èƒ½åŠ›åŒ¹é…çŸ©é˜µ
            match_matrix = await self._calculate_match_matrix(requirements, processed_capabilities)

            # 3. åˆ†æåŒ¹é…ç»“æœ
            match_analysis = await self._analyze_match_results(match_matrix, requirements, processed_capabilities)

            # 4. è¯†åˆ«å·®è·å’Œä¼˜åŠ¿
            gap_analysis = await self._analyze_gaps(match_analysis, requirements)

            # 5. ç”ŸæˆåŒ¹é…å»ºè®®
            recommendations = await self._generate_recommendations(match_analysis, gap_analysis)

            # 6. è®¡ç®—æ•´ä½“åŒ¹é…åˆ†æ•°
            overall_metrics = self._calculate_overall_metrics(match_analysis)

            return {
                'match_analysis': match_analysis,
                'gap_analysis': gap_analysis,
                'recommendations': recommendations,
                'overall_metrics': overall_metrics,
                'matching_metadata': {
                    'total_requirements': len(requirements),
                    'total_capabilities': len(processed_capabilities),
                    'processing_time': 0  # å®é™…è®¡ç®—
                }
            }

        except Exception as e:
            raise MatchingError(f"èƒ½åŠ›åŒ¹é…å¤±è´¥: {str(e)}")

    async def _preprocess_capabilities(self, capabilities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é¢„å¤„ç†ä¼ä¸šèƒ½åŠ›æ•°æ®"""
        processed = []

        for capability in capabilities:
            # æ ‡å‡†åŒ–æ–‡æœ¬
            description = capability.get('description', '')
            features = ' '.join(capability.get('features', []))
            benefits = ' '.join(capability.get('benefits', []))

            # åˆå¹¶æ–‡æœ¬å†…å®¹
            combined_text = f"{capability['name']} {description} {features} {benefits}"

            processed_capability = {
                'id': capability['id'],
                'name': capability['name'],
                'type': capability['type'],
                'combined_text': combined_text,
                'original': capability,
                'keywords': self._extract_keywords(combined_text),
                'proficiency_level': capability.get('proficiency_level', 3),
                'experience_years': capability.get('experience_years', 0),
                'case_study_count': capability.get('case_study_count', 0),
                'tags': capability.get('tags', [])
            }

            processed.append(processed_capability)

        return processed

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ä½¿ç”¨TF-IDFæå–å…³é”®è¯
        try:
            tfidf_matrix = self.tfidf_vectorizer.fit_transform([text])
            feature_names = self.tfidf_vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]

            # è·å–å‰20ä¸ªå…³é”®è¯
            top_indices = np.argsort(tfidf_scores)[-20:][::-1]
            keywords = [feature_names[i] for i in top_indices if tfidf_scores[i] > 0]

            return keywords
        except:
            return []

    async def _calculate_match_matrix(self,
                                    requirements: List[str],
                                    capabilities: List[Dict[str, Any]]) -> np.ndarray:
        """è®¡ç®—éœ€æ±‚-èƒ½åŠ›åŒ¹é…çŸ©é˜µ"""
        # å‡†å¤‡æ–‡æœ¬æ•°æ®
        all_texts = requirements + [cap['combined_text'] for cap in capabilities]

        # è®¡ç®—TF-IDFçŸ©é˜µ
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_texts)

        # åˆ†ç¦»éœ€æ±‚å’Œèƒ½åŠ›å‘é‡
        req_vectors = tfidf_matrix[:len(requirements)]
        cap_vectors = tfidf_matrix[len(requirements):]

        # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ
        semantic_similarity_matrix = cosine_similarity(req_vectors, cap_vectors)

        # è®¡ç®—å…³é”®è¯åŒ¹é…çŸ©é˜µ
        keyword_match_matrix = self._calculate_keyword_matches(requirements, capabilities)

        # è®¡ç®—ç»éªŒåŒ¹é…çŸ©é˜µ
        experience_match_matrix = self._calculate_experience_matches(requirements, capabilities)

        # è®¡ç®—è®¤è¯åŒ¹é…çŸ©é˜µ
        certification_match_matrix = self._calculate_certification_matches(requirements, capabilities)

        # åŠ æƒç»„åˆæ‰€æœ‰åŒ¹é…åˆ†æ•°
        final_match_matrix = (
            semantic_similarity_matrix * self.match_weights['semantic_similarity'] +
            keyword_match_matrix * self.match_weights['keyword_matching'] +
            experience_match_matrix * self.match_weights['experience_matching'] +
            certification_match_matrix * self.match_weights['certification_matching']
        )

        return final_match_matrix

    def _calculate_keyword_matches(self, requirements: List[str], capabilities: List[Dict[str, Any]]) -> np.ndarray:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        matrix = np.zeros((len(requirements), len(capabilities)))

        # ä¸ºæ¯ä¸ªéœ€æ±‚æå–å…³é”®è¯
        req_keywords_list = []
        for req in requirements:
            req_keywords = self._extract_keywords(req)
            req_keywords_list.append(set(req_keywords))

        # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
        for i, req_keywords in enumerate(req_keywords_list):
            for j, capability in enumerate(capabilities):
                cap_keywords = set(capability['keywords'])
                cap_tags = set(capability['tags'])

                # è®¡ç®—å…³é”®è¯é‡å åº¦
                all_keywords = req_keywords.union(cap_keywords).union(cap_tags)
                if len(all_keywords) > 0:
                    overlap = len(req_keywords.intersection(cap_keywords).union(req_keywords.intersection(cap_tags)))
                    matrix[i][j] = overlap / len(all_keywords)

        return matrix

    def _calculate_experience_matches(self, requirements: List[str], capabilities: List[Dict[str, Any]]) -> np.ndarray:
        """è®¡ç®—ç»éªŒåŒ¹é…åˆ†æ•°"""
        matrix = np.zeros((len(requirements), len(capabilities)))

        # å®šä¹‰ç»éªŒå…³é”®è¯
        experience_keywords = {
            'senior': ['5å¹´', 'é«˜çº§', 'èµ„æ·±', 'ä¸“å®¶'],
            'intermediate': ['3å¹´', 'ä¸­çº§', 'ç†Ÿç»ƒ'],
            'junior': ['1å¹´', 'åˆçº§', 'åŸºç¡€']
        }

        for i, req in enumerate(requirements):
            for j, capability in enumerate(capabilities):
                # åŸºäºèƒ½åŠ›ç±»å‹çš„ç»éªŒåŒ¹é…
                base_score = 0

                if capability['type'] in ['project', 'personnel']:
                    # é¡¹ç›®å’Œäººå‘˜èƒ½åŠ›æ›´æ³¨é‡ç»éªŒ
                    experience_score = min(capability['experience_years'] / 10, 1.0)
                    case_study_score = min(capability['case_study_count'] / 20, 1.0)
                    proficiency_score = capability['proficiency_level'] / 5

                    base_score = (experience_score * 0.4 + case_study_score * 0.3 + proficiency_score * 0.3)
                else:
                    # äº§å“å’ŒæœåŠ¡èƒ½åŠ›
                    base_score = capability['proficiency_level'] / 5

                # æ£€æŸ¥éœ€æ±‚ä¸­çš„ç»éªŒè¦æ±‚
                req_experience_level = self._detect_experience_requirement(req)
                if req_experience_level:
                    if req_experience_level == 'senior' and capability['experience_years'] >= 5:
                        base_score *= 1.2
                    elif req_experience_level == 'intermediate' and capability['experience_years'] >= 3:
                        base_score *= 1.1

                matrix[i][j] = min(base_score, 1.0)

        return matrix

    def _detect_experience_requirement(self, requirement: str) -> str:
        """æ£€æµ‹éœ€æ±‚ä¸­çš„ç»éªŒè¦æ±‚"""
        for level, keywords in experience_keywords.items():
            if any(keyword in requirement for keyword in keywords):
                return level
        return None

    def _calculate_certification_matches(self, requirements: List[str], capabilities: List[Dict[str, Any]]) -> np.ndarray:
        """è®¡ç®—è®¤è¯åŒ¹é…åˆ†æ•°"""
        matrix = np.zeros((len(requirements), len(capabilities)))

        # å®šä¹‰è®¤è¯å…³é”®è¯
        certification_keywords = [
            'ISO', 'CMMI', 'PMP', 'è®¤è¯', 'èµ„è´¨', 'è¯ä¹¦', 'è®¸å¯è¯',
            'å®‰å…¨è®¤è¯', 'è´¨é‡è®¤è¯', 'è¡Œä¸šè®¤è¯', 'ä¸“ä¸šè®¤è¯'
        ]

        for i, req in enumerate(requirements):
            # æ£€æŸ¥éœ€æ±‚ä¸­çš„è®¤è¯è¦æ±‚
            req_certifications = set()
            for keyword in certification_keywords:
                if keyword in req:
                    req_certifications.add(keyword)

            for j, capability in enumerate(capabilities):
                # æ£€æŸ¥èƒ½åŠ›çš„è®¤è¯æƒ…å†µ
                cap_certifications = set()
                cap_data = capability['original']

                if 'certifications' in cap_data:
                    for cert in cap_data['certifications']:
                        for keyword in certification_keywords:
                            if keyword in cert.get('name', ''):
                                cap_certifications.add(keyword)

                # è®¡ç®—è®¤è¯åŒ¹é…åº¦
                if len(req_certifications) > 0:
                    overlap = len(req_certifications.intersection(cap_certifications))
                    matrix[i][j] = overlap / len(req_certifications)
                else:
                    # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„è®¤è¯è¦æ±‚ï¼ŒåŸºäºèƒ½åŠ›ç±»å‹ç»™åŸºç¡€åˆ†æ•°
                    if capability['type'] in ['service', 'technology']:
                        matrix[i][j] = 0.3  # æœåŠ¡å’ŒæŠ€æœ¯é€šå¸¸éœ€è¦è®¤è¯
                    else:
                        matrix[i][j] = 0.1

        return matrix

    async def _analyze_match_results(self,
                                   match_matrix: np.ndarray,
                                   requirements: List[str],
                                   capabilities: List[Dict[str, Any]]) -> List[RequirementMatch]:
        """åˆ†æåŒ¹é…ç»“æœ"""
        requirement_matches = []

        # è®¾ç½®åŒ¹é…é˜ˆå€¼
        match_threshold = 0.3

        for i, requirement in enumerate(requirements):
            # è·å–è¯¥éœ€æ±‚çš„æ‰€æœ‰åŒ¹é…åˆ†æ•°
            match_scores = match_matrix[i]

            # æ‰¾åˆ°åŒ¹é…çš„èƒ½åŠ›
            matched_capabilities = []
            matched_indices = np.where(match_scores >= match_threshold)[0]

            for j in matched_indices:
                match_score = match_scores[j]
                capability = capabilities[j]

                # åˆ›å»ºèƒ½åŠ›åŒ¹é…å¯¹è±¡
                capability_match = CapabilityMatch(
                    capability_id=capability['id'],
                    capability_name=capability['name'],
                    match_score=match_score,
                    match_details=self._analyze_capability_match(requirement, capability, match_score),
                    gaps=self._identify_capability_gaps(requirement, capability),
                    strengths=self._identify_capability_strengths(requirement, capability)
                )

                matched_capabilities.append(capability_match)

            # æŒ‰åŒ¹é…åˆ†æ•°æ’åº
            matched_capabilities.sort(key=lambda x: x.match_score, reverse=True)

            # è®¡ç®—æ•´ä½“æŒ‡æ ‡
            if matched_capabilities:
                overall_match_score = max(cap.match_score for cap in matched_capabilities)
                coverage_percentage = len(matched_capabilities) / len(capabilities) * 100
            else:
                overall_match_score = 0
                coverage_percentage = 0

            # é£é™©è¯„ä¼°
            risk_assessment = self._assess_requirement_risk(requirement, matched_capabilities)

            requirement_match = RequirementMatch(
                requirement_id=str(i),
                requirement_text=requirement,
                matched_capabilities=matched_capabilities,
                overall_match_score=overall_match_score,
                coverage_percentage=coverage_percentage,
                risk_assessment=risk_assessment
            )

            requirement_matches.append(requirement_match)

        return requirement_matches

    def _analyze_capability_match(self, requirement: str, capability: Dict[str, Any], match_score: float) -> Dict[str, Any]:
        """åˆ†æèƒ½åŠ›åŒ¹é…è¯¦æƒ…"""
        details = {
            'semantic_similarity': 0,
            'keyword_overlap': 0,
            'experience_match': 0,
            'certification_match': 0,
            'type_weight': self.capability_type_weights.get(capability['type'], 0.2)
        }

        # è®¡ç®—å„é¡¹åŒ¹é…åˆ†æ•°ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        req_keywords = set(self._extract_keywords(requirement))
        cap_keywords = set(capability['keywords'])

        if len(req_keywords) > 0:
            details['keyword_overlap'] = len(req_keywords.intersection(cap_keywords)) / len(req_keywords)

        details['experience_match'] = capability['proficiency_level'] / 5

        return details

    def _identify_capability_gaps(self, requirement: str, capability: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«èƒ½åŠ›å·®è·"""
        gaps = []

        # åŸºäºç»éªŒå¹´é™çš„å·®è·
        if capability['experience_years'] < 3:
            gaps.append("ç»éªŒç›¸å¯¹è¾ƒå°‘")

        # åŸºäºæ¡ˆä¾‹æ•°é‡çš„å·®è·
        if capability['case_study_count'] < 5:
            gaps.append("æˆåŠŸæ¡ˆä¾‹è¾ƒå°‘")

        # åŸºäºèƒ½åŠ›ç­‰çº§çš„å·®è·
        if capability['proficiency_level'] < 3:
            gaps.append("èƒ½åŠ›ç†Ÿç»ƒåº¦æœ‰å¾…æå‡")

        # åŸºäºå…³é”®è¯è¦†ç›–çš„å·®è·
        req_keywords = set(self._extract_keywords(requirement))
        cap_keywords = set(capability['keywords'])
        uncovered_keywords = req_keywords - cap_keywords

        if uncovered_keywords and len(uncovered_keywords) <= 5:
            gaps.append(f"æœªè¦†ç›–å…³é”®è¯: {', '.join(list(uncovered_keywords)[:3])}")

        return gaps

    def _identify_capability_strengths(self, requirement: str, capability: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«èƒ½åŠ›ä¼˜åŠ¿"""
        strengths = []

        # åŸºäºç»éªŒçš„ä¼˜åŠ¿
        if capability['experience_years'] >= 5:
            strengths.append("ç»éªŒä¸°å¯Œ")

        # åŸºäºæ¡ˆä¾‹çš„ä¼˜åŠ¿
        if capability['case_study_count'] >= 10:
            strengths.append("æˆåŠŸæ¡ˆä¾‹ä¸°å¯Œ")

        # åŸºäºèƒ½åŠ›ç­‰çº§çš„ä¼˜åŠ¿
        if capability['proficiency_level'] >= 4:
            strengths.append("ä¸“ä¸šèƒ½åŠ›çªå‡º")

        # åŸºäºåŒ¹é…åº¦çš„ä¼˜åŠ¿
        if capability['type'] in ['product', 'service']:
            strengths.append("æ ¸å¿ƒäº§å“/æœåŠ¡èƒ½åŠ›")

        return strengths

    def _assess_requirement_risk(self, requirement: str, matched_capabilities: List[CapabilityMatch]) -> Dict[str, Any]:
        """è¯„ä¼°éœ€æ±‚é£é™©"""
        if not matched_capabilities:
            return {
                'risk_level': 'high',
                'risk_score': 1.0,
                'risk_factors': ['æ— åŒ¹é…èƒ½åŠ›', 'å¯èƒ½éœ€è¦å¤–éƒ¨èµ„æº'],
                'mitigation_suggestions': ['è€ƒè™‘å¤–åŒ…', 'é‡æ–°è¯„ä¼°éœ€æ±‚', 'å¯»æ‰¾åˆä½œä¼™ä¼´']
            }

        # è®¡ç®—é£é™©æŒ‡æ ‡
        best_match_score = matched_capabilities[0].match_score
        match_count = len(matched_capabilities)

        # é£é™©å› ç´ 
        risk_factors = []

        if best_match_score < 0.5:
            risk_factors.append('æœ€ä½³åŒ¹é…åˆ†æ•°è¾ƒä½')

        if match_count < 2:
            risk_factors.append('å¤‡é€‰èƒ½åŠ›ä¸è¶³')

        # æ£€æŸ¥èƒ½åŠ›å·®è·
        total_gaps = sum(len(cap.gaps) for cap in matched_capabilities)
        if total_gaps > 0:
            risk_factors.append('å­˜åœ¨èƒ½åŠ›å·®è·')

        # è®¡ç®—é£é™©åˆ†æ•°
        risk_score = 1.0 - (best_match_score * 0.6 + min(match_count / 5, 1.0) * 0.4)

        # ç¡®å®šé£é™©ç­‰çº§
        if risk_score >= 0.7:
            risk_level = 'high'
        elif risk_score >= 0.4:
            risk_level = 'medium'
        else:
            risk_level = 'low'

        # ç”Ÿæˆç¼“è§£å»ºè®®
        mitigation_suggestions = self._generate_risk_mitigation_suggestions(risk_factors, matched_capabilities)

        return {
            'risk_level': risk_level,
            'risk_score': risk_score,
            'risk_factors': risk_factors,
            'mitigation_suggestions': mitigation_suggestions
        }

    def _generate_risk_mitigation_suggestions(self, risk_factors: List[str], matched_capabilities: List[CapabilityMatch]) -> List[str]:
        """ç”Ÿæˆé£é™©ç¼“è§£å»ºè®®"""
        suggestions = []

        if 'æœ€ä½³åŒ¹é…åˆ†æ•°è¾ƒä½' in risk_factors:
            suggestions.append("å»ºè®®åŠ å¼ºç›¸å…³èƒ½åŠ›å»ºè®¾æˆ–å¯»æ‰¾å¤–éƒ¨åˆä½œ")

        if 'å¤‡é€‰èƒ½åŠ›ä¸è¶³' in risk_factors:
            suggestions.append("å»ºè®®åŸ¹å…»å¤šç§å¤‡é€‰èƒ½åŠ›ä»¥é™ä½é£é™©")

        if 'å­˜åœ¨èƒ½åŠ›å·®è·' in risk_factors:
            suggestions.append("é’ˆå¯¹èƒ½åŠ›å·®è·åˆ¶å®šæ”¹è¿›è®¡åˆ’")

        if 'æ— åŒ¹é…èƒ½åŠ›' in risk_factors:
            suggestions.extend(["è€ƒè™‘å¤–éƒ¨é‡‡è´­", "é‡æ–°è¯„ä¼°éœ€æ±‚å¯è¡Œæ€§", "å¯»æ‰¾æˆ˜ç•¥åˆä½œä¼™ä¼´"])

        # åŸºäºæœ€ä½³èƒ½åŠ›çš„å…·ä½“å»ºè®®
        if matched_capabilities:
            best_cap = matched_capabilities[0]
            if best_cap.gaps:
                suggestions.append(f"é‡ç‚¹æå‡{best_cap.capability_name}èƒ½åŠ›: {', '.join(best_cap.gaps[:2])}")

        return suggestions

    async def _analyze_gaps(self, match_analysis: List[RequirementMatch], requirements: List[str]) -> Dict[str, Any]:
        """åˆ†æèƒ½åŠ›å·®è·"""
        # æ‰¾å‡ºæœªåŒ¹é…æˆ–ä½åŒ¹é…çš„éœ€æ±‚
        unmatched_requirements = []
        low_match_requirements = []

        for req_match in match_analysis:
            if req_match.overall_match_score < 0.3:
                unmatched_requirements.append(req_match)
            elif req_match.overall_match_score < 0.6:
                low_match_requirements.append(req_match)

        # åˆ†æå·®è·ç±»å‹
        gap_types = {
            'capability_gaps': [],
            'experience_gaps': [],
            'resource_gaps': [],
            'certification_gaps': []
        }

        for req_match in low_match_requirements:
            for cap_match in req_match.matched_capabilities:
                for gap in cap_match.gaps:
                    if 'ç»éªŒ' in gap:
                        gap_types['experience_gaps'].append({
                            'requirement_id': req_match.requirement_id,
                            'capability_id': cap_match.capability_id,
                            'gap_description': gap
                        })
                    elif 'æ¡ˆä¾‹' in gap:
                        gap_types['capability_gaps'].append({
                            'requirement_id': req_match.requirement_id,
                            'capability_id': cap_match.capability_id,
                            'gap_description': gap
                        })
                    elif 'è®¤è¯' in gap or 'èµ„è´¨' in gap:
                        gap_types['certification_gaps'].append({
                            'requirement_id': req_match.requirement_id,
                            'capability_id': cap_match.capability_id,
                            'gap_description': gap
                        })
                    else:
                        gap_types['resource_gaps'].append({
                            'requirement_id': req_match.requirement_id,
                            'capability_id': cap_match.capability_id,
                            'gap_description': gap
                        })

        # è®¡ç®—å·®è·ä¸¥é‡ç¨‹åº¦
        total_requirements = len(requirements)
        gap_severity = {
            'critical': len(unmatched_requirements) / total_requirements,
            'high': len(low_match_requirements) / total_requirements,
            'medium': 0,  # å¯ä»¥æ ¹æ®å…¶ä»–æŒ‡æ ‡è®¡ç®—
            'low': 0
        }

        return {
            'unmatched_requirements': unmatched_requirements,
            'low_match_requirements': low_match_requirements,
            'gap_types': gap_types,
            'gap_severity': gap_severity,
            'gap_summary': self._generate_gap_summary(gap_types, gap_severity)
        }

    def _generate_gap_summary(self, gap_types: Dict[str, List], gap_severity: Dict[str, float]) -> Dict[str, Any]:
        """ç”Ÿæˆå·®è·æ‘˜è¦"""
        summary = {
            'total_gaps': sum(len(gaps) for gaps in gap_types.values()),
            'most_common_gap_type': '',
            'priority_gaps': [],
            'overall_assessment': ''
        }

        # æ‰¾å‡ºæœ€å¸¸è§çš„å·®è·ç±»å‹
        max_gap_type = max(gap_types.items(), key=lambda x: len(x[1]))
        summary['most_common_gap_type'] = max_gap_type[0]

        # ç¡®å®šä¼˜å…ˆå·®è·
        all_gaps = []
        for gap_type, gaps in gap_types.items():
            for gap in gaps:
                gap['type'] = gap_type
                all_gaps.append(gap)

        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        priority_order = ['capability_gaps', 'experience_gaps', 'certification_gaps', 'resource_gaps']
        all_gaps.sort(key=lambda x: priority_order.index(x['type']))

        summary['priority_gaps'] = all_gaps[:10]  # å‰10ä¸ªä¼˜å…ˆå·®è·

        # æ•´ä½“è¯„ä¼°
        if gap_severity['critical'] > 0.3:
            summary['overall_assessment'] = 'å­˜åœ¨ä¸¥é‡èƒ½åŠ›å·®è·ï¼Œéœ€è¦ç«‹å³é‡‡å–è¡ŒåŠ¨'
        elif gap_severity['high'] > 0.5:
            summary['overall_assessment'] = 'å­˜åœ¨æ˜æ˜¾èƒ½åŠ›å·®è·ï¼Œéœ€è¦åˆ¶å®šæ”¹è¿›è®¡åˆ’'
        else:
            summary['overall_assessment'] = 'èƒ½åŠ›å·®è·å¯æ§ï¼Œé€šè¿‡æŒç»­æ”¹è¿›å¯è§£å†³'

        return summary

    async def _generate_recommendations(self, match_analysis: List[RequirementMatch], gap_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”ŸæˆåŒ¹é…å»ºè®®"""
        recommendations = []

        # åŸºäºåŒ¹é…ç»“æœçš„å»ºè®®
        high_match_requirements = [req for req in match_analysis if req.overall_match_score >= 0.7]
        medium_match_requirements = [req for req in match_analysis if 0.4 <= req.overall_match_score < 0.7]
        low_match_requirements = [req for req in match_analysis if req.overall_match_score < 0.4]

        # ä¼˜åŠ¿å‘æŒ¥å»ºè®®
        if high_match_requirements:
            recommendations.append({
                'type': 'strength_leverage',
                'priority': 'high',
                'title': 'å‘æŒ¥æ ¸å¿ƒä¼˜åŠ¿',
                'description': f'å‘ç° {len(high_match_requirements)} ä¸ªé«˜åŒ¹é…åº¦éœ€æ±‚ï¼Œå»ºè®®åœ¨æŠ•æ ‡ä¸­é‡ç‚¹çªå‡ºè¿™äº›ä¼˜åŠ¿',
                'action_items': [
                    'åœ¨æ ‡ä¹¦ä¸­é‡ç‚¹å±•ç¤ºåŒ¹é…åº¦é«˜çš„èƒ½åŠ›',
                    'å‡†å¤‡ç›¸å…³çš„æˆåŠŸæ¡ˆä¾‹å’Œè¯æ˜ææ–™',
                    'å¼ºè°ƒè¿™äº›èƒ½åŠ›çš„ç‹¬ç‰¹ä»·å€¼'
                ],
                'affected_requirements': [req.requirement_id for req in high_match_requirements]
            })

        # æ”¹è¿›å»ºè®®
        if medium_match_requirements:
            recommendations.append({
                'type': 'capability_improvement',
                'priority': 'medium',
                'title': 'æå‡åŒ¹é…èƒ½åŠ›',
                'description': f'å‘ç° {len(medium_match_requirements)} ä¸ªä¸­ç­‰åŒ¹é…åº¦éœ€æ±‚ï¼Œå»ºè®®é’ˆå¯¹æ€§æå‡ç›¸å…³èƒ½åŠ›',
                'action_items': [
                    'åˆ†æä¸­ç­‰åŒ¹é…éœ€æ±‚çš„å…·ä½“è¦æ±‚',
                    'åˆ¶å®šèƒ½åŠ›æå‡è®¡åˆ’',
                    'è€ƒè™‘ä¸ä¸“ä¸šæœºæ„åˆä½œè¡¥å……èƒ½åŠ›'
                ],
                'affected_requirements': [req.requirement_id for req in medium_match_requirements]
            })

        # é£é™©åº”å¯¹å»ºè®®
        if low_match_requirements:
            recommendations.append({
                'type': 'risk_mitigation',
                'priority': 'high',
                'title': 'é™ä½æŠ•æ ‡é£é™©',
                'description': f'å‘ç° {len(low_match_requirements)} ä¸ªä½åŒ¹é…åº¦éœ€æ±‚ï¼Œå»ºè®®åˆ¶å®šé£é™©åº”å¯¹ç­–ç•¥',
                'action_items': [
                    'è¯„ä¼°å¤–åŒ…æˆ–åˆä½œçš„å¯èƒ½æ€§',
                    'é‡æ–°è¯„ä¼°éœ€æ±‚çš„å¿…è¦æ€§',
                    'åˆ¶å®šå¤‡é€‰è§£å†³æ–¹æ¡ˆ'
                ],
                'affected_requirements': [req.requirement_id for req in low_match_requirements]
            })

        # åŸºäºå·®è·åˆ†æçš„æ”¹è¿›å»ºè®®
        if gap_analysis['gap_summary']['total_gaps'] > 0:
            recommendations.append({
                'type': 'gap_addressal',
                'priority': 'medium',
                'title': 'å¼¥è¡¥èƒ½åŠ›å·®è·',
                'description': f'è¯†åˆ«å‡º {gap_analysis["gap_summary"]["total_gaps"]} ä¸ªèƒ½åŠ›å·®è·ï¼Œå»ºè®®åˆ¶å®šç³»ç»Ÿçš„æ”¹è¿›è®¡åˆ’',
                'action_items': self._generate_gap_action_items(gap_analysis['gap_types']),
                'affected_requirements': []
            })

        return recommendations

    def _generate_gap_action_items(self, gap_types: Dict[str, List]) -> List[str]:
        """ç”Ÿæˆå·®è·æ”¹è¿›è¡ŒåŠ¨é¡¹"""
        action_items = []

        if gap_types['experience_gaps']:
            action_items.append('åŠ å¼ºå›¢é˜Ÿç»éªŒåŸ¹å…»ï¼Œå¢åŠ é¡¹ç›®å®è·µæœºä¼š')

        if gap_types['capability_gaps']:
            action_items.append('æŠ•èµ„æ ¸å¿ƒèƒ½åŠ›å»ºè®¾ï¼Œæå‡æŠ€æœ¯å’ŒæœåŠ¡æ°´å¹³')

        if gap_types['certification_gaps']:
            action_items.append('è·å–å¿…è¦çš„è¡Œä¸šè®¤è¯å’Œèµ„è´¨è¯ä¹¦')

        if gap_types['resource_gaps']:
            action_items.append('ä¼˜åŒ–èµ„æºé…ç½®ï¼Œç¡®ä¿å…³é”®èµ„æºå¯ç”¨æ€§')

        return action_items

    def _calculate_overall_metrics(self, match_analysis: List[RequirementMatch]) -> Dict[str, Any]:
        """è®¡ç®—æ•´ä½“åŒ¹é…æŒ‡æ ‡"""
        if not match_analysis:
            return {
                'overall_match_score': 0,
                'coverage_rate': 0,
                'match_quality_distribution': {},
                'risk_distribution': {}
            }

        # æ•´ä½“åŒ¹é…åˆ†æ•°
        overall_match_score = sum(req.overall_match_score for req in match_analysis) / len(match_analysis)

        # è¦†ç›–ç‡
        covered_requirements = len([req for req in match_analysis if req.overall_match_score > 0])
        coverage_rate = covered_requirements / len(match_analysis)

        # åŒ¹é…è´¨é‡åˆ†å¸ƒ
        quality_distribution = {
            'excellent': len([req for req in match_analysis if req.overall_match_score >= 0.8]),
            'good': len([req for req in match_analysis if 0.6 <= req.overall_match_score < 0.8]),
            'fair': len([req for req in match_analysis if 0.4 <= req.overall_match_score < 0.6]),
            'poor': len([req for req in match_analysis if req.overall_match_score < 0.4])
        }

        # é£é™©åˆ†å¸ƒ
        risk_distribution = {
            'high': len([req for req in match_analysis if req.risk_assessment['risk_level'] == 'high']),
            'medium': len([req for req in match_analysis if req.risk_assessment['risk_level'] == 'medium']),
            'low': len([req for req in match_analysis if req.risk_assessment['risk_level'] == 'low'])
        }

        return {
            'overall_match_score': overall_match_score,
            'coverage_rate': coverage_rate,
            'match_quality_distribution': quality_distribution,
            'risk_distribution': risk_distribution,
            'recommendation_score': self._calculate_recommendation_score(overall_match_score, coverage_rate, risk_distribution)
        }

    def _calculate_recommendation_score(self, match_score: float, coverage_rate: float, risk_dist: Dict[str, int]) -> float:
        """è®¡ç®—å»ºè®®åˆ†æ•°"""
        # åŸºç¡€åˆ†æ•°
        base_score = (match_score * 0.5 + coverage_rate * 0.3)

        # é£é™©è°ƒæ•´
        total_risks = sum(risk_dist.values())
        if total_risks > 0:
            high_risk_ratio = risk_dist['high'] / total_risks
            risk_penalty = high_risk_ratio * 0.2
        else:
            risk_penalty = 0

        final_score = max(0, base_score - risk_penalty)

        return final_score

class MatchingError(Exception):
    """åŒ¹é…å¼‚å¸¸"""
    pass
```

è¿™ä»½è¯¦ç»†çš„AIç®—æ³•ä¸æ¨¡å‹è®¾è®¡æ–‡æ¡£æ¶µç›–äº†æ‹›æ ‡æ–‡æ¡£è§£æã€éœ€æ±‚åˆ†æå’Œæ™ºèƒ½åŒ¹é…ä¸‰å¤§æ ¸å¿ƒç®—æ³•æ¨¡å—ï¼Œä¸ºå¹³å°çš„AIèƒ½åŠ›æä¾›äº†å…·ä½“çš„æŠ€æœ¯å®ç°æ–¹æ¡ˆã€‚
TodoWrite
<arg_key>todos</arg_key>
<arg_value>[{"content": "ç»†åŒ–æŠ€æœ¯æ¶æ„å®ç°ç»†èŠ‚", "status": "completed", "activeForm": "ç»†åŒ–æŠ€æœ¯æ¶æ„å®ç°ç»†èŠ‚"}, {"content": "å®Œå–„AIç®—æ³•å’Œæ¨¡å‹è®¾è®¡", "status": "completed", "activeForm": "å®Œå–„AIç®—æ³•å’Œæ¨¡å‹è®¾è®¡"}, {"content": "è¯¦ç»†è®¾è®¡æ•°æ®æ¨¡å‹å’ŒAPIæ¥å£", "status": "in_progress", "activeForm": "è¯¦ç»†è®¾è®¡æ•°æ®æ¨¡å‹å’ŒAPIæ¥å£"}, {"content": "åˆ¶å®šè¯¦ç»†çš„å¸‚åœºæ¨å¹¿ç­–ç•¥", "status": "pending", "activeForm": "åˆ¶å®šè¯¦ç»†çš„å¸‚åœºæ¨å¹¿ç­–ç•¥"}, {"content": "è®¾è®¡è¯¦ç»†çš„è´¢åŠ¡æ¨¡å‹å’ŒKPIä½“ç³»", "status": "pending", "activeForm": "è®¾è®¡è¯¦ç»†çš„è´¢åŠ¡æ¨¡å‹å’ŒKPIä½“ç³»"}]