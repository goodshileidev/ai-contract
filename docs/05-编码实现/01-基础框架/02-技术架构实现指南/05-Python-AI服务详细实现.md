---
æ–‡æ¡£ç±»å‹: æ¶æ„æ–‡æ¡£
éœ€æ±‚ç¼–å·: DOC-2025-11-002
åˆ›å»ºæ—¥æœŸ: 2025-11-15
åˆ›å»ºè€…: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
æœ€åæ›´æ–°: 2025-11-26
æ›´æ–°è€…: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
çŠ¶æ€: å·²æ‰¹å‡†
---

# AIæ ‡ä¹¦æ™ºèƒ½åˆ›ä½œå¹³å° - æŠ€æœ¯æ¶æ„è¯¦ç»†å®ç° - ğŸ¤– Python AIæœåŠ¡è¯¦ç»†å®ç°

### FastAPIé¡¹ç›®ç»“æ„
```
python-ai-service/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                      # FastAPIåº”ç”¨å…¥å£
â”‚   â”œâ”€â”€ config/                      # é…ç½®
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py
â”‚   â”‚   â””â”€â”€ database.py
â”‚   â”œâ”€â”€ api/                         # APIè·¯ç”±
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ analyze.py
â”‚   â”‚   â”‚   â”œâ”€â”€ generate.py
â”‚   â”‚   â”‚   â””â”€â”€ match.py
â”‚   â”œâ”€â”€ services/                    # ä¸šåŠ¡é€»è¾‘å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â”œâ”€â”€ rag_service.py          # LlamaIndex RAGæœåŠ¡
â”‚   â”‚   â”œâ”€â”€ vector_service.py       # Elasticsearchå‘é‡æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ analysis_service.py
â”‚   â”‚   â””â”€â”€ generation_service.py
â”‚   â”œâ”€â”€ models/                      # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ request.py
â”‚   â”‚   â””â”€â”€ response.py
â”‚   â”œâ”€â”€ core/                        # æ ¸å¿ƒç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llama_index_config.py
â”‚   â”‚   â”œâ”€â”€ langchain_config.py
â”‚   â”‚   â””â”€â”€ elasticsearch_config.py
â”‚   â””â”€â”€ utils/                       # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ .env
```

### FastAPIä¸»åº”ç”¨
```python
# main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from contextlib import asynccontextmanager

from app.api.v1 import analyze, generate, match
from app.config.settings import settings
from app.core.llama_index_config import initialize_llama_index
from app.core.elasticsearch_config import initialize_elasticsearch

@asynccontextmanager
async def lifespan(app: FastAPI):
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    print("åˆå§‹åŒ–AIæœåŠ¡...")
    await initialize_llama_index()
    await initialize_elasticsearch()
    yield
    # å…³é—­æ—¶æ¸…ç†
    print("å…³é—­AIæœåŠ¡...")

app = FastAPI(
    title="AI Bid Composer - AI Service",
    description="AIèƒ½åŠ›æœåŠ¡ï¼Œæä¾›æ–‡æ¡£åˆ†æã€å†…å®¹ç”Ÿæˆç­‰æ™ºèƒ½åŠŸèƒ½",
    version="2.0.0",
    lifespan=lifespan
)

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Gzipå‹ç¼©
app.add_middleware(GZipMiddleware, minimum_size=1000)

# æ³¨å†Œè·¯ç”±
app.include_router(analyze.router, prefix="/api/v1/analyze", tags=["analyze"])
app.include_router(generate.router, prefix="/api/v1/generate", tags=["generate"])
app.include_router(match.router, prefix="/api/v1/match", tags=["match"])

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "ai-service",
        "version": "2.0.0"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8001,
        reload=settings.DEBUG
    )

# é…ç½® - config/settings.py
from pydantic_settings import BaseSettings
from typing import List

class Settings(BaseSettings):
    # åº”ç”¨é…ç½®
    APP_NAME: str = "AI Bid Composer"
    DEBUG: bool = False
    ALLOWED_ORIGINS: List[str] = ["http://localhost:3000"]

    # æ•°æ®åº“é…ç½®
    POSTGRES_URL: str = "postgresql://user:pass@postgres:5432/aibid"

    # Redisé…ç½®
    REDIS_URL: str = "redis://redis:6379/0"

    # Elasticsearché…ç½®
    ELASTICSEARCH_URL: str = "http://elasticsearch:9200"
    ELASTICSEARCH_INDEX_PREFIX: str = "bid_"

    # LLM APIé…ç½®
    OPENAI_API_KEY: str
    ANTHROPIC_API_KEY: str = ""
    ZHIPU_API_KEY: str = ""

    # æ¨¡å‹é…ç½®
    DEFAULT_LLM_MODEL: str = "gpt-4"
    DEFAULT_EMBEDDING_MODEL: str = "text-embedding-3-small"
    DEFAULT_TEMPERATURE: float = 0.1
    MAX_TOKENS: int = 4000

    # RAGé…ç½®
    CHUNK_SIZE: int = 512
    CHUNK_OVERLAP: int = 50
    SIMILARITY_TOP_K: int = 5

    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()
```

### LlamaIndex RAGæœåŠ¡å®ç°
```python
# services/rag_service.py
from typing import List, Dict, Any, Optional
from llama_index.core import (
    VectorStoreIndex,
    ServiceContext,
    StorageContext,
    Document,
    Settings
)
from llama_index.vector_stores.elasticsearch import ElasticsearchStore
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter

from app.config.settings import settings
from app.core.elasticsearch_config import get_elasticsearch_client

class RAGService:
    """åŸºäºLlamaIndexçš„RAGæœåŠ¡"""

    def __init__(self):
        self.es_client = get_elasticsearch_client()

        # é…ç½®LLM
        Settings.llm = OpenAI(
            model=settings.DEFAULT_LLM_MODEL,
            temperature=settings.DEFAULT_TEMPERATURE,
            api_key=settings.OPENAI_API_KEY
        )

        # é…ç½®Embeddingæ¨¡å‹
        Settings.embed_model = OpenAIEmbedding(
            model=settings.DEFAULT_EMBEDDING_MODEL,
            api_key=settings.OPENAI_API_KEY
        )

        # é…ç½®æ–‡æœ¬åˆ†å‰²å™¨
        Settings.node_parser = SentenceSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP
        )

        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        self.vector_store = ElasticsearchStore(
            index_name=f"{settings.ELASTICSEARCH_INDEX_PREFIX}knowledge",
            es_url=settings.ELASTICSEARCH_URL
        )

        # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
        self.storage_context = StorageContext.from_defaults(
            vector_store=self.vector_store
        )

        # åˆ›å»ºç´¢å¼•
        self.index = VectorStoreIndex.from_vector_store(
            vector_store=self.vector_store
        )

    async def analyze_tender(
        self,
        tender_doc: str,
        analysis_type: str = "comprehensive"
    ) -> Dict[str, Any]:
        """åˆ†ææ‹›æ ‡æ–‡æ¡£"""

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        query_engine = self.index.as_query_engine(
            similarity_top_k=settings.SIMILARITY_TOP_K,
            response_mode="tree_summarize"
        )

        # æ„å»ºåˆ†ææç¤º
        prompt = f"""
        è¯·è¯¦ç»†åˆ†æä»¥ä¸‹æ‹›æ ‡æ–‡æ¡£ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

        æ‹›æ ‡æ–‡æ¡£å†…å®¹ï¼š
        {tender_doc[:4000]}  # é™åˆ¶é•¿åº¦

        è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æï¼š
        1. é¡¹ç›®åŸºæœ¬ä¿¡æ¯ï¼ˆé¡¹ç›®åç§°ã€é¢„ç®—ã€æ—¶é—´ç­‰ï¼‰
        2. æŠ€æœ¯è¦æ±‚ï¼ˆæŠ€æœ¯è§„æ ¼ã€æ€§èƒ½æŒ‡æ ‡ç­‰ï¼‰
        3. å•†åŠ¡æ¡æ¬¾ï¼ˆä»˜æ¬¾æ–¹å¼ã€äº¤ä»˜è¦æ±‚ç­‰ï¼‰
        4. è¯„åˆ†æ ‡å‡†ï¼ˆæŠ€æœ¯åˆ†ã€å•†åŠ¡åˆ†ç­‰ï¼‰
        5. é£é™©å› ç´ ï¼ˆæŠ€æœ¯é£é™©ã€å•†åŠ¡é£é™©ç­‰ï¼‰
        6. æŠ•æ ‡å»ºè®®ï¼ˆç­–ç•¥å»ºè®®ã€æ³¨æ„äº‹é¡¹ç­‰ï¼‰

        è¯·ä»¥ç»“æ„åŒ–çš„JSONæ ¼å¼è¾“å‡ºç»“æœã€‚
        """

        # æ‰§è¡ŒæŸ¥è¯¢
        response = await query_engine.aquery(prompt)

        return {
            "analysis": response.response,
            "source_nodes": [
                {
                    "text": node.node.text,
                    "score": node.score,
                    "metadata": node.node.metadata
                }
                for node in response.source_nodes
            ],
            "confidence_score": self._calculate_confidence(response.source_nodes)
        }

    async def generate_content(
        self,
        requirements: Dict[str, Any],
        company_profile: Dict[str, Any],
        section: str,
        style: str = "professional"
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ ‡ä¹¦å†…å®¹"""

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        query_engine = self.index.as_query_engine(
            similarity_top_k=settings.SIMILARITY_TOP_K,
            response_mode="compact"
        )

        # æ„å»ºç”Ÿæˆæç¤º
        prompt = f"""
        è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆæ ‡ä¹¦çš„{section}éƒ¨åˆ†å†…å®¹ï¼š

        é¡¹ç›®è¦æ±‚ï¼š
        {requirements}

        å…¬å¸ä¿¡æ¯ï¼š
        {company_profile}

        ç”Ÿæˆè¦æ±‚ï¼š
        1. é£æ ¼ï¼š{style}ï¼ˆä¸“ä¸š/åˆ›æ–°/ä¿å®ˆï¼‰
        2. çªå‡ºå…¬å¸ä¼˜åŠ¿å’Œæ ¸å¿ƒç«äº‰åŠ›
        3. é’ˆå¯¹æ€§è§£å†³å®¢æˆ·éœ€æ±‚å’Œç—›ç‚¹
        4. ä½“ç°ä¸“ä¸šæ€§å’Œå¯ä¿¡åº¦
        5. ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†

        è¯·ç”Ÿæˆé«˜è´¨é‡ã€æœ‰è¯´æœåŠ›çš„å†…å®¹ã€‚
        """

        # æ‰§è¡ŒæŸ¥è¯¢ç”Ÿæˆ
        response = await query_engine.aquery(prompt)

        # è¯„ä¼°ç”Ÿæˆè´¨é‡
        quality_score = await self._assess_quality(response.response)

        return {
            "content": response.response,
            "quality_score": quality_score,
            "source_references": [
                node.node.text[:200] for node in response.source_nodes
            ]
        }

    async def index_documents(
        self,
        documents: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ç´¢å¼•æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""

        # è½¬æ¢ä¸ºLlamaIndex Documentå¯¹è±¡
        llama_docs = [
            Document(
                text=doc["content"],
                metadata=doc.get("metadata", {}),
                id_=doc.get("id")
            )
            for doc in documents
        ]

        # åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex.from_documents(
            llama_docs,
            storage_context=self.storage_context
        )

        return {
            "indexed_count": len(documents),
            "index_name": self.vector_store.index_name,
            "status": "success"
        }

    async def semantic_search(
        self,
        query: str,
        filters: Optional[Dict[str, Any]] = None,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æœç´¢"""

        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = self.index.as_retriever(
            similarity_top_k=top_k
        )

        # æ‰§è¡Œæ£€ç´¢
        nodes = await retriever.aretrieve(query)

        return [
            {
                "text": node.node.text,
                "score": node.score,
                "metadata": node.node.metadata,
                "id": node.node.id_
            }
            for node in nodes
        ]

    def _calculate_confidence(self, source_nodes) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
        if not source_nodes:
            return 0.0

        # åŸºäºæ£€ç´¢å¾—åˆ†è®¡ç®—ç½®ä¿¡åº¦
        avg_score = sum(node.score for node in source_nodes) / len(source_nodes)
        return min(avg_score, 1.0)

    async def _assess_quality(self, content: str) -> float:
        """è¯„ä¼°å†…å®¹è´¨é‡"""
        # ç®€å•çš„è´¨é‡è¯„ä¼°é€»è¾‘
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„è¯„ä¼°æ¨¡å‹

        factors = {
            "length": min(len(content) / 1000, 1.0) * 0.3,
            "structure": 0.4 if "\n\n" in content else 0.2,
            "professional": 0.3 if any(keyword in content for keyword in ["æŠ€æœ¯", "æ–¹æ¡ˆ", "å®æ–½"]) else 0.1
        }

        return sum(factors.values())

# è·å–RAGæœåŠ¡å®ä¾‹
_rag_service = None

def get_rag_service() -> RAGService:
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service
```

### æ–‡æ¡£åˆ†æAPI
```python
# api/v1/analyze.py
from fastapi import APIRouter, HTTPException, BackgroundTasks
from typing import List, Optional
import time

from app.models.request import DocumentAnalysisRequest
from app.models.response import DocumentAnalysisResponse, ApiResponse
from app.services.rag_service import get_rag_service
from app.services.analysis_service import get_analysis_service

router = APIRouter()

@router.post("/document", response_model=ApiResponse[DocumentAnalysisResponse])
async def analyze_document(
    request: DocumentAnalysisRequest,
    background_tasks: BackgroundTasks
):
    """
    åˆ†ææ‹›æ ‡æ–‡æ¡£

    - **document_id**: æ–‡æ¡£ID
    - **document_content**: æ–‡æ¡£å†…å®¹
    - **document_type**: æ–‡æ¡£ç±»å‹ï¼ˆtender/proposal/contractï¼‰
    - **analysis_options**: åˆ†æé€‰é¡¹åˆ—è¡¨
    """
    start_time = time.time()

    try:
        rag_service = get_rag_service()
        analysis_service = get_analysis_service()

        # æ‰§è¡Œæ–‡æ¡£åˆ†æ
        analysis_results = await rag_service.analyze_tender(
            tender_doc=request.document_content,
            analysis_type="comprehensive"
        )

        # æå–éœ€æ±‚å’Œé£é™©
        requirements = await analysis_service.extract_requirements(
            request.document_content
        )
        risks = await analysis_service.analyze_risks(
            request.document_content
        )

        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = time.time() - start_time

        response_data = DocumentAnalysisResponse(
            document_id=request.document_id,
            analysis_results={
                "main_analysis": analysis_results["analysis"],
                "requirements": requirements,
                "risks": risks,
                "source_nodes": analysis_results["source_nodes"]
            },
            confidence_score=analysis_results["confidence_score"],
            processing_time=processing_time
        )

        # åå°ä»»åŠ¡ï¼šä¿å­˜åˆ†æç»“æœ
        background_tasks.add_task(
            save_analysis_results,
            request.document_id,
            response_data.model_dump()
        )

        return ApiResponse(
            success=True,
            data=response_data,
            message="æ–‡æ¡£åˆ†æå®Œæˆ"
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æ–‡æ¡£åˆ†æå¤±è´¥: {str(e)}"
        )

async def save_analysis_results(document_id: str, results: dict):
    """åå°ä»»åŠ¡ï¼šä¿å­˜åˆ†æç»“æœ"""
    # ä¿å­˜åˆ°æ•°æ®åº“æˆ–ç¼“å­˜
    pass

# models/request.py
from pydantic import BaseModel, Field
from typing import List, Optional

class DocumentAnalysisRequest(BaseModel):
    document_id: str = Field(..., description="æ–‡æ¡£ID")
    document_content: str = Field(..., description="æ–‡æ¡£å†…å®¹")
    document_type: str = Field(..., description="æ–‡æ¡£ç±»å‹")
    analysis_options: List[str] = Field(
        default=["requirements", "risks", "opportunities"],
        description="åˆ†æé€‰é¡¹"
    )

# models/response.py
from pydantic import BaseModel, Field
from typing import Dict, Any, Optional, Generic, TypeVar

T = TypeVar('T')

class ApiResponse(BaseModel, Generic[T]):
    success: bool = Field(..., description="æ˜¯å¦æˆåŠŸ")
    data: Optional[T] = Field(None, description="å“åº”æ•°æ®")
    message: str = Field("", description="å“åº”æ¶ˆæ¯")
    timestamp: str = Field(
        default_factory=lambda: datetime.utcnow().isoformat(),
        description="æ—¶é—´æˆ³"
    )

class DocumentAnalysisResponse(BaseModel):
    document_id: str = Field(..., description="æ–‡æ¡£ID")
    analysis_results: Dict[str, Any] = Field(..., description="åˆ†æç»“æœ")
    confidence_score: float = Field(..., description="ç½®ä¿¡åº¦åˆ†æ•°")
    processing_time: float = Field(..., description="å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰")
```

### å†…å®¹ç”ŸæˆAPI
```python
# api/v1/generate.py
from fastapi import APIRouter, HTTPException, BackgroundTasks
from typing import Dict, Any
import time

from app.models.request import ContentGenerationRequest
from app.models.response import ContentGenerationResponse, ApiResponse
from app.services.rag_service import get_rag_service
from app.services.generation_service import get_generation_service

router = APIRouter()

@router.post("/content", response_model=ApiResponse[ContentGenerationResponse])
async def generate_content(
    request: ContentGenerationRequest,
    background_tasks: BackgroundTasks
):
    """
    ç”Ÿæˆæ ‡ä¹¦å†…å®¹

    - **project_id**: é¡¹ç›®ID
    - **template_id**: æ¨¡æ¿ID
    - **requirements**: é¡¹ç›®è¦æ±‚
    - **company_profile**: å…¬å¸ä¿¡æ¯
    - **generation_options**: ç”Ÿæˆé€‰é¡¹
    """
    start_time = time.time()

    try:
        rag_service = get_rag_service()
        generation_service = get_generation_service()

        # ç”Ÿæˆå„ä¸ªç« èŠ‚å†…å®¹
        sections = {}

        # ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        if "executive_summary" in request.generation_options.get("sections", []):
            sections["executive_summary"] = await rag_service.generate_content(
                requirements=request.requirements,
                company_profile=request.company_profile,
                section="æ‰§è¡Œæ‘˜è¦",
                style="professional"
            )

        # ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆ
        if "technical_proposal" in request.generation_options.get("sections", []):
            sections["technical_proposal"] = await rag_service.generate_content(
                requirements=request.requirements,
                company_profile=request.company_profile,
                section="æŠ€æœ¯æ–¹æ¡ˆ",
                style="professional"
            )

        # ç”Ÿæˆç®¡ç†æ–¹æ¡ˆ
        if "management_approach" in request.generation_options.get("sections", []):
            sections["management_approach"] = await rag_service.generate_content(
                requirements=request.requirements,
                company_profile=request.company_profile,
                section="é¡¹ç›®ç®¡ç†æ–¹æ¡ˆ",
                style="professional"
            )

        # è¯„ä¼°æ•´ä½“è´¨é‡
        quality_score = await generation_service.assess_overall_quality(sections)

        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        suggestions = await generation_service.generate_suggestions(
            sections,
            quality_score
        )

        processing_time = time.time() - start_time

        response_data = ContentGenerationResponse(
            generated_content={
                "sections": sections,
                "generation_metadata": {
                    "project_id": request.project_id,
                    "template_id": request.template_id,
                    "timestamp": datetime.utcnow().isoformat()
                }
            },
            quality_score=quality_score,
            suggestions=suggestions,
            processing_time=processing_time
        )

        return ApiResponse(
            success=True,
            data=response_data,
            message="å†…å®¹ç”Ÿæˆå®Œæˆ"
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}"
        )
```
