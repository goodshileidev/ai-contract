---
æ–‡æ¡£ç±»å‹: æ¶æ„æ–‡æ¡£
éœ€æ±‚ç¼–å·: DOC-2025-11-001
åˆ›å»ºæ—¥æœŸ: 2025-11-29
åˆ›å»ºè€…: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
æœ€åæ›´æ–°: 2025-11-29
æ›´æ–°è€…: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
çŠ¶æ€: å·²æ‰¹å‡†
---

# AIæ ‡ä¹¦æ™ºèƒ½åˆ›ä½œå¹³å° - å‘é‡æ£€ç´¢æœåŠ¡

> **ä¸»åŠ›æ–¹æ¡ˆ**: Elasticsearch 8.x+ (kNNå‘é‡æ£€ç´¢)
> **å¤‡é€‰æ–¹æ¡ˆ**: Pineconeäº‘æœåŠ¡

## ğŸ“‹ æ–‡æ¡£å¯¼èˆª

æœ¬æ–‡æ¡£æ˜¯AIèƒ½åŠ›å±‚è®¾è®¡çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä»–ç›¸å…³æ–‡æ¡£ï¼š

1. [00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md](./00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md) - AIèƒ½åŠ›çŸ©é˜µå’ŒæŠ€æœ¯æ ˆæ¦‚è§ˆ
2. [01-LLMæœåŠ¡æ¶æ„.md](./01-LLMæœåŠ¡æ¶æ„.md) - LLMå®¢æˆ·ç«¯ã€Promptç®¡ç†ã€ä»»åŠ¡é˜Ÿåˆ—
3. **02-å‘é‡æ£€ç´¢æœåŠ¡.md**ï¼ˆæœ¬æ–‡æ¡£ï¼‰- å‘é‡åµŒå…¥æœåŠ¡ã€Elasticsearchã€Pineconeå­˜å‚¨
4. [03-çŸ¥è¯†å›¾è°±æœåŠ¡.md](./03-çŸ¥è¯†å›¾è°±æœåŠ¡.md) - çŸ¥è¯†å›¾è°±ç®¡ç†ã€æ™ºèƒ½åŒ¹é…å¼•æ“
5. [04-å·¥ä½œæµä¸ä¼˜åŒ–.md](./04-å·¥ä½œæµä¸ä¼˜åŒ–.md) - å·¥ä½œæµç¼–æ’ã€æˆæœ¬ä¼˜åŒ–ã€æ€§èƒ½ä¼˜åŒ–

## ğŸ” å‘é‡æ£€ç´¢æœåŠ¡

### 1. å‘é‡åµŒå…¥æœåŠ¡

```python
# app/services/ai/embedding_service.py
# Python AIæœåŠ¡ - å‘é‡åµŒå…¥
from typing import List, Dict, Any
import openai
from llama_index.embeddings import OpenAIEmbedding, HuggingFaceEmbedding

class EmbeddingService:
    """å‘é‡åµŒå…¥æœåŠ¡"""

    def __init__(self):
        self.openai_embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            openai_api_key=settings.OPENAI_API_KEY
        )

        self.huggingface_embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )

    async def embed_text(
        self,
        text: str,
        provider: str = "openai"
    ) -> List[float]:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡

        Args:
            text: æ–‡æœ¬å†…å®¹
            provider: åµŒå…¥æ¨¡å‹æä¾›å•†

        Returns:
            å‘é‡æ•°ç»„
        """
        if provider == "openai":
            return await self.openai_embeddings.aembed_query(text)
        elif provider == "huggingface":
            return self.huggingface_embeddings.embed_query(text)
        else:
            raise ValueError(f"Unknown provider: {provider}")

    async def embed_documents(
        self,
        documents: List[str],
        provider: str = "openai"
    ) -> List[List[float]]:
        """
        æ‰¹é‡åµŒå…¥æ–‡æ¡£

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            provider: åµŒå…¥æ¨¡å‹æä¾›å•†

        Returns:
            å‘é‡æ•°ç»„åˆ—è¡¨
        """
        if provider == "openai":
            return await self.openai_embeddings.aembed_documents(documents)
        elif provider == "huggingface":
            return self.huggingface_embeddings.embed_documents(documents)
        else:
            raise ValueError(f"Unknown provider: {provider}")
```

### 2. Elasticsearchå‘é‡å­˜å‚¨æœåŠ¡ï¼ˆä¸»åŠ›æ–¹æ¡ˆï¼‰

```python
# app/services/ai/elasticsearch_store.py
# Python AIæœåŠ¡ - Elasticsearchå‘é‡å­˜å‚¨ï¼ˆä¸»åŠ›ï¼‰
from typing import List, Dict, Any, Optional
from elasticsearch import AsyncElasticsearch
from llama_index.vector_stores import ElasticsearchStore
from llama_index import VectorStoreIndex, ServiceContext

class ElasticsearchVectorStore:
    """Elasticsearchå‘é‡å­˜å‚¨æœåŠ¡ï¼ˆä¸»åŠ›æ–¹æ¡ˆï¼‰"""

    def __init__(self):
        # åˆå§‹åŒ–Elasticsearchè¿æ¥
        self.es_client = AsyncElasticsearch(
            hosts=[settings.ELASTICSEARCH_URL],
            basic_auth=(settings.ELASTICSEARCH_USER, settings.ELASTICSEARCH_PASSWORD),
            verify_certs=True,
            ssl_show_warn=False
        )

        self.index_name = "aibidcomposer-vectors"

        # ä½¿ç”¨LlamaIndexçš„ElasticsearchStore
        self.vector_store = ElasticsearchStore(
            es_client=self.es_client,
            index_name=self.index_name,
            vector_field="embedding",
            text_field="content",
            metadata_fields=["source", "document_id", "created_at"]
        )

    async def initialize_index(self):
        """åˆå§‹åŒ–ç´¢å¼•"""
        # åˆ›å»ºç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not await self.es_client.indices.exists(index=self.index_name):
            await self.es_client.indices.create(
                index=self.index_name,
                body={
                    "mappings": {
                        "properties": {
                            "content": {"type": "text"},
                            "embedding": {
                                "type": "dense_vector",
                                "dims": 1536,  # OpenAI embeddingç»´åº¦
                                "index": True,
                                "similarity": "cosine"
                            },
                            "source": {"type": "keyword"},
                            "document_id": {"type": "keyword"},
                            "created_at": {"type": "date"}
                        }
                    }
                }
            )

    async def add_documents(
        self,
        documents: List[Dict[str, Any]],
        embeddings: List[List[float]]
    ) -> None:
        """
        æ·»åŠ æ–‡æ¡£åˆ°Elasticsearchå‘é‡åº“

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            embeddings: å‘é‡åˆ—è¡¨
        """
        from elasticsearch.helpers import async_bulk

        actions = []
        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):
            actions.append({
                "_index": self.index_name,
                "_id": doc.get("id", f"doc_{i}"),
                "_source": {
                    "content": doc.get("text", ""),
                    "embedding": embedding,
                    "source": doc.get("source", ""),
                    "document_id": doc.get("document_id", ""),
                    "created_at": doc.get("created_at", ""),
                    **doc.get("metadata", {})
                }
            })

        # æ‰¹é‡æ’å…¥
        await async_bulk(self.es_client, actions)

    async def search(
        self,
        query_embedding: List[float],
        top_k: int = 10,
        filter_dict: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        å‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼ˆä½¿ç”¨Elasticsearch kNNï¼‰

        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›ç»“æœæ•°é‡
            filter_dict: è¿‡æ»¤æ¡ä»¶

        Returns:
            æœç´¢ç»“æœ
        """
        # æ„å»ºæŸ¥è¯¢
        query = {
            "knn": {
                "field": "embedding",
                "query_vector": query_embedding,
                "k": top_k,
                "num_candidates": top_k * 10
            }
        }

        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filter_dict:
            query["knn"]["filter"] = {
                "bool": {
                    "must": [
                        {"term": {k: v}} for k, v in filter_dict.items()
                    ]
                }
            }

        # æ‰§è¡Œæœç´¢
        response = await self.es_client.search(
            index=self.index_name,
            body=query,
            size=top_k
        )

        return [
            {
                "id": hit["_id"],
                "score": hit["_score"],
                "content": hit["_source"]["content"],
                "metadata": {
                    k: v for k, v in hit["_source"].items()
                    if k not in ["content", "embedding"]
                }
            }
            for hit in response["hits"]["hits"]
        ]

    async def semantic_search(
        self,
        query_text: str,
        top_k: int = 10,
        filter_dict: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢ï¼ˆæ–‡æœ¬æŸ¥è¯¢ï¼‰

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            filter_dict: è¿‡æ»¤æ¡ä»¶

        Returns:
            æœç´¢ç»“æœ
        """
        # è·å–åµŒå…¥æœåŠ¡
        embedding_service = EmbeddingService()

        # å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        query_embedding = await embedding_service.embed_text(query_text)

        # æ‰§è¡Œæœç´¢
        return await self.search(
            query_embedding=query_embedding,
            top_k=top_k,
            filter_dict=filter_dict
        )

    async def close(self):
        """å…³é—­è¿æ¥"""
        await self.es_client.close()
```

### 3. Pineconeå‘é‡å­˜å‚¨æœåŠ¡ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰

```python
# app/services/ai/pinecone_store.py
# Python AIæœåŠ¡ - Pineconeå‘é‡å­˜å‚¨ï¼ˆå¤‡é€‰ï¼‰
from typing import List, Dict, Any, Optional
import pinecone
from llama_index.vector_stores import PineconeVectorStore

class PineconeVectorStore:
    """Pineconeå‘é‡å­˜å‚¨æœåŠ¡ï¼ˆäº‘ç«¯å¤‡é€‰æ–¹æ¡ˆï¼‰"""

    def __init__(self):
        # åˆå§‹åŒ–Pinecone
        pinecone.init(
            api_key=settings.PINECONE_API_KEY,
            environment=settings.PINECONE_ENVIRONMENT
        )

        self.index_name = "aibidcomposer"

        # åˆ›å»ºæˆ–è·å–ç´¢å¼•
        if self.index_name not in pinecone.list_indexes():
            pinecone.create_index(
                name=self.index_name,
                dimension=1536,  # OpenAI embeddingç»´åº¦
                metric="cosine"
            )

        self.pinecone_index = pinecone.Index(self.index_name)

        # ä½¿ç”¨LlamaIndexçš„PineconeVectorStore
        self.vector_store = PineconeVectorStore(
            pinecone_index=self.pinecone_index
        )
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **AIèƒ½åŠ›å±‚æ€»è§ˆ**: [00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md](./00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md)
- **LLMæœåŠ¡æ¶æ„**: [01-LLMæœåŠ¡æ¶æ„.md](./01-LLMæœåŠ¡æ¶æ„.md)
- **çŸ¥è¯†å›¾è°±æœåŠ¡**: [03-çŸ¥è¯†å›¾è°±æœåŠ¡.md](./03-çŸ¥è¯†å›¾è°±æœåŠ¡.md)
- **å·¥ä½œæµä¸ä¼˜åŒ–**: [04-å·¥ä½œæµä¸ä¼˜åŒ–.md](./04-å·¥ä½œæµä¸ä¼˜åŒ–.md)

---

## ä¿®æ”¹å†å²

| æ—¥æœŸ | ç‰ˆæœ¬ | ä¿®æ”¹è€… | ä¿®æ”¹å†…å®¹æ¦‚è¦ |
|------|------|--------|-------------|
| 2025-11-29 | 1.0 | claude-sonnet-4-5 (claude-sonnet-4-5-20250929) | ä»05-AIèƒ½åŠ›å±‚è®¾è®¡.mdæ‹†åˆ†åˆ›å»ºå‘é‡æ£€ç´¢æœåŠ¡æ–‡æ¡£ |

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´11æœˆ29æ—¥
**æ–‡æ¡£çŠ¶æ€**: âœ… å·²æ‰¹å‡†
