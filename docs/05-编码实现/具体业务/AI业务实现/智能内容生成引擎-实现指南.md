# Python FastAPI AI æœåŠ¡ä»»åŠ¡è¯¦ç»†è®¡åˆ’ - AI-002

**æ–‡æ¡£ç±»å‹**: å®æ–½æ–‡æ¡£
**éœ€æ±‚ç¼–å·**: REQ-AI-002
**åˆ›å»ºæ—¥æœŸ**: 2025-11-26
**åˆ›å»ºè€…**: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
**æœ€åæ›´æ–°**: 2025-11-27
**æ›´æ–°è€…**: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
**çŠ¶æ€**: å¾…å¼€å§‹

---

## ä¿®æ”¹å†å²

| æ—¥æœŸ | ç‰ˆæœ¬ | ä¿®æ”¹è€… | ä¿®æ”¹å†…å®¹æ¦‚è¦ |
|------|------|--------|-------------|
| 2025-11-27 14:55 | 2.0 | claude-sonnet-4-5 | ä» task-plan-python-ai-è¯¦ç»†.md æ‹†åˆ†å‡º AI-002 æ¨¡å— |
| 2025-11-26 | 1.0 | claude-sonnet-4-5 | åˆ›å»ºPython AIæœåŠ¡è¯¦ç»†ä»»åŠ¡è®¡åˆ’ |

---

## ğŸ“‘ æ–‡æ¡£å¯¼èˆª

**è¿”å›ç´¢å¼•**: [task-plan-python-ai-è¯¦ç»†-INDEX.md](./task-plan-python-ai-è¯¦ç»†-INDEX.md)

**å…¶ä»–æ¨¡å—**: [AI-001](./task-plan-python-ai-è¯¦ç»†-AI-001.md) | [AI-003](./task-plan-python-ai-è¯¦ç»†-AI-003.md) | [AI-004](./task-plan-python-ai-è¯¦ç»†-AI-004.md)

---

## AI-002: æ™ºèƒ½å†…å®¹ç”Ÿæˆå¼•æ“

**éœ€æ±‚ç¼–å·**: REQ-AI-002
**è´Ÿè´£äºº**: Python AI å¼€å‘
**ä¼˜å…ˆçº§**: P1 - é«˜ä¼˜å…ˆçº§
**å¼€å§‹æ—¶é—´**: YYYY-MM-DD
**é¢„è®¡å®Œæˆ**: YYYY-MM-DD
**å®é™…å®Œæˆ**: -
**å½“å‰çŠ¶æ€**: â¸ï¸ å¾…å¼€å§‹
**å®Œæˆè¿›åº¦**: 0% (0/5 å­ä»»åŠ¡)

### æ¨¡å—æ¦‚è¿°

æ™ºèƒ½å†…å®¹ç”Ÿæˆå¼•æ“æ˜¯å¹³å°çš„æ ¸å¿ƒAIèƒ½åŠ›ï¼Œè´Ÿè´£ï¼š
- åŸºäºRAGæŠ€æœ¯ç”Ÿæˆé«˜è´¨é‡æ ‡ä¹¦å†…å®¹
- å¤šé¢†åŸŸAIåŠ©æ‰‹çŸ©é˜µæä¾›ä¸“ä¸šå»ºè®®
- å†…å®¹ä¼˜åŒ–å’Œè´¨é‡ä¿è¯

**æŠ€æœ¯æ ˆ**: Python 3.11 + FastAPI + LlamaIndex (ä¸»åŠ›) + GPT-4 + Elasticsearch
**é¢„è®¡æ€»å·¥ä½œé‡**: 22 äººå¤©

---

### äºŒçº§ä»»åŠ¡ 2.1: RAGç³»ç»Ÿæ„å»º

**é¢„è®¡å·¥ä½œé‡**: 5 äººå¤©
**å®Œæˆè¿›åº¦**: 0% (0/5 ç±»åˆ«)

#### 2.1.1 æ•°æ®å®šä¹‰

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] å®šä¹‰çŸ¥è¯†åº“æ–‡æ¡£æ•°æ®æ¨¡å‹
  ```python
  # apps/backend-python/app/models/knowledge_base.py
  from pydantic import BaseModel
  from typing import List, Dict, Any, Optional
  from datetime import datetime

  class KnowledgeDocument(BaseModel):
      """çŸ¥è¯†åº“æ–‡æ¡£"""
      doc_id: str
      doc_type: str  # 'product'|'case'|'personnel'|'certification'
      title: str
      content: str
      metadata: Dict[str, Any]
      embedding: Optional[List[float]]  # å‘é‡è¡¨ç¤º
      created_at: datetime

  class RAGContext(BaseModel):
      """RAGä¸Šä¸‹æ–‡"""
      query: str
      retrieved_docs: List[KnowledgeDocument]
      relevance_scores: List[float]
      total_retrieved: int
  ```

- [ ] è®¾è®¡PostgreSQLçŸ¥è¯†åº“è¡¨ï¼ˆJavaæœåŠ¡è´Ÿè´£ï¼‰
  ```sql
  -- JavaæœåŠ¡ç®¡ç†çš„ä¼ä¸šèƒ½åŠ›è¡¨
  CREATE TABLE company_capabilities (
      id UUID PRIMARY KEY,
      organization_id UUID NOT NULL,
      capability_type VARCHAR(50),  -- 'product'|'service'|'case'|'certificate'
      title VARCHAR(200) NOT NULL,
      description TEXT,
      content TEXT,  -- è¯¦ç»†å†…å®¹
      tags TEXT[],
      embedding_status VARCHAR(20),  -- 'pending'|'completed'
      created_at TIMESTAMP WITH TIME ZONE,
      FOREIGN KEY (organization_id) REFERENCES organizations(id)
  );
  ```

#### 2.1.2 å‰ç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] åˆ›å»ºçŸ¥è¯†åº“ç®¡ç†é¡µé¢
  ```typescript
  // apps/frontend/src/pages/knowledge/KnowledgeBase.tsx
  import { ProTable } from '@ant-design/pro-table';
  import { Button, Tag } from 'antd';

  export default function KnowledgeBase() {
      const columns = [
          { title: 'ID', dataIndex: 'id', width: 100 },
          { title: 'ç±»å‹', dataIndex: 'capability_type', render: (type) => (
              <Tag color={type === 'product' ? 'blue' : 'green'}>{type}</Tag>
          )},
          { title: 'æ ‡é¢˜', dataIndex: 'title' },
          { title: 'å‘é‡åŒ–çŠ¶æ€', dataIndex: 'embedding_status', render: (status) => (
              <Tag color={status === 'completed' ? 'success' : 'processing'}>{status}</Tag>
          )},
          { title: 'åˆ›å»ºæ—¶é—´', dataIndex: 'created_at', valueType: 'dateTime' },
          {
              title: 'æ“ä½œ',
              render: (_, record) => (
                  <>
                      <Button onClick={() => handleVectorize(record.id)}>å‘é‡åŒ–</Button>
                      <Button onClick={() => handleEdit(record)}>ç¼–è¾‘</Button>
                  </>
              )
          }
      ];

      const handleVectorize = async (id: string) => {
          // è°ƒç”¨PythonæœåŠ¡å‘é‡åŒ–API
          await fetch(`http://localhost:8001/api/v1/ai/vectorize-capability/${id}`, {
              method: 'POST'
          });
          message.success('å‘é‡åŒ–ä»»åŠ¡å·²æäº¤');
      };

      return (
          <ProTable
              columns={columns}
              request={async (params) => {
                  // ä»JavaæœåŠ¡è·å–çŸ¥è¯†åº“åˆ—è¡¨
                  const response = await fetch('http://localhost:8080/api/v1/capabilities');
                  return response.json();
              }}
              rowKey="id"
              search={false}
          />
      );
  }
  ```

- [ ] åˆ›å»ºRAGæ£€ç´¢æµ‹è¯•é¡µé¢
  ```typescript
  // apps/frontend/src/pages/knowledge/RAGTest.tsx
  import { ProForm, ProFormText, ProFormDigit } from '@ant-design/pro-form';

  export function RAGTest() {
      const [retrievedDocs, setRetrievedDocs] = useState([]);

      const testRetrieval = async (values: any) => {
          const response = await fetch('http://localhost:8001/api/v1/ai/rag-retrieve', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                  query: values.query,
                  top_k: values.top_k || 5
              })
          });

          const result = await response.json();
          setRetrievedDocs(result.retrieved_docs);
      };

      return (
          <div>
              <ProForm onFinish={testRetrieval}>
                  <ProFormText name="query" label="æŸ¥è¯¢" placeholder="è¾“å…¥æŸ¥è¯¢å†…å®¹..." />
                  <ProFormDigit name="top_k" label="è¿”å›æ•°é‡" initialValue={5} />
              </ProForm>

              <div>
                  <h3>æ£€ç´¢ç»“æœ:</h3>
                  {retrievedDocs.map((doc, idx) => (
                      <Card key={idx} style={{ marginBottom: 16 }}>
                          <p><strong>æ ‡é¢˜:</strong> {doc.title}</p>
                          <p><strong>ç›¸å…³åº¦:</strong> {doc.relevance_score.toFixed(2)}</p>
                          <p>{doc.content.substring(0, 200)}...</p>
                      </Card>
                  ))}
              </div>
          </div>
      );
  }
  ```

#### 2.1.3 Javaåç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] åˆ›å»º `CompanyCapability` å®ä½“
  ```java
  // apps/backend-java/src/main/java/com/aibidcomposer/entity/CompanyCapability.java
  @Data
  @TableName("company_capabilities")
  public class CompanyCapability extends BaseEntity {
      @TableField("organization_id")
      private String organizationId;

      @TableField("capability_type")
      private String capabilityType;  // 'product'|'service'|'case'|'certificate'

      @TableField("title")
      private String title;

      @TableField("description")
      private String description;

      @TableField("content")
      private String content;  // è¯¦ç»†å†…å®¹ï¼ˆä¾›RAGä½¿ç”¨ï¼‰

      @TableField("tags")
      private String tags;  // JSONæ•°ç»„

      @TableField("embedding_status")
      private String embeddingStatus;  // 'pending'|'processing'|'completed'
  }
  ```

- [ ] åˆ›å»º `CapabilityService`
  ```java
  // apps/backend-java/src/main/java/com/aibidcomposer/service/CapabilityService.java
  @Service
  @RequiredArgsConstructor
  public class CapabilityService {

      private final CapabilityMapper capabilityMapper;
      private final RabbitTemplate rabbitTemplate;

      /**
       * åˆ›å»ºä¼ä¸šèƒ½åŠ›è®°å½•
       * éœ€æ±‚ç¼–å·: REQ-AI-002
       */
      public CompanyCapability create(CreateCapabilityRequest request) {
          CompanyCapability capability = new CompanyCapability();
          capability.setOrganizationId(request.getOrganizationId());
          capability.setCapabilityType(request.getCapabilityType());
          capability.setTitle(request.getTitle());
          capability.setContent(request.getContent());
          capability.setEmbeddingStatus("pending");

          capabilityMapper.insert(capability);

          // å‘é€RabbitMQæ¶ˆæ¯é€šçŸ¥PythonæœåŠ¡è¿›è¡Œå‘é‡åŒ–
          rabbitTemplate.convertAndSend("ai.vectorize.queue", capability.getId());

          return capability;
      }

      /**
       * æ›´æ–°å‘é‡åŒ–çŠ¶æ€ï¼ˆPythonæœåŠ¡å›è°ƒï¼‰
       */
      public void updateEmbeddingStatus(String capabilityId, String status) {
          CompanyCapability capability = capabilityMapper.selectById(capabilityId);
          capability.setEmbeddingStatus(status);
          capabilityMapper.updateById(capability);
      }

      /**
       * æŸ¥è¯¢æ‰€æœ‰å¾…å‘é‡åŒ–çš„èƒ½åŠ›
       */
      public List<CompanyCapability> getPendingEmbedding(String organizationId) {
          LambdaQueryWrapper<CompanyCapability> wrapper = new LambdaQueryWrapper<>();
          wrapper.eq(CompanyCapability::getOrganizationId, organizationId)
                 .eq(CompanyCapability::getEmbeddingStatus, "pending");
          return capabilityMapper.selectList(wrapper);
      }
  }
  ```

- [ ] åˆ›å»ºREST API
  ```java
  @RestController
  @RequestMapping("/api/v1/capabilities")
  @RequiredArgsConstructor
  public class CapabilityController {

      private final CapabilityService capabilityService;

      @PostMapping
      public Result<CompanyCapability> create(@RequestBody CreateCapabilityRequest request) {
          CompanyCapability capability = capabilityService.create(request);
          return Result.success(capability);
      }

      @GetMapping
      public Result<Page<CompanyCapability>> list(
          @RequestParam String organizationId,
          @RequestParam(defaultValue = "1") int page,
          @RequestParam(defaultValue = "20") int pageSize
      ) {
          // åˆ†é¡µæŸ¥è¯¢
          Page<CompanyCapability> result = capabilityService.list(organizationId, page, pageSize);
          return Result.success(result);
      }

      @PutMapping("/{id}/embedding-status")
      public Result<Void> updateEmbeddingStatus(
          @PathVariable String id,
          @RequestParam String status
      ) {
          // PythonæœåŠ¡å›è°ƒæ›´æ–°å‘é‡åŒ–çŠ¶æ€
          capabilityService.updateEmbeddingStatus(id, status);
          return Result.success();
      }
  }
  ```

#### 2.1.4 Pythonåç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] å®ç° LlamaIndex RAG æœåŠ¡
  ```python
  # apps/backend-python/app/services/ai/rag_service.py
  from llama_index import VectorStoreIndex, ServiceContext, Document
  from llama_index.vector_stores import ElasticsearchStore
  from llama_index.llms import OpenAI
  from llama_index.embeddings import OpenAIEmbedding
  from typing import List, Dict, Any
  from app.services.ai.elasticsearch_store import ElasticsearchVectorStore

  class RAGService:
      """RAGæ£€ç´¢å¢å¼ºç”ŸæˆæœåŠ¡ï¼ˆåŸºäºLlamaIndexï¼‰"""

      def __init__(self):
          # åˆå§‹åŒ–LLM
          self.llm = OpenAI(
              model="gpt-4-turbo-preview",
              api_key=settings.OPENAI_API_KEY,
              temperature=0.7
          )

          # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
          self.embed_model = OpenAIEmbedding(api_key=settings.OPENAI_API_KEY)

          # åˆå§‹åŒ–Elasticsearchå‘é‡å­˜å‚¨
          self.es_store = ElasticsearchVectorStore()

          # åˆ›å»ºæœåŠ¡ä¸Šä¸‹æ–‡
          self.service_context = ServiceContext.from_defaults(
              llm=self.llm,
              embed_model=self.embed_model
          )

      async def build_index(self, documents: List[Dict[str, Any]]) -> VectorStoreIndex:
          """
          æ„å»ºå‘é‡ç´¢å¼•
          éœ€æ±‚ç¼–å·: REQ-AI-002
          """
          # 1. è½¬æ¢ä¸ºLlamaIndex Documentå¯¹è±¡
          llama_docs = []
          for doc in documents:
              llama_docs.append(Document(
                  text=doc['content'],
                  metadata={
                      'doc_id': doc['id'],
                      'title': doc['title'],
                      'doc_type': doc['type']
                  }
              ))

          # 2. ä½¿ç”¨Elasticsearchä½œä¸ºå‘é‡å­˜å‚¨æ„å»ºç´¢å¼•
          index = VectorStoreIndex.from_documents(
              llama_docs,
              service_context=self.service_context,
              vector_store=self.es_store.vector_store  # ä½¿ç”¨Elasticsearch
          )

          return index

      async def retrieve(
          self,
          query: str,
          top_k: int = 5,
          organization_id: Optional[str] = None
      ) -> RAGContext:
          """
          RAGæ£€ç´¢
          éœ€æ±‚ç¼–å·: REQ-AI-002
          """
          # 1. ä»JavaæœåŠ¡è·å–ç»„ç»‡çš„æ‰€æœ‰èƒ½åŠ›æ•°æ®
          capabilities = await self.java_client.get_capabilities(organization_id)

          # 2. æ„å»ºç´¢å¼•
          index = await self.build_index(capabilities)

          # 3. åˆ›å»ºæŸ¥è¯¢å¼•æ“
          query_engine = index.as_query_engine(
              similarity_top_k=top_k,
              response_mode="compact"  # ç´§å‡‘æ¨¡å¼ï¼Œåªè¿”å›ç›¸å…³å†…å®¹
          )

          # 4. æ‰§è¡Œæ£€ç´¢
          response = await query_engine.aquery(query)

          # 5. æå–æ£€ç´¢åˆ°çš„æ–‡æ¡£
          retrieved_docs = []
          for node in response.source_nodes:
              retrieved_docs.append({
                  'doc_id': node.metadata.get('doc_id'),
                  'title': node.metadata.get('title'),
                  'content': node.text,
                  'relevance_score': node.score
              })

          return RAGContext(
              query=query,
              retrieved_docs=retrieved_docs,
              relevance_scores=[node.score for node in response.source_nodes],
              total_retrieved=len(retrieved_docs)
          )

      async def generate_with_rag(
          self,
          query: str,
          context: RAGContext,
          generation_prompt: str
      ) -> str:
          """
          åŸºäºRAGä¸Šä¸‹æ–‡ç”Ÿæˆå†…å®¹
          éœ€æ±‚ç¼–å·: REQ-AI-002
          """
          # 1. æ„å»ºå¢å¼ºPrompt
          context_text = "\n\n".join([
              f"[{doc['title']}]\n{doc['content']}"
              for doc in context.retrieved_docs
          ])

          full_prompt = f"""
{generation_prompt}

å‚è€ƒä¸Šä¸‹æ–‡ï¼š
{context_text}

ç”¨æˆ·éœ€æ±‚ï¼š
{query}

è¯·åŸºäºä»¥ä¸Šå‚è€ƒä¸Šä¸‹æ–‡ç”Ÿæˆå†…å®¹ã€‚
"""

          # 2. è°ƒç”¨LLMç”Ÿæˆ
          response = await self.llm.acomplete(full_prompt)

          return response.text
  ```

- [ ] åˆ›å»ºå‘é‡åŒ–Celeryä»»åŠ¡
  ```python
  # apps/backend-python/app/tasks/vectorization.py
  from app.tasks.celery_app import celery_app
  from app.services.ai.rag_service import RAGService
  from app.clients.java_api_client import JavaAPIClient

  @celery_app.task(bind=True, max_retries=3)
  def vectorize_capability_task(self: Task, capability_id: str):
      """
      å‘é‡åŒ–ä¼ä¸šèƒ½åŠ›ä»»åŠ¡
      éœ€æ±‚ç¼–å·: REQ-AI-002
      """
      try:
          java_client = JavaAPIClient()

          # 1. æ›´æ–°JavaæœåŠ¡ï¼šçŠ¶æ€=processing
          await java_client.update_embedding_status(capability_id, "processing")

          # 2. è·å–èƒ½åŠ›æ•°æ®
          capability = await java_client.get_capability(capability_id)

          # 3. å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°Elasticsearch
          rag_service = RAGService()
          await rag_service.build_index([{
              'id': capability['id'],
              'title': capability['title'],
              'content': capability['content'],
              'type': capability['capability_type']
          }])

          # 4. æ›´æ–°JavaæœåŠ¡ï¼šçŠ¶æ€=completed
          await java_client.update_embedding_status(capability_id, "completed")

          return {"status": "success", "capability_id": capability_id}

      except Exception as e:
          await java_client.update_embedding_status(capability_id, "failed")
          if self.request.retries < self.max_retries:
              raise self.retry(exc=e, countdown=60)
          raise
  ```

- [ ] åˆ›å»ºRAG APIç«¯ç‚¹
  ```python
  # apps/backend-python/app/api/v1/rag.py
  from fastapi import APIRouter, HTTPException
  from pydantic import BaseModel

  router = APIRouter(prefix="/api/v1/ai", tags=["RAG"])

  class RAGRetrieveRequest(BaseModel):
      query: str
      top_k: int = 5
      organization_id: Optional[str] = None

  @router.post("/rag-retrieve")
  async def rag_retrieve(request: RAGRetrieveRequest):
      """
      RAGæ£€ç´¢
      éœ€æ±‚ç¼–å·: REQ-AI-002
      """
      rag_service = RAGService()
      context = await rag_service.retrieve(
          query=request.query,
          top_k=request.top_k,
          organization_id=request.organization_id
      )

      return {
          "query": context.query,
          "retrieved_docs": context.retrieved_docs,
          "total_retrieved": context.total_retrieved
      }

  @router.post("/vectorize-capability/{capability_id}")
  async def vectorize_capability(capability_id: str):
      """
      è§¦å‘èƒ½åŠ›å‘é‡åŒ–
      éœ€æ±‚ç¼–å·: REQ-AI-002
      """
      from app.tasks.vectorization import vectorize_capability_task

      task = vectorize_capability_task.delay(capability_id)

      return {
          "task_id": task.id,
          "status": "processing",
          "message": "å‘é‡åŒ–ä»»åŠ¡å·²æäº¤"
      }
  ```

#### 2.1.5 éƒ¨ç½²é…ç½®

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] ç¡®ä¿ LlamaIndex 0.14.8 å·²å®‰è£… âœ…
- [ ] é…ç½® Elasticsearch å‘é‡ç´¢å¼•
  ```yaml
  # docker-compose.yml
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:9.2.1
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTICSEARCH_PASSWORD}
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
  ```

- [ ] é…ç½® Celery å‘é‡åŒ–é˜Ÿåˆ—
  ```python
  # apps/backend-python/app/tasks/celery_app.py
  celery_app = Celery(
      'aibidcomposer',
      broker=settings.RABBITMQ_URL,
      backend=settings.REDIS_URL
  )

  celery_app.conf.task_routes = {
      'app.tasks.vectorization.vectorize_capability_task': {'queue': 'vectorization'},
  }
  ```

**éªŒæ”¶æ ‡å‡†**:
- [ ] RAGæ£€ç´¢èƒ½å¤Ÿè¿”å›ç›¸å…³æ–‡æ¡£
- [ ] ç›¸å…³æ€§è¯„åˆ†å‡†ç¡®
- [ ] å‘é‡åŒ–ä»»åŠ¡æˆåŠŸæ‰§è¡Œ
- [ ] Elasticsearchç´¢å¼•æ­£å¸¸å·¥ä½œ

---

### äºŒçº§ä»»åŠ¡ 2.2: æ™ºèƒ½ç”Ÿæˆå¼•æ“

**é¢„è®¡å·¥ä½œé‡**: 6 äººå¤©
**å®Œæˆè¿›åº¦**: 0% (0/5 ç±»åˆ«)

#### 2.2.1 æ•°æ®å®šä¹‰

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] å®šä¹‰å†…å®¹ç”Ÿæˆè¯·æ±‚/å“åº”æ¨¡å‹
  ```python
  # apps/backend-python/app/models/generation.py
  from pydantic import BaseModel
  from typing import List, Dict, Any, Optional
  from enum import Enum

  class GenerationType(str, Enum):
      """ç”Ÿæˆç±»å‹"""
      TECHNICAL_SOLUTION = "technical_solution"  # æŠ€æœ¯æ–¹æ¡ˆ
      IMPLEMENTATION_PLAN = "implementation_plan"  # å®æ–½æ–¹æ¡ˆ
      TEAM_INTRODUCTION = "team_introduction"  # å›¢é˜Ÿä»‹ç»
      CASE_REFERENCE = "case_reference"  # æ¡ˆä¾‹å¼•ç”¨

  class GenerationRequest(BaseModel):
      """å†…å®¹ç”Ÿæˆè¯·æ±‚"""
      generation_type: GenerationType
      project_id: str
      document_id: str
      section_id: Optional[str]
      requirements: List[Dict[str, Any]]  # æ‹›æ ‡éœ€æ±‚
      context: Dict[str, Any]  # é¢å¤–ä¸Šä¸‹æ–‡
      word_count: Optional[int] = 500  # ç›®æ ‡å­—æ•°

  class GenerationResponse(BaseModel):
      """å†…å®¹ç”Ÿæˆå“åº”"""
      task_id: str
      generated_content: str
      tokens_used: int
      generation_time: float  # ç§’
      quality_score: float  # 0-100
      suggestions: List[str]  # æ”¹è¿›å»ºè®®
  ```

- [ ] PostgreSQLç”Ÿæˆå†å²è¡¨ï¼ˆJavaæœåŠ¡ç®¡ç†ï¼‰
  ```sql
  CREATE TABLE ai_generation_history (
      id UUID PRIMARY KEY,
      project_id UUID NOT NULL,
      document_id UUID NOT NULL,
      generation_type VARCHAR(50),
      prompt_used TEXT,
      generated_content TEXT,
      tokens_used INT,
      generation_time FLOAT,
      created_by UUID,
      created_at TIMESTAMP WITH TIME ZONE,
      FOREIGN KEY (project_id) REFERENCES projects(id),
      FOREIGN KEY (document_id) REFERENCES bid_documents(id)
  );
  ```

#### 2.2.2 å‰ç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] åˆ›å»ºAIç”Ÿæˆè§¦å‘æŒ‰é’®ç»„ä»¶
  ```typescript
  // apps/frontend/src/components/ai/GenerateButton.tsx
  import { Button, Modal, Spin } from 'antd';
  import { RobotOutlined } from '@ant-design/icons';

  interface Props {
      generationType: string;
      projectId: string;
      documentId: string;
      onSuccess: (content: string) => void;
  }

  export function GenerateButton({ generationType, projectId, documentId, onSuccess }: Props) {
      const [generating, setGenerating] = useState(false);
      const [modalVisible, setModalVisible] = useState(false);

      const handleGenerate = async () => {
          setGenerating(true);
          setModalVisible(true);

          try {
              const response = await fetch('http://localhost:8001/api/v1/ai/generate-content', {
                  method: 'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body: JSON.stringify({
                      generation_type: generationType,
                      project_id: projectId,
                      document_id: documentId,
                      word_count: 800
                  })
              });

              const result = await response.json();

              // è½®è¯¢ä»»åŠ¡çŠ¶æ€
              const taskId = result.task_id;
              const pollResult = await pollTaskStatus(taskId);

              if (pollResult.status === 'success') {
                  onSuccess(pollResult.generated_content);
                  message.success('å†…å®¹ç”ŸæˆæˆåŠŸï¼');
              } else {
                  message.error('ç”Ÿæˆå¤±è´¥: ' + pollResult.error);
              }
          } catch (error) {
              message.error('ç”Ÿæˆå¤±è´¥');
          } finally {
              setGenerating(false);
              setModalVisible(false);
          }
      };

      const pollTaskStatus = async (taskId: string): Promise<any> => {
          // è½®è¯¢ä»»åŠ¡çŠ¶æ€ç›´åˆ°å®Œæˆ
          while (true) {
              const response = await fetch(`http://localhost:8001/api/v1/ai/tasks/${taskId}`);
              const result = await response.json();

              if (result.status === 'success' || result.status === 'failed') {
                  return result;
              }

              await new Promise(resolve => setTimeout(resolve, 2000));  // ç­‰å¾…2ç§’
          }
      };

      return (
          <>
              <Button
                  type="primary"
                  icon={<RobotOutlined />}
                  onClick={handleGenerate}
                  loading={generating}
              >
                  AIç”Ÿæˆ
              </Button>

              <Modal
                  title="AIæ­£åœ¨ç”Ÿæˆå†…å®¹..."
                  visible={modalVisible}
                  footer={null}
                  closable={false}
              >
                  <div style={{ textAlign: 'center', padding: '40px 0' }}>
                      <Spin size="large" />
                      <p style={{ marginTop: 20 }}>è¯·ç¨å€™ï¼ŒAIæ­£åœ¨åˆ†æéœ€æ±‚å¹¶ç”Ÿæˆå†…å®¹...</p>
                  </div>
              </Modal>
          </>
      );
  }
  ```

- [ ] åˆ›å»ºç”Ÿæˆç»“æœé¢„è§ˆå’Œç¼–è¾‘ç•Œé¢
  ```typescript
  // apps/frontend/src/components/ai/GeneratedContentPreview.tsx
  import { Card, Button, Rate } from 'antd';
  import { CheckOutlined, EditOutlined } from '@ant-design/icons';

  interface Props {
      content: string;
      qualityScore: number;
      suggestions: string[];
      onAccept: () => void;
      onEdit: () => void;
  }

  export function GeneratedContentPreview({ content, qualityScore, suggestions, onAccept, onEdit }: Props) {
      return (
          <Card
              title="AIç”Ÿæˆå†…å®¹é¢„è§ˆ"
              extra={
                  <div>
                      <Button icon={<EditOutlined />} onClick={onEdit} style={{ marginRight: 8 }}>
                          ç¼–è¾‘
                      </Button>
                      <Button type="primary" icon={<CheckOutlined />} onClick={onAccept}>
                          æ¥å—å¹¶æ’å…¥
                      </Button>
                  </div>
              }
          >
              <div>
                  <p><strong>è´¨é‡è¯„åˆ†:</strong> <Rate disabled value={qualityScore / 20} /></p>
                  <div style={{ marginTop: 16, padding: 16, background: '#f5f5f5', borderRadius: 4 }}>
                      {content}
                  </div>
                  {suggestions.length > 0 && (
                      <div style={{ marginTop: 16 }}>
                          <strong>æ”¹è¿›å»ºè®®:</strong>
                          <ul>
                              {suggestions.map((suggestion, idx) => (
                                  <li key={idx}>{suggestion}</li>
                              ))}
                          </ul>
                      </div>
                  )}
              </div>
          </Card>
      );
  }
  ```

#### 2.2.3 Javaåç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] åˆ›å»º `AIGenerationHistory` å®ä½“
  ```java
  @Data
  @TableName("ai_generation_history")
  public class AIGenerationHistory extends BaseEntity {
      @TableField("project_id")
      private String projectId;

      @TableField("document_id")
      private String documentId;

      @TableField("generation_type")
      private String generationType;

      @TableField("prompt_used")
      private String promptUsed;

      @TableField("generated_content")
      private String generatedContent;

      @TableField("tokens_used")
      private Integer tokensUsed;

      @TableField("generation_time")
      private Double generationTime;
  }
  ```

- [ ] åˆ›å»ºREST APIä¿å­˜ç”Ÿæˆå†å²
  ```java
  @RestController
  @RequestMapping("/api/v1/ai-generation-history")
  public class GenerationHistoryController {

      @PostMapping
      public Result<Void> save(@RequestBody SaveHistoryRequest request) {
          // PythonæœåŠ¡å›è°ƒï¼šä¿å­˜ç”Ÿæˆå†å²
          AIGenerationHistory history = new AIGenerationHistory();
          history.setProjectId(request.getProjectId());
          history.setDocumentId(request.getDocumentId());
          history.setGenerationType(request.getGenerationType());
          history.setGeneratedContent(request.getGeneratedContent());
          history.setTokensUsed(request.getTokensUsed());

          historyMapper.insert(history);
          return Result.success();
      }

      @GetMapping
      public Result<List<AIGenerationHistory>> list(@RequestParam String documentId) {
          // æŸ¥è¯¢æ–‡æ¡£çš„ç”Ÿæˆå†å²
          return Result.success(historyService.listByDocumentId(documentId));
      }
  }
  ```

#### 2.2.4 Pythonåç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] å®ç°æ™ºèƒ½ç”ŸæˆæœåŠ¡
  ```python
  # apps/backend-python/app/services/ai/generation_service.py
  from app.services.ai.rag_service import RAGService
  from llama_index.llms import OpenAI
  import time

  class GenerationService:
      """æ™ºèƒ½å†…å®¹ç”ŸæˆæœåŠ¡"""

      def __init__(self):
          self.rag_service = RAGService()
          self.llm = OpenAI(model="gpt-4-turbo-preview", api_key=settings.OPENAI_API_KEY)

      async def generate_technical_solution(
          self,
          requirements: List[Dict[str, Any]],
          organization_id: str,
          word_count: int = 800
      ) -> GenerationResponse:
          """
          ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆ
          éœ€æ±‚ç¼–å·: REQ-AI-002
          """
          start_time = time.time()

          # 1. æå–éœ€æ±‚å…³é”®è¯
          requirements_text = "\n".join([
              f"- {req['title']}: {req['description']}"
              for req in requirements
          ])

          # 2. RAGæ£€ç´¢ç›¸å…³èƒ½åŠ›å’Œæ¡ˆä¾‹
          context = await self.rag_service.retrieve(
              query=requirements_text,
              top_k=10,
              organization_id=organization_id
          )

          # 3. æ„å»ºç”ŸæˆPrompt
          prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ ‡ä¹¦æŠ€æœ¯æ–¹æ¡ˆæ’°å†™ä¸“å®¶ã€‚

æ‹›æ ‡æŠ€æœ¯è¦æ±‚ï¼š
{requirements_text}

ä¼ä¸šç›¸å…³èƒ½åŠ›å’Œæ¡ˆä¾‹ï¼ˆä¾›å‚è€ƒï¼‰ï¼š
{self._format_context(context)}

è¯·åŸºäºä»¥ä¸Šæ‹›æ ‡è¦æ±‚å’Œä¼ä¸šèƒ½åŠ›ï¼Œæ’°å†™ä¸€ä»½ä¸“ä¸šçš„æŠ€æœ¯æ–¹æ¡ˆã€‚

è¦æ±‚ï¼š
1. å­—æ•°çº¦{word_count}å­—
2. ç´§å¯†è´´åˆæ‹›æ ‡éœ€æ±‚
3. çªå‡ºä¼ä¸šæŠ€æœ¯ä¼˜åŠ¿
4. ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘ä¸¥å¯†
5. ä½¿ç”¨ä¸“ä¸šæœ¯è¯­
6. å¼•ç”¨å…·ä½“æ¡ˆä¾‹å¢å¼ºè¯´æœåŠ›

è¯·ç›´æ¥è¾“å‡ºæŠ€æœ¯æ–¹æ¡ˆå†…å®¹ï¼š
"""

          # 4. è°ƒç”¨GPT-4ç”Ÿæˆ
          response = await self.llm.acomplete(prompt)
          generated_content = response.text

          # 5. è´¨é‡è¯„ä¼°
          quality_score = await self._evaluate_quality(generated_content, requirements)

          # 6. ç”Ÿæˆæ”¹è¿›å»ºè®®
          suggestions = await self._generate_suggestions(generated_content, requirements)

          generation_time = time.time() - start_time

          return GenerationResponse(
              task_id="",  # ç”±Celeryä»»åŠ¡IDå¡«å……
              generated_content=generated_content,
              tokens_used=response.additional_kwargs.get('usage', {}).get('total_tokens', 0),
              generation_time=generation_time,
              quality_score=quality_score,
              suggestions=suggestions
          )

      def _format_context(self, context: RAGContext) -> str:
          """æ ¼å¼åŒ–RAGä¸Šä¸‹æ–‡"""
          formatted = []
          for doc in context.retrieved_docs:
              formatted.append(f"[{doc['title']}]\n{doc['content']}\n")
          return "\n".join(formatted)

      async def _evaluate_quality(
          self,
          content: str,
          requirements: List[Dict[str, Any]]
      ) -> float:
          """è¯„ä¼°ç”Ÿæˆå†…å®¹è´¨é‡ï¼ˆ0-100ï¼‰"""
          # ä½¿ç”¨LLMè¯„ä¼°è´¨é‡
          eval_prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹æŠ€æœ¯æ–¹æ¡ˆçš„è´¨é‡ï¼ˆ0-100åˆ†ï¼‰ï¼š

æŠ€æœ¯æ–¹æ¡ˆï¼š
{content}

åŸå§‹éœ€æ±‚ï¼š
{requirements}

è¯„ä¼°ç»´åº¦ï¼š
1. ä¸éœ€æ±‚çš„åŒ¹é…åº¦ï¼ˆ40åˆ†ï¼‰
2. å†…å®¹çš„ä¸“ä¸šæ€§ï¼ˆ30åˆ†ï¼‰
3. é€»è¾‘çš„ä¸¥å¯†æ€§ï¼ˆ20åˆ†ï¼‰
4. è¯­è¨€çš„æµç•…æ€§ï¼ˆ10åˆ†ï¼‰

è¯·åªè¿”å›ä¸€ä¸ª0-100çš„åˆ†æ•°ã€‚
"""

          response = await self.llm.acomplete(eval_prompt)
          try:
              score = float(response.text.strip())
              return min(max(score, 0), 100)
          except:
              return 75.0  # é»˜è®¤åˆ†æ•°

      async def _generate_suggestions(
          self,
          content: str,
          requirements: List[Dict[str, Any]]
      ) -> List[str]:
          """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
          suggestions_prompt = f"""
è¯·ä¸ºä»¥ä¸‹æŠ€æœ¯æ–¹æ¡ˆæä¾›3-5æ¡æ”¹è¿›å»ºè®®ï¼š

æŠ€æœ¯æ–¹æ¡ˆï¼š
{content}

è¯·åˆ—å‡ºå…·ä½“çš„æ”¹è¿›å»ºè®®ï¼ˆæ¯æ¡ä¸è¶…è¿‡30å­—ï¼‰ï¼š
"""

          response = await self.llm.acomplete(suggestions_prompt)
          suggestions = [
              line.strip().lstrip('123456789.- ')
              for line in response.text.strip().split('\n')
              if line.strip()
          ]
          return suggestions[:5]
  ```

- [ ] åˆ›å»ºCeleryç”Ÿæˆä»»åŠ¡
  ```python
  # apps/backend-python/app/tasks/generation.py
  @celery_app.task(bind=True)
  def generate_content_task(
      self: Task,
      generation_type: str,
      project_id: str,
      document_id: str,
      requirements: List[Dict],
      word_count: int = 800
  ):
      """
      å†…å®¹ç”Ÿæˆä»»åŠ¡
      éœ€æ±‚ç¼–å·: REQ-AI-002
      """
      try:
          generation_service = GenerationService()
          java_client = JavaAPIClient()

          # 1. è·å–ç»„ç»‡ID
          project = await java_client.get_project(project_id)
          organization_id = project['organization_id']

          # 2. æ‰§è¡Œç”Ÿæˆ
          if generation_type == 'technical_solution':
              result = await generation_service.generate_technical_solution(
                  requirements=requirements,
                  organization_id=organization_id,
                  word_count=word_count
              )
          # elif ...å…¶ä»–ç”Ÿæˆç±»å‹

          # 3. ä¿å­˜åˆ°JavaæœåŠ¡
          await java_client.save_generation_history({
              'project_id': project_id,
              'document_id': document_id,
              'generation_type': generation_type,
              'generated_content': result.generated_content,
              'tokens_used': result.tokens_used
          })

          return {
              'status': 'success',
              'generated_content': result.generated_content,
              'quality_score': result.quality_score,
              'suggestions': result.suggestions
          }

      except Exception as e:
          return {'status': 'failed', 'error': str(e)}
  ```

- [ ] åˆ›å»ºAPIç«¯ç‚¹
  ```python
  # apps/backend-python/app/api/v1/generation.py
  @router.post("/generate-content")
  async def generate_content(request: GenerationRequest):
      """
      è§¦å‘å†…å®¹ç”Ÿæˆ
      éœ€æ±‚ç¼–å·: REQ-AI-002
      """
      from app.tasks.generation import generate_content_task

      # å¼‚æ­¥ç”Ÿæˆä»»åŠ¡
      task = generate_content_task.delay(
          generation_type=request.generation_type,
          project_id=request.project_id,
          document_id=request.document_id,
          requirements=request.requirements,
          word_count=request.word_count
      )

      return {
          "task_id": task.id,
          "status": "processing",
          "message": "å†…å®¹ç”Ÿæˆä»»åŠ¡å·²æäº¤"
      }

  @router.get("/tasks/{task_id}")
  async def get_task_status(task_id: str):
      """è·å–ä»»åŠ¡çŠ¶æ€"""
      from app.tasks.celery_app import celery_app

      task = celery_app.AsyncResult(task_id)

      if task.ready():
          return {
              "status": "success" if task.successful() else "failed",
              **task.result
          }
      else:
          return {
              "status": "processing",
              "progress": 50  # å¯ä»¥æ›´ç²¾ç»†åœ°è·Ÿè¸ªè¿›åº¦
          }
  ```

#### 2.2.5 éƒ¨ç½²é…ç½®

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] é…ç½®Celeryç”Ÿæˆé˜Ÿåˆ—
  ```python
  celery_app.conf.task_routes = {
      'app.tasks.generation.generate_content_task': {'queue': 'generation'},
  }
  ```

- [ ] å¯åŠ¨ç”ŸæˆWorker
  ```bash
  # docker-compose.yml
  generation-worker:
    build:
      context: ./apps/backend-python
    command: celery -A app.tasks.celery_app worker --loglevel=info -Q generation -c 2
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - rabbitmq
      - redis
  ```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æŠ€æœ¯æ–¹æ¡ˆç”ŸæˆæˆåŠŸ
- [ ] ç”Ÿæˆå†…å®¹è´¨é‡è¯„åˆ†å‡†ç¡®
- [ ] æ”¹è¿›å»ºè®®åˆç†
- [ ] ç”Ÿæˆä»»åŠ¡å¯è¿½è¸ª

---

### äºŒçº§ä»»åŠ¡ 2.3: AIåŠ©æ‰‹çŸ©é˜µ

**é¢„è®¡å·¥ä½œé‡**: 4 äººå¤©
**å®Œæˆè¿›åº¦**: 0% (0/5 ç±»åˆ«)

#### 2.3.1 æ•°æ®å®šä¹‰

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] å®šä¹‰AIåŠ©æ‰‹ç±»å‹æšä¸¾
  ```python
  # apps/backend-python/app/models/assistant.py
  from enum import Enum

  class AssistantType(str, Enum):
      """AIåŠ©æ‰‹ç±»å‹"""
      TECHNICAL_EXPERT = "technical_expert"  # æŠ€æœ¯ä¸“å®¶
      BUSINESS_EXPERT = "business_expert"    # å•†åŠ¡ä¸“å®¶
      COMPLIANCE_EXPERT = "compliance_expert"  # åˆè§„ä¸“å®¶
      QUALITY_ASSURER = "quality_assurer"    # è´¨é‡å®¡æŸ¥

  class AssistantConfig(BaseModel):
      """åŠ©æ‰‹é…ç½®"""
      assistant_type: AssistantType
      model: str = "gpt-4-turbo-preview"
      temperature: float = 0.7
      system_prompt: str
      max_tokens: int = 2000

  class AssistantMessage(BaseModel):
      """åŠ©æ‰‹æ¶ˆæ¯"""
      role: str  # 'user'|'assistant'
      content: str
      timestamp: datetime
  ```

#### 2.3.2 å‰ç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] åˆ›å»ºAIåŠ©æ‰‹å¯¹è¯ç•Œé¢
  ```typescript
  // apps/frontend/src/components/ai/AssistantChat.tsx
  import { Card, Input, Button, Avatar, List } from 'antd';
  import { RobotOutlined, UserOutlined, SendOutlined } from '@ant-design/icons';

  interface Message {
      role: 'user' | 'assistant';
      content: string;
      timestamp: string;
  }

  interface Props {
      assistantType: string;  // 'technical_expert' | 'business_expert' ...
  }

  export function AssistantChat({ assistantType }: Props) {
      const [messages, setMessages] = useState<Message[]>([]);
      const [inputValue, setInputValue] = useState('');
      const [loading, setLoading] = useState(false);

      const assistantNames = {
          technical_expert: 'æŠ€æœ¯ä¸“å®¶åŠ©æ‰‹',
          business_expert: 'å•†åŠ¡ä¸“å®¶åŠ©æ‰‹',
          compliance_expert: 'åˆè§„ä¸“å®¶åŠ©æ‰‹',
          quality_assurer: 'è´¨é‡å®¡æŸ¥åŠ©æ‰‹',
      };

      const handleSend = async () => {
          if (!inputValue.trim()) return;

          const userMessage: Message = {
              role: 'user',
              content: inputValue,
              timestamp: new Date().toISOString()
          };

          setMessages([...messages, userMessage]);
          setInputValue('');
          setLoading(true);

          try {
              const response = await fetch('http://localhost:8001/api/v1/ai/assistant-chat', {
                  method: 'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body: JSON.stringify({
                      assistant_type: assistantType,
                      message: inputValue,
                      conversation_history: messages
                  })
              });

              const result = await response.json();

              const assistantMessage: Message = {
                  role: 'assistant',
                  content: result.response,
                  timestamp: new Date().toISOString()
              };

              setMessages([...messages, userMessage, assistantMessage]);
          } catch (error) {
              message.error('åŠ©æ‰‹å“åº”å¤±è´¥');
          } finally {
              setLoading(false);
          }
      };

      return (
          <Card title={assistantNames[assistantType]} style={{ height: '600px', display: 'flex', flexDirection: 'column' }}>
              <div style={{ flex: 1, overflowY: 'auto', marginBottom: 16 }}>
                  <List
                      dataSource={messages}
                      renderItem={(msg) => (
                          <List.Item style={{ border: 'none' }}>
                              <List.Item.Meta
                                  avatar={
                                      msg.role === 'user' ?
                                          <Avatar icon={<UserOutlined />} /> :
                                          <Avatar icon={<RobotOutlined />} style={{ backgroundColor: '#1890ff' }} />
                                  }
                                  description={
                                      <div style={{ whiteSpace: 'pre-wrap' }}>
                                          {msg.content}
                                      </div>
                                  }
                              />
                          </List.Item>
                      )}
                  />
              </div>

              <div style={{ display: 'flex', gap: 8 }}>
                  <Input.TextArea
                      value={inputValue}
                      onChange={(e) => setInputValue(e.target.value)}
                      onPressEnter={(e) => {
                          if (!e.shiftKey) {
                              e.preventDefault();
                              handleSend();
                          }
                      }}
                      placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜... (Enterå‘é€, Shift+Enteræ¢è¡Œ)"
                      autoSize={{ minRows: 1, maxRows: 4 }}
                  />
                  <Button
                      type="primary"
                      icon={<SendOutlined />}
                      onClick={handleSend}
                      loading={loading}
                  >
                      å‘é€
                  </Button>
              </div>
          </Card>
      );
  }
  ```

- [ ] åˆ›å»ºåŠ©æ‰‹é€‰æ‹©é¢æ¿
  ```typescript
  // apps/frontend/src/pages/ai/AssistantPanel.tsx
  import { Tabs } from 'antd';

  export default function AssistantPanel() {
      return (
          <Tabs defaultActiveKey="technical_expert">
              <Tabs.TabPane tab="æŠ€æœ¯ä¸“å®¶" key="technical_expert">
                  <AssistantChat assistantType="technical_expert" />
              </Tabs.TabPane>
              <Tabs.TabPane tab="å•†åŠ¡ä¸“å®¶" key="business_expert">
                  <AssistantChat assistantType="business_expert" />
              </Tabs.TabPane>
              <Tabs.TabPane tab="åˆè§„ä¸“å®¶" key="compliance_expert">
                  <AssistantChat assistantType="compliance_expert" />
              </Tabs.TabPane>
              <Tabs.TabPane tab="è´¨é‡å®¡æŸ¥" key="quality_assurer">
                  <AssistantChat assistantType="quality_assurer" />
              </Tabs.TabPane>
          </Tabs>
      );
  }
  ```

#### 2.3.3 Javaåç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] æ— éœ€Javaåç«¯æ”¯æŒï¼ˆçº¯Pythonå®ç°ï¼‰

#### 2.3.4 Pythonåç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] å®ç°AIåŠ©æ‰‹çŸ©é˜µæœåŠ¡
  ```python
  # apps/backend-python/app/services/ai/assistant_matrix.py
  from llama_index.llms import OpenAI
  from typing import List, Dict

  class AIAssistantMatrix:
      """AIåŠ©æ‰‹çŸ©é˜µ"""

      def __init__(self):
          self.assistants = {
              AssistantType.TECHNICAL_EXPERT: AssistantConfig(
                  assistant_type=AssistantType.TECHNICAL_EXPERT,
                  model="gpt-4-turbo-preview",
                  temperature=0.2,
                  system_prompt="""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æŠ€æœ¯ä¸“å®¶ï¼Œæ“…é•¿ï¼š
- åˆ†ææŠ€æœ¯éœ€æ±‚å’ŒæŠ€æœ¯éš¾ç‚¹
- è®¾è®¡æŠ€æœ¯æ–¹æ¡ˆå’Œæ¶æ„
- è¯„ä¼°æŠ€æœ¯å¯è¡Œæ€§
- è¯†åˆ«æŠ€æœ¯é£é™©

è¯·ä»¥ä¸“ä¸šã€ä¸¥è°¨çš„æ€åº¦å›ç­”ç”¨æˆ·çš„æŠ€æœ¯é—®é¢˜ã€‚"""
              ),
              AssistantType.BUSINESS_EXPERT: AssistantConfig(
                  assistant_type=AssistantType.BUSINESS_EXPERT,
                  model="gpt-3.5-turbo",
                  temperature=0.5,
                  system_prompt="""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å•†åŠ¡ä¸“å®¶ï¼Œæ“…é•¿ï¼š
- å•†åŠ¡æ¡æ¬¾åˆ†æ
- æŠ¥ä»·ç­–ç•¥åˆ¶å®š
- å•†åŠ¡é£é™©è¯„ä¼°
- åˆåŒè°ˆåˆ¤å»ºè®®

è¯·ä»¥ä¸“ä¸šã€åŠ¡å®çš„æ€åº¦å›ç­”ç”¨æˆ·çš„å•†åŠ¡é—®é¢˜ã€‚"""
              ),
              AssistantType.COMPLIANCE_EXPERT: AssistantConfig(
                  assistant_type=AssistantType.COMPLIANCE_EXPERT,
                  model="gpt-4-turbo-preview",
                  temperature=0.1,
                  system_prompt="""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åˆè§„ä¸“å®¶ï¼Œæ“…é•¿ï¼š
- æ ‡ä¹¦åˆè§„æ€§å®¡æŸ¥
- æ³•å¾‹æ³•è§„è§£è¯»
- èµ„è´¨è¦æ±‚æ ¸æŸ¥
- åˆè§„é£é™©è¯†åˆ«

è¯·ä»¥ä¸¥è°¨ã€ç»†è‡´çš„æ€åº¦å®¡æŸ¥åˆè§„é—®é¢˜ã€‚"""
              ),
              AssistantType.QUALITY_ASSURER: AssistantConfig(
                  assistant_type=AssistantType.QUALITY_ASSURER,
                  model="gpt-4-turbo-preview",
                  temperature=0.3,
                  system_prompt="""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è´¨é‡ä¿è¯ä¸“å®¶ï¼Œæ“…é•¿ï¼š
- å†…å®¹è´¨é‡è¯„ä¼°
- å®Œæ•´æ€§æ£€æŸ¥
- æ ¼å¼è§„èŒƒå®¡æŸ¥
- æ”¹è¿›å»ºè®®æä¾›

è¯·ä»¥ä¸“ä¸šã€å®¢è§‚çš„æ€åº¦è¯„ä¼°å†…å®¹è´¨é‡ã€‚"""
              ),
          }

      async def chat(
          self,
          assistant_type: AssistantType,
          message: str,
          conversation_history: List[Dict[str, str]] = None
      ) -> str:
          """
          ä¸AIåŠ©æ‰‹å¯¹è¯
          éœ€æ±‚ç¼–å·: REQ-AI-002
          """
          config = self.assistants[assistant_type]

          # åˆå§‹åŒ–LLM
          llm = OpenAI(
              model=config.model,
              api_key=settings.OPENAI_API_KEY,
              temperature=config.temperature
          )

          # æ„å»ºæ¶ˆæ¯å†å²
          messages = [{"role": "system", "content": config.system_prompt}]

          if conversation_history:
              messages.extend(conversation_history)

          messages.append({"role": "user", "content": message})

          # è°ƒç”¨LLM
          response = await llm.achat(messages)

          return response.message.content
  ```

- [ ] åˆ›å»ºAPIç«¯ç‚¹
  ```python
  # apps/backend-python/app/api/v1/assistant.py
  @router.post("/assistant-chat")
  async def assistant_chat(
      assistant_type: AssistantType,
      message: str,
      conversation_history: List[Dict[str, str]] = None
  ):
      """
      AIåŠ©æ‰‹å¯¹è¯
      éœ€æ±‚ç¼–å·: REQ-AI-002
      """
      assistant_matrix = AIAssistantMatrix()

      response = await assistant_matrix.chat(
          assistant_type=assistant_type,
          message=message,
          conversation_history=conversation_history or []
      )

      return {
          "assistant_type": assistant_type,
          "response": response,
          "timestamp": datetime.now().isoformat()
      }
  ```

#### 2.3.5 éƒ¨ç½²é…ç½®

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] é…ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ âœ…
- [ ] æ— éœ€é¢å¤–éƒ¨ç½²é…ç½®

**éªŒæ”¶æ ‡å‡†**:
- [ ] 4ç§åŠ©æ‰‹éƒ½èƒ½æ­£å¸¸å“åº”
- [ ] åŠ©æ‰‹å›ç­”ç¬¦åˆè§’è‰²å®šä½
- [ ] å¯¹è¯å†å²ä¿æŒè¿è´¯

---

### äºŒçº§ä»»åŠ¡ 2.4: å†…å®¹ä¼˜åŒ–

**é¢„è®¡å·¥ä½œé‡**: 3 äººå¤©
**å®Œæˆè¿›åº¦**: 0% (0/5 ç±»åˆ«)

#### 2.4.1 æ•°æ®å®šä¹‰

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] å®šä¹‰ä¼˜åŒ–è¯·æ±‚æ¨¡å‹
  ```python
  # apps/backend-python/app/models/optimization.py
  class OptimizationType(str, Enum):
      POLISH = "polish"  # æ¶¦è‰²
      SIMPLIFY = "simplify"  # ç®€åŒ–
      EXPAND = "expand"  # æ‰©å±•
      FORMALIZE = "formalize"  # æ­£å¼åŒ–
      TERMINOLOGY_CHECK = "terminology_check"  # æœ¯è¯­æ£€æŸ¥

  class OptimizationRequest(BaseModel):
      content: str
      optimization_type: OptimizationType
      context: Optional[Dict[str, Any]]

  class OptimizationResponse(BaseModel):
      original_content: str
      optimized_content: str
      changes: List[Dict[str, Any]]  # ä¿®æ”¹è¯´æ˜
      suggestions: List[str]
  ```

#### 2.4.2 å‰ç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] åˆ›å»ºå†…å®¹ä¼˜åŒ–æŒ‰é’®ç»„
  ```typescript
  // apps/frontend/src/components/ai/OptimizationTools.tsx
  import { Button, Dropdown, Menu, message } from 'antd';
  import { ThunderboltOutlined } from '@ant-design/icons';

  interface Props {
      selectedText: string;
      onOptimized: (optimizedText: string) => void;
  }

  export function OptimizationTools({ selectedText, onOptimized }: Props) {
      const handleOptimize = async (type: string) => {
          if (!selectedText) {
              message.warning('è¯·å…ˆé€‰ä¸­è¦ä¼˜åŒ–çš„æ–‡æœ¬');
              return;
          }

          try {
              const response = await fetch('http://localhost:8001/api/v1/ai/optimize-content', {
                  method: 'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body: JSON.stringify({
                      content: selectedText,
                      optimization_type: type
                  })
              });

              const result = await response.json();
              onOptimized(result.optimized_content);
              message.success('ä¼˜åŒ–å®Œæˆ');
          } catch (error) {
              message.error('ä¼˜åŒ–å¤±è´¥');
          }
      };

      const menu = (
          <Menu onClick={({ key }) => handleOptimize(key)}>
              <Menu.Item key="polish">æ¶¦è‰²</Menu.Item>
              <Menu.Item key="simplify">ç®€åŒ–</Menu.Item>
              <Menu.Item key="expand">æ‰©å±•</Menu.Item>
              <Menu.Item key="formalize">æ­£å¼åŒ–</Menu.Item>
              <Menu.Item key="terminology_check">æœ¯è¯­æ£€æŸ¥</Menu.Item>
          </Menu>
      );

      return (
          <Dropdown overlay={menu}>
              <Button icon={<ThunderboltOutlined />}>
                  AIä¼˜åŒ–
              </Button>
          </Dropdown>
      );
  }
  ```

#### 2.4.3 Javaåç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] æ— éœ€Javaåç«¯æ”¯æŒï¼ˆçº¯Pythonå®ç°ï¼‰

#### 2.4.4 Pythonåç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] å®ç°å†…å®¹ä¼˜åŒ–æœåŠ¡
  ```python
  # apps/backend-python/app/services/ai/optimization_service.py
  class OptimizationService:
      """å†…å®¹ä¼˜åŒ–æœåŠ¡"""

      def __init__(self):
          self.llm = OpenAI(model="gpt-4-turbo-preview", api_key=settings.OPENAI_API_KEY)

      async def optimize(
          self,
          content: str,
          optimization_type: OptimizationType
      ) -> OptimizationResponse:
          """
          ä¼˜åŒ–å†…å®¹
          éœ€æ±‚ç¼–å·: REQ-AI-002
          """
          prompts = {
              OptimizationType.POLISH: f"""
è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ¶¦è‰²ï¼Œæå‡æ–‡å­—çš„æµç•…æ€§å’Œä¸“ä¸šæ€§ï¼Œä¿æŒåŸæ„ä¸å˜ï¼š

{content}

è¯·ç›´æ¥è¾“å‡ºæ¶¦è‰²åçš„å†…å®¹ï¼š
""",
              OptimizationType.SIMPLIFY: f"""
è¯·å°†ä»¥ä¸‹å†…å®¹ç®€åŒ–ï¼Œä½¿å…¶æ›´ç®€æ´æ˜äº†ï¼Œå»é™¤å†—ä½™è¡¨è¿°ï¼š

{content}

è¯·ç›´æ¥è¾“å‡ºç®€åŒ–åçš„å†…å®¹ï¼š
""",
              OptimizationType.EXPAND: f"""
è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ‰©å±•ï¼Œå¢åŠ ç»†èŠ‚å’Œè¯´æ˜ï¼Œä½¿å…¶æ›´åŠ å……å®ï¼š

{content}

è¯·ç›´æ¥è¾“å‡ºæ‰©å±•åçš„å†…å®¹ï¼š
""",
              OptimizationType.FORMALIZE: f"""
è¯·å°†ä»¥ä¸‹å†…å®¹æ­£å¼åŒ–ï¼Œä½¿ç”¨æ›´æ­£å¼çš„è¡¨è¿°å’Œä¸“ä¸šæœ¯è¯­ï¼š

{content}

è¯·ç›´æ¥è¾“å‡ºæ­£å¼åŒ–åçš„å†…å®¹ï¼š
""",
              OptimizationType.TERMINOLOGY_CHECK: f"""
è¯·æ£€æŸ¥ä»¥ä¸‹å†…å®¹ä¸­çš„ä¸“ä¸šæœ¯è¯­ä½¿ç”¨æ˜¯å¦å‡†ç¡®ï¼Œå¦‚æœ‰é”™è¯¯è¯·æŒ‡å‡ºå¹¶ä¿®æ­£ï¼š

{content}

è¯·åˆ—å‡ºå‘ç°çš„æœ¯è¯­é—®é¢˜å’Œä¿®æ­£å»ºè®®ï¼š
"""
          }

          prompt = prompts[optimization_type]

          # è°ƒç”¨LLMä¼˜åŒ–
          response = await self.llm.acomplete(prompt)
          optimized_content = response.text.strip()

          # åˆ†æä¿®æ”¹
          changes = await self._analyze_changes(content, optimized_content)

          return OptimizationResponse(
              original_content=content,
              optimized_content=optimized_content,
              changes=changes,
              suggestions=[]
          )

      async def _analyze_changes(
          self,
          original: str,
          optimized: str
      ) -> List[Dict[str, Any]]:
          """åˆ†æä¿®æ”¹ç‚¹"""
          # ç®€å•çš„å·®å¼‚åˆ†æï¼ˆå¯ä»¥ç”¨difflibæ›´ç²¾ç»†ï¼‰
          import difflib

          changes = []
          diff = difflib.unified_diff(
              original.splitlines(),
              optimized.splitlines(),
              lineterm=''
          )

          for line in diff:
              if line.startswith('+'):
                  changes.append({
                      'type': 'add',
                      'content': line[1:]
                  })
              elif line.startswith('-'):
                  changes.append({
                      'type': 'remove',
                      'content': line[1:]
                  })

          return changes
  ```

- [ ] åˆ›å»ºAPIç«¯ç‚¹
  ```python
  @router.post("/optimize-content")
  async def optimize_content(request: OptimizationRequest):
      """
      ä¼˜åŒ–å†…å®¹
      éœ€æ±‚ç¼–å·: REQ-AI-002
      """
      optimization_service = OptimizationService()

      result = await optimization_service.optimize(
          content=request.content,
          optimization_type=request.optimization_type
      )

      return result
  ```

#### 2.4.5 éƒ¨ç½²é…ç½®

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] æ— éœ€é¢å¤–éƒ¨ç½²é…ç½®

**éªŒæ”¶æ ‡å‡†**:
- [ ] å„ç±»ä¼˜åŒ–åŠŸèƒ½æ­£å¸¸
- [ ] ä¼˜åŒ–ç»“æœè´¨é‡é«˜
- [ ] ä¿®æ”¹åˆ†æå‡†ç¡®

---

### äºŒçº§ä»»åŠ¡ 2.5: APIæ¥å£æ•´åˆå’Œæµ‹è¯•

**é¢„è®¡å·¥ä½œé‡**: 4 äººå¤©
**å®Œæˆè¿›åº¦**: 0% (0/5 ç±»åˆ«)

#### 2.5.1 æ•°æ®å®šä¹‰

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] å®šä¹‰ç»Ÿä¸€çš„AIæœåŠ¡å“åº”æ ¼å¼ï¼ˆå·²å®Œæˆï¼‰

#### 2.5.2 å‰ç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] æ•´åˆæ‰€æœ‰AIæœåŠ¡API
  ```typescript
  // apps/frontend/src/services/ai.service.ts
  export const aiService = {
      // RAGç›¸å…³
      ragRetrieve: (query: string, topK: number = 5) => { ... },

      // ç”Ÿæˆç›¸å…³
      generateContent: (type: string, projectId: string, documentId: string) => { ... },

      // åŠ©æ‰‹ç›¸å…³
      chatWithAssistant: (assistantType: string, message: string) => { ... },

      // ä¼˜åŒ–ç›¸å…³
      optimizeContent: (content: string, type: string) => { ... },

      // ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢
      getTaskStatus: (taskId: string) => { ... },
  };
  ```

#### 2.5.3 Javaåç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] åˆ›å»ºAIæœåŠ¡ç»Ÿä¸€ä»£ç†ï¼ˆå¯é€‰ï¼‰

#### 2.5.4 Pythonåç«¯å®ç°

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] æ•´åˆæ‰€æœ‰APIè·¯ç”±
  ```python
  # apps/backend-python/app/api/v1/__init__.py
  from fastapi import APIRouter
  from app.api.v1 import (
      document_parser,
      information_extraction,
      rag,
      generation,
      assistant,
      optimization
  )

  api_router = APIRouter()
  api_router.include_router(document_parser.router)
  api_router.include_router(information_extraction.router)
  api_router.include_router(rag.router)
  api_router.include_router(generation.router)
  api_router.include_router(assistant.router)
  api_router.include_router(optimization.router)
  ```

- [ ] å®Œå–„APIæ–‡æ¡£
  ```python
  # apps/backend-python/app/main.py
  app = FastAPI(
      title="AIæ ‡ä¹¦æ™ºèƒ½åˆ›ä½œå¹³å° - AIæœåŠ¡",
      description="""
## æ™ºèƒ½å†…å®¹ç”Ÿæˆå¼•æ“ API

æä¾›ä»¥ä¸‹AIèƒ½åŠ›ï¼š

### 1. RAGæ£€ç´¢å¢å¼ºç”Ÿæˆ
- ä¼ä¸šçŸ¥è¯†åº“ç®¡ç†
- è¯­ä¹‰æ£€ç´¢
- ä¸Šä¸‹æ–‡å¢å¼ºç”Ÿæˆ

### 2. æ™ºèƒ½å†…å®¹ç”Ÿæˆ
- æŠ€æœ¯æ–¹æ¡ˆç”Ÿæˆ
- å®æ–½æ–¹æ¡ˆç”Ÿæˆ
- å›¢é˜Ÿä»‹ç»ç”Ÿæˆ
- æ¡ˆä¾‹å¼•ç”¨ç”Ÿæˆ

### 3. AIåŠ©æ‰‹çŸ©é˜µ
- æŠ€æœ¯ä¸“å®¶åŠ©æ‰‹
- å•†åŠ¡ä¸“å®¶åŠ©æ‰‹
- åˆè§„ä¸“å®¶åŠ©æ‰‹
- è´¨é‡å®¡æŸ¥åŠ©æ‰‹

### 4. å†…å®¹ä¼˜åŒ–
- å†…å®¹æ¶¦è‰²
- ç®€åŒ–/æ‰©å±•
- æ­£å¼åŒ–
- æœ¯è¯­æ£€æŸ¥
      """,
      version="1.0.0",
      docs_url="/docs",
      redoc_url="/redoc"
  )
  ```

#### 2.5.5 éƒ¨ç½²é…ç½®

**å¾…å®Œæˆä»»åŠ¡**:
- [ ] ç¼–å†™é›†æˆæµ‹è¯•è„šæœ¬
  ```bash
  # scripts/test-ai-002-integration.sh
  #!/bin/bash

  echo "===== æµ‹è¯•AI-002: æ™ºèƒ½å†…å®¹ç”Ÿæˆå¼•æ“ ====="

  echo "1. æµ‹è¯•RAGæ£€ç´¢..."
  curl -X POST http://localhost:8001/api/v1/ai/rag-retrieve \
    -H "Content-Type: application/json" \
    -d '{"query": "äº‘è®¡ç®—æŠ€æœ¯", "top_k": 5}'

  echo "\n2. æµ‹è¯•å†…å®¹ç”Ÿæˆ..."
  curl -X POST http://localhost:8001/api/v1/ai/generate-content \
    -H "Content-Type: application/json" \
    -d '{
      "generation_type": "technical_solution",
      "project_id": "test-project-id",
      "document_id": "test-doc-id",
      "requirements": [{"title": "äº‘å¹³å°æ­å»º", "description": "éœ€è¦æ­å»ºç§æœ‰äº‘å¹³å°"}],
      "word_count": 500
    }'

  echo "\n3. æµ‹è¯•AIåŠ©æ‰‹..."
  curl -X POST http://localhost:8001/api/v1/ai/assistant-chat \
    -H "Content-Type: application/json" \
    -d '{
      "assistant_type": "technical_expert",
      "message": "å¦‚ä½•è®¾è®¡ä¸€ä¸ªé«˜å¯ç”¨çš„äº‘å¹³å°æ¶æ„ï¼Ÿ"
    }'

  echo "\n4. æµ‹è¯•å†…å®¹ä¼˜åŒ–..."
  curl -X POST http://localhost:8001/api/v1/ai/optimize-content \
    -H "Content-Type: application/json" \
    -d '{
      "content": "æˆ‘ä»¬å…¬å¸æœ‰å¾ˆå¤šå¹´çš„ç»éªŒã€‚",
      "optimization_type": "formalize"
    }'
  ```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰APIç«¯ç‚¹æ­£å¸¸å“åº”
- [ ] APIæ–‡æ¡£å®Œæ•´å‡†ç¡®
- [ ] é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] é”™è¯¯å¤„ç†æ­£ç¡®

---

## AI-002 æ€»ç»“

### æ€»ä½“è¿›åº¦

- **æ€»è®¡äºŒçº§ä»»åŠ¡**: 5ä¸ª
- **å·²å®Œæˆ**: 0ä¸ªï¼ˆ0%ï¼‰
- **å¾…å¼€å§‹**: 5ä¸ª
- **é¢„è®¡æ€»å·¥ä½œé‡**: 22 äººå¤©

### ç»†åŒ–æ•ˆæœ

âœ… **ç»†åŒ–å‰**: 5ä¸ªç²—ç²’åº¦å­ä»»åŠ¡
âœ… **ç»†åŒ–å**: æ¯ä¸ªå­ä»»åŠ¡æ‹†åˆ†ä¸º5ç±»åˆ«ï¼Œå…± 25+ å…·ä½“ä»»åŠ¡é¡¹

### å…³é”®ä¾èµ–

1. **AI-001 â†’ AI-002**: AI-002ä¾èµ–AI-001çš„å‘é‡åŒ–åŠŸèƒ½
2. **RAGç³»ç»Ÿ â†’ ç”Ÿæˆå¼•æ“**: ç”Ÿæˆå¼•æ“ä¾èµ–RAGæ£€ç´¢
3. **çŸ¥è¯†åº“æ•°æ®**: éœ€è¦JavaæœåŠ¡æä¾›ä¼ä¸šèƒ½åŠ›æ•°æ®

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ä¼˜å…ˆå¼€å§‹**: äºŒçº§ä»»åŠ¡ 2.1ï¼ˆRAGç³»ç»Ÿæ„å»ºï¼‰
   - ä¾èµ–AI-001çš„Elasticsearchå‘é‡å­˜å‚¨
   - æ˜¯åç»­ç”Ÿæˆå¼•æ“çš„åŸºç¡€

2. **å¹¶è¡Œå¼€å‘**:
   - RAGç³»ç»Ÿï¼ˆ2.1ï¼‰å’ŒAIåŠ©æ‰‹çŸ©é˜µï¼ˆ2.3ï¼‰å¯ä»¥å¹¶è¡Œ
   - ç”Ÿæˆå¼•æ“ï¼ˆ2.2ï¼‰éœ€è¦ç­‰å¾…RAGç³»ç»Ÿå®Œæˆ

---


---
**ğŸ“– ä¸‹ä¸€æ­¥**: [AI-003 ä¼ä¸šèƒ½åŠ›åº“å‘é‡åŒ–](./task-plan-python-ai-è¯¦ç»†-AI-003.md)
