---
文档类型: 实现文档
需求编号: REQ-2025-11-007
创建日期: 2025-11-30 13:20
创建者: claude-opus-4-1-20250805
最后更新: 2025-11-30 13:20
更新者: claude-opus-4-1-20250805
状态: 草稿
---

# 文档智能解析算法 - Python实现代码

本文档包含招标文档智能解析算法的完整Python实现代码，用于将各种格式的招标文档解析成结构化数据。

## 1. 导入依赖

```python
import os
import re
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
import hashlib

# PDF处理
import PyPDF2
import pdfplumber
from pdf2image import convert_from_path
import pytesseract

# Word处理
from docx import Document
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
from docx.shared import Pt, RGBColor

# Excel处理
import pandas as pd
import openpyxl

# 图像处理
from PIL import Image
import cv2
import numpy as np

# NLP处理
import jieba
import jieba.analyse
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

# 表格识别
import camelot
from tabula import read_pdf

# 异步处理
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
```

## 2. 数据模型定义

```python
@dataclass
class DocumentSection:
    """文档章节"""
    section_id: str
    section_type: str
    title: str
    content: str
    page_number: int
    level: int
    subsections: List['DocumentSection'] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """转换为字典"""
        return {
            'section_id': self.section_id,
            'section_type': self.section_type,
            'title': self.title,
            'content': self.content,
            'page_number': self.page_number,
            'level': self.level,
            'subsections': [s.to_dict() for s in self.subsections],
            'metadata': self.metadata
        }

@dataclass
class ParsedDocument:
    """解析后的文档"""
    document_id: str
    document_type: str
    title: str
    sections: List[DocumentSection]
    tables: List[Dict[str, Any]]
    images: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    parsing_timestamp: str
    parsing_duration: float

@dataclass
class Requirement:
    """需求项"""
    req_id: str
    req_type: str  # technical/business/qualification
    description: str
    priority: str  # high/medium/low
    mandatory: bool
    source_section: str
    keywords: List[str]
```

## 3. 文档解析器基类

```python
class DocumentParser:
    """文档解析器基类"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.section_patterns = self._load_section_patterns()
        self.keyword_extractor = KeywordExtractor()

    def parse(self, file_path: str, options: Dict[str, Any] = None) -> ParsedDocument:
        """
        解析文档

        Args:
            file_path: 文档路径
            options: 解析选项

        Returns:
            解析后的文档
        """
        raise NotImplementedError("子类必须实现parse方法")

    def _load_section_patterns(self) -> Dict[str, List[str]]:
        """加载章节识别模式"""
        return {
            'project_info': ['项目概况', '项目背景', '项目简介', '概述'],
            'technical_requirements': ['技术要求', '技术规范', '技术参数', '功能要求'],
            'commercial_terms': ['商务条款', '合同条款', '商务要求', '付款方式'],
            'evaluation_criteria': ['评标办法', '评分标准', '评审标准', '评标细则'],
            'submission_requirements': ['投标要求', '投标须知', '投标说明', '投标指南'],
            'qualification': ['资质要求', '资格条件', '投标人资格', '资质条件']
        }

    def _identify_section_type(self, title: str) -> str:
        """识别章节类型"""
        for section_type, patterns in self.section_patterns.items():
            for pattern in patterns:
                if pattern in title:
                    return section_type
        return 'other'

    def _extract_structure(self, text: str) -> List[DocumentSection]:
        """提取文档结构"""
        sections = []
        lines = text.split('\n')

        current_section = None
        current_content = []

        for i, line in enumerate(lines):
            # 识别标题
            if self._is_title(line):
                # 保存前一个章节
                if current_section:
                    current_section.content = '\n'.join(current_content)
                    sections.append(current_section)

                # 创建新章节
                level = self._get_title_level(line)
                current_section = DocumentSection(
                    section_id=f'sec_{i}',
                    section_type=self._identify_section_type(line),
                    title=line.strip(),
                    content='',
                    page_number=1,  # 需要根据实际页码更新
                    level=level
                )
                current_content = []
            else:
                current_content.append(line)

        # 保存最后一个章节
        if current_section:
            current_section.content = '\n'.join(current_content)
            sections.append(current_section)

        return self._build_hierarchy(sections)

    def _is_title(self, line: str) -> bool:
        """判断是否为标题"""
        # 编号模式
        patterns = [
            r'^第[一二三四五六七八九十]+[章节条]',
            r'^\d+\.?\d*\s+\S+',
            r'^[一二三四五六七八九十]+、',
            r'^（[一二三四五六七八九十]+）',
            r'^\([一二三四五六七八九十]+\)',
            r'^[A-Z]\.\s+\S+'
        ]

        for pattern in patterns:
            if re.match(pattern, line.strip()):
                return True

        # 格式特征（如全大写、特定长度等）
        if line.isupper() and 2 <= len(line) <= 50:
            return True

        return False

    def _get_title_level(self, title: str) -> int:
        """获取标题层级"""
        # 根据编号格式判断层级
        if re.match(r'^第[一二三四五六七八九十]+章', title):
            return 1
        elif re.match(r'^第[一二三四五六七八九十]+节', title):
            return 2
        elif re.match(r'^\d+\.?\s', title):
            return 1
        elif re.match(r'^\d+\.\d+\.?\s', title):
            return 2
        elif re.match(r'^\d+\.\d+\.\d+\.?\s', title):
            return 3
        else:
            return 2  # 默认二级

    def _build_hierarchy(self, sections: List[DocumentSection]) -> List[DocumentSection]:
        """构建层级结构"""
        if not sections:
            return []

        root_sections = []
        stack = []

        for section in sections:
            # 找到父节点
            while stack and stack[-1].level >= section.level:
                stack.pop()

            if stack:
                # 添加为子节点
                stack[-1].subsections.append(section)
            else:
                # 添加为根节点
                root_sections.append(section)

            stack.append(section)

        return root_sections
```

## 4. PDF文档解析器

```python
class PDFParser(DocumentParser):
    """PDF文档解析器"""

    def __init__(self):
        super().__init__()
        self.ocr_engine = OCREngine()

    def parse(self, file_path: str, options: Dict[str, Any] = None) -> ParsedDocument:
        """
        解析PDF文档

        Args:
            file_path: PDF文件路径
            options: 解析选项

        Returns:
            解析后的文档
        """
        start_time = datetime.now()
        options = options or {}

        try:
            # 1. 提取文本内容
            text_content = self._extract_text(file_path)

            # 2. 如果文本提取失败，使用OCR
            if not text_content or len(text_content) < 100:
                self.logger.info("文本提取内容少，使用OCR识别")
                text_content = self._extract_text_with_ocr(file_path)

            # 3. 提取表格
            tables = self._extract_tables(file_path) if options.get('extract_tables', True) else []

            # 4. 提取图片
            images = self._extract_images(file_path) if options.get('extract_images', False) else []

            # 5. 解析文档结构
            sections = self._extract_structure(text_content)

            # 6. 提取元数据
            metadata = self._extract_metadata(file_path)

            # 7. 生成文档ID
            doc_id = self._generate_document_id(file_path)

            # 8. 获取文档标题
            title = self._extract_title(sections, metadata)

            duration = (datetime.now() - start_time).total_seconds()

            return ParsedDocument(
                document_id=doc_id,
                document_type='pdf',
                title=title,
                sections=sections,
                tables=tables,
                images=images,
                metadata=metadata,
                parsing_timestamp=datetime.now().isoformat(),
                parsing_duration=duration
            )

        except Exception as e:
            self.logger.error(f"PDF解析失败: {str(e)}")
            raise DocumentParseError(f"PDF解析失败: {str(e)}")

    def _extract_text(self, file_path: str) -> str:
        """提取PDF文本"""
        text = ""

        # 尝试使用pdfplumber（更准确）
        try:
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
        except:
            # 备用方案：使用PyPDF2
            try:
                with open(file_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    for page_num in range(len(pdf_reader.pages)):
                        page = pdf_reader.pages[page_num]
                        text += page.extract_text() + "\n"
            except Exception as e:
                self.logger.error(f"PDF文本提取失败: {str(e)}")

        return text

    def _extract_text_with_ocr(self, file_path: str) -> str:
        """使用OCR提取文本"""
        text = ""

        try:
            # 将PDF转换为图片
            images = convert_from_path(file_path, dpi=300)

            # 对每页进行OCR
            for i, image in enumerate(images):
                # 图像预处理
                processed_image = self._preprocess_image(image)

                # OCR识别
                page_text = pytesseract.image_to_string(
                    processed_image,
                    lang='chi_sim+eng'
                )
                text += f"\n--- 第 {i+1} 页 ---\n{page_text}"

        except Exception as e:
            self.logger.error(f"OCR识别失败: {str(e)}")

        return text

    def _preprocess_image(self, image: Image.Image) -> np.ndarray:
        """图像预处理"""
        # 转换为OpenCV格式
        img_array = np.array(image)

        # 转为灰度图
        if len(img_array.shape) == 3:
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        else:
            gray = img_array

        # 二值化
        _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

        # 去噪
        denoised = cv2.medianBlur(binary, 3)

        # 倾斜校正
        corrected = self._deskew(denoised)

        return corrected

    def _deskew(self, image: np.ndarray) -> np.ndarray:
        """倾斜校正"""
        coords = np.column_stack(np.where(image > 0))
        angle = cv2.minAreaRect(coords)[-1]

        if angle < -45:
            angle = -(90 + angle)
        else:
            angle = -angle

        (h, w) = image.shape[:2]
        center = (w // 2, h // 2)
        M = cv2.getRotationMatrix2D(center, angle, 1.0)
        rotated = cv2.warpAffine(
            image,
            M,
            (w, h),
            flags=cv2.INTER_CUBIC,
            borderMode=cv2.BORDER_REPLICATE
        )

        return rotated

    def _extract_tables(self, file_path: str) -> List[Dict[str, Any]]:
        """提取PDF中的表格"""
        tables = []

        try:
            # 使用camelot提取表格（更精确）
            camelot_tables = camelot.read_pdf(file_path, pages='all', flavor='stream')

            for i, table in enumerate(camelot_tables):
                tables.append({
                    'table_id': f'table_{i}',
                    'page': table.page,
                    'data': table.df.to_dict('records'),
                    'accuracy': table.accuracy
                })
        except:
            # 备用方案：使用tabula
            try:
                tabula_tables = read_pdf(file_path, pages='all', multiple_tables=True)
                for i, df in enumerate(tabula_tables):
                    tables.append({
                        'table_id': f'table_{i}',
                        'page': i + 1,
                        'data': df.to_dict('records'),
                        'accuracy': 0.8  # 默认准确度
                    })
            except Exception as e:
                self.logger.error(f"表格提取失败: {str(e)}")

        return tables

    def _extract_images(self, file_path: str) -> List[Dict[str, Any]]:
        """提取PDF中的图片"""
        images = []

        try:
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    # 提取页面中的图片
                    page_images = page.images

                    for i, img in enumerate(page_images):
                        images.append({
                            'image_id': f'img_{page_num}_{i}',
                            'page': page_num + 1,
                            'bbox': img['bbox'],
                            'width': img['width'],
                            'height': img['height']
                        })
        except Exception as e:
            self.logger.error(f"图片提取失败: {str(e)}")

        return images

    def _extract_metadata(self, file_path: str) -> Dict[str, Any]:
        """提取PDF元数据"""
        metadata = {}

        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                if pdf_reader.metadata:
                    metadata = {
                        'title': pdf_reader.metadata.get('/Title', ''),
                        'author': pdf_reader.metadata.get('/Author', ''),
                        'subject': pdf_reader.metadata.get('/Subject', ''),
                        'creator': pdf_reader.metadata.get('/Creator', ''),
                        'producer': pdf_reader.metadata.get('/Producer', ''),
                        'creation_date': str(pdf_reader.metadata.get('/CreationDate', '')),
                        'modification_date': str(pdf_reader.metadata.get('/ModDate', ''))
                    }

                metadata['page_count'] = len(pdf_reader.pages)
                metadata['file_size'] = os.path.getsize(file_path)
        except Exception as e:
            self.logger.error(f"元数据提取失败: {str(e)}")

        return metadata

    def _generate_document_id(self, file_path: str) -> str:
        """生成文档ID"""
        # 基于文件内容的哈希值
        with open(file_path, 'rb') as f:
            content = f.read()
            hash_obj = hashlib.sha256(content)
            return hash_obj.hexdigest()[:16]

    def _extract_title(self,
                      sections: List[DocumentSection],
                      metadata: Dict[str, Any]) -> str:
        """提取文档标题"""
        # 优先从元数据获取
        if metadata.get('title'):
            return metadata['title']

        # 从第一个章节获取
        if sections and sections[0].title:
            return sections[0].title

        return "未知标题"
```

## 5. Word文档解析器

```python
class WordParser(DocumentParser):
    """Word文档解析器"""

    def parse(self, file_path: str, options: Dict[str, Any] = None) -> ParsedDocument:
        """
        解析Word文档

        Args:
            file_path: Word文件路径
            options: 解析选项

        Returns:
            解析后的文档
        """
        start_time = datetime.now()
        options = options or {}

        try:
            doc = Document(file_path)

            # 1. 提取文本和结构
            sections = self._extract_word_structure(doc)

            # 2. 提取表格
            tables = self._extract_word_tables(doc) if options.get('extract_tables', True) else []

            # 3. 提取图片信息
            images = self._extract_word_images(doc) if options.get('extract_images', False) else []

            # 4. 提取元数据
            metadata = self._extract_word_metadata(doc, file_path)

            # 5. 生成文档ID
            doc_id = self._generate_document_id(file_path)

            # 6. 获取标题
            title = self._extract_word_title(doc, sections)

            duration = (datetime.now() - start_time).total_seconds()

            return ParsedDocument(
                document_id=doc_id,
                document_type='word',
                title=title,
                sections=sections,
                tables=tables,
                images=images,
                metadata=metadata,
                parsing_timestamp=datetime.now().isoformat(),
                parsing_duration=duration
            )

        except Exception as e:
            self.logger.error(f"Word文档解析失败: {str(e)}")
            raise DocumentParseError(f"Word文档解析失败: {str(e)}")

    def _extract_word_structure(self, doc: Document) -> List[DocumentSection]:
        """提取Word文档结构"""
        sections = []
        current_section = None
        current_content = []
        page_number = 1

        for para in doc.paragraphs:
            # 检查是否为标题
            if para.style.name.startswith('Heading'):
                # 保存前一个章节
                if current_section:
                    current_section.content = '\n'.join(current_content)
                    sections.append(current_section)

                # 获取标题级别
                level = int(para.style.name.replace('Heading ', ''))

                # 创建新章节
                current_section = DocumentSection(
                    section_id=f'sec_{len(sections)}',
                    section_type=self._identify_section_type(para.text),
                    title=para.text,
                    content='',
                    page_number=page_number,
                    level=level
                )
                current_content = []
            else:
                # 添加到当前内容
                if para.text.strip():
                    current_content.append(para.text)

            # 简单的分页检测（可以改进）
            if '分页符' in para.text or len(current_content) > 100:
                page_number += 1

        # 保存最后一个章节
        if current_section:
            current_section.content = '\n'.join(current_content)
            sections.append(current_section)

        # 如果没有识别到标题，将整个文档作为一个章节
        if not sections:
            all_text = '\n'.join([para.text for para in doc.paragraphs if para.text.strip()])
            sections = self._extract_structure(all_text)

        return sections

    def _extract_word_tables(self, doc: Document) -> List[Dict[str, Any]]:
        """提取Word文档中的表格"""
        tables = []

        for i, table in enumerate(doc.tables):
            table_data = []

            for row in table.rows:
                row_data = []
                for cell in row.cells:
                    row_data.append(cell.text)
                table_data.append(row_data)

            # 将表格转换为DataFrame格式
            if table_data:
                df = pd.DataFrame(table_data[1:], columns=table_data[0] if len(table_data) > 0 else None)
                tables.append({
                    'table_id': f'table_{i}',
                    'data': df.to_dict('records'),
                    'rows': len(table.rows),
                    'columns': len(table.columns)
                })

        return tables

    def _extract_word_images(self, doc: Document) -> List[Dict[str, Any]]:
        """提取Word文档中的图片信息"""
        images = []

        # 遍历文档关系获取图片
        for rel in doc.part.rels.values():
            if "image" in rel.reltype:
                images.append({
                    'image_id': rel.rId,
                    'target': rel.target_ref
                })

        return images

    def _extract_word_metadata(self, doc: Document, file_path: str) -> Dict[str, Any]:
        """提取Word文档元数据"""
        metadata = {
            'file_size': os.path.getsize(file_path),
            'paragraph_count': len(doc.paragraphs),
            'table_count': len(doc.tables),
            'section_count': len(doc.sections)
        }

        # 提取核心属性
        try:
            core_properties = doc.core_properties
            metadata.update({
                'title': core_properties.title or '',
                'author': core_properties.author or '',
                'subject': core_properties.subject or '',
                'keywords': core_properties.keywords or '',
                'created': str(core_properties.created) if core_properties.created else '',
                'modified': str(core_properties.modified) if core_properties.modified else ''
            })
        except:
            pass

        return metadata

    def _extract_word_title(self, doc: Document, sections: List[DocumentSection]) -> str:
        """提取Word文档标题"""
        # 尝试从文档属性获取
        try:
            if doc.core_properties.title:
                return doc.core_properties.title
        except:
            pass

        # 从第一个标题段落获取
        for para in doc.paragraphs:
            if para.style.name == 'Title' or para.style.name == 'Heading 1':
                return para.text

        # 从章节获取
        if sections and sections[0].title:
            return sections[0].title

        return "未知标题"

    def _generate_document_id(self, file_path: str) -> str:
        """生成文档ID"""
        with open(file_path, 'rb') as f:
            content = f.read()
            hash_obj = hashlib.sha256(content)
            return hash_obj.hexdigest()[:16]
```

## 6. 需求提取器

```python
class RequirementExtractor:
    """需求提取器"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.requirement_patterns = self._load_requirement_patterns()

    def extract_requirements(self,
                            parsed_doc: ParsedDocument) -> List[Requirement]:
        """
        从解析后的文档中提取需求

        Args:
            parsed_doc: 解析后的文档

        Returns:
            需求列表
        """
        requirements = []

        # 遍历所有章节
        for section in self._flatten_sections(parsed_doc.sections):
            # 提取该章节的需求
            section_reqs = self._extract_section_requirements(section)
            requirements.extend(section_reqs)

        # 去重和合并
        requirements = self._merge_requirements(requirements)

        # 优先级排序
        requirements = self._prioritize_requirements(requirements)

        return requirements

    def _load_requirement_patterns(self) -> Dict[str, List[str]]:
        """加载需求识别模式"""
        return {
            'mandatory': ['必须', '应当', '需要', '要求', '不得', '禁止'],
            'technical': ['功能', '性能', '接口', '兼容', '支持', '实现'],
            'business': ['交付', '工期', '付款', '质保', '维护', '培训'],
            'qualification': ['资质', '资格', '认证', '经验', '业绩', '人员']
        }

    def _flatten_sections(self, sections: List[DocumentSection]) -> List[DocumentSection]:
        """扁平化章节结构"""
        flat = []
        for section in sections:
            flat.append(section)
            if section.subsections:
                flat.extend(self._flatten_sections(section.subsections))
        return flat

    def _extract_section_requirements(self, section: DocumentSection) -> List[Requirement]:
        """从章节中提取需求"""
        requirements = []

        # 分句
        sentences = self._split_sentences(section.content)

        for i, sentence in enumerate(sentences):
            # 判断是否为需求句
            if self._is_requirement_sentence(sentence):
                req_type = self._identify_requirement_type(sentence)
                priority = self._assess_priority(sentence)
                mandatory = self._is_mandatory(sentence)
                keywords = self._extract_keywords(sentence)

                requirement = Requirement(
                    req_id=f'req_{section.section_id}_{i}',
                    req_type=req_type,
                    description=sentence,
                    priority=priority,
                    mandatory=mandatory,
                    source_section=section.title,
                    keywords=keywords
                )
                requirements.append(requirement)

        return requirements

    def _split_sentences(self, text: str) -> List[str]:
        """分句"""
        # 简单的分句规则
        sentences = re.split(r'[。；]', text)
        return [s.strip() for s in sentences if s.strip()]

    def _is_requirement_sentence(self, sentence: str) -> bool:
        """判断是否为需求句"""
        # 检查是否包含需求关键词
        for patterns in self.requirement_patterns.values():
            for pattern in patterns:
                if pattern in sentence:
                    return True
        return False

    def _identify_requirement_type(self, sentence: str) -> str:
        """识别需求类型"""
        max_count = 0
        req_type = 'other'

        for type_name, patterns in self.requirement_patterns.items():
            count = sum(1 for p in patterns if p in sentence)
            if count > max_count:
                max_count = count
                req_type = type_name

        return req_type

    def _assess_priority(self, sentence: str) -> str:
        """评估需求优先级"""
        high_priority_words = ['必须', '应当', '核心', '关键', '重要']
        medium_priority_words = ['需要', '应该', '建议']

        for word in high_priority_words:
            if word in sentence:
                return 'high'

        for word in medium_priority_words:
            if word in sentence:
                return 'medium'

        return 'low'

    def _is_mandatory(self, sentence: str) -> bool:
        """判断是否为强制性需求"""
        mandatory_words = ['必须', '应当', '不得', '禁止', '强制']
        return any(word in sentence for word in mandatory_words)

    def _extract_keywords(self, sentence: str) -> List[str]:
        """提取关键词"""
        # 使用jieba提取关键词
        keywords = jieba.analyse.extract_tags(sentence, topK=5)
        return keywords

    def _merge_requirements(self, requirements: List[Requirement]) -> List[Requirement]:
        """合并重复需求"""
        merged = []
        seen_descriptions = set()

        for req in requirements:
            # 简单的去重策略
            if req.description not in seen_descriptions:
                merged.append(req)
                seen_descriptions.add(req.description)

        return merged

    def _prioritize_requirements(self, requirements: List[Requirement]) -> List[Requirement]:
        """按优先级排序需求"""
        priority_order = {'high': 0, 'medium': 1, 'low': 2}
        return sorted(requirements, key=lambda r: (priority_order.get(r.priority, 3), not r.mandatory))
```

## 7. 关键词提取器

```python
class KeywordExtractor:
    """关键词提取器"""

    def __init__(self):
        self.tfidf_model = None

    def extract_keywords(self, text: str, top_k: int = 10) -> List[str]:
        """
        提取关键词

        Args:
            text: 文本
            top_k: 返回前K个关键词

        Returns:
            关键词列表
        """
        # 使用多种方法提取关键词
        tfidf_keywords = self._extract_tfidf_keywords(text, top_k)
        textrank_keywords = self._extract_textrank_keywords(text, top_k)

        # 合并结果
        all_keywords = list(set(tfidf_keywords + textrank_keywords))

        # 按重要性排序
        scored_keywords = []
        for kw in all_keywords:
            score = self._calculate_keyword_score(kw, text)
            scored_keywords.append((kw, score))

        scored_keywords.sort(key=lambda x: x[1], reverse=True)

        return [kw for kw, _ in scored_keywords[:top_k]]

    def _extract_tfidf_keywords(self, text: str, top_k: int) -> List[str]:
        """使用TF-IDF提取关键词"""
        return jieba.analyse.extract_tags(text, topK=top_k)

    def _extract_textrank_keywords(self, text: str, top_k: int) -> List[str]:
        """使用TextRank提取关键词"""
        return jieba.analyse.textrank(text, topK=top_k)

    def _calculate_keyword_score(self, keyword: str, text: str) -> float:
        """计算关键词分数"""
        # 简单的评分：词频 * 词长
        frequency = text.count(keyword)
        length_factor = min(len(keyword) / 10, 1.0)
        return frequency * length_factor
```

## 8. OCR引擎

```python
class OCREngine:
    """OCR引擎"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def recognize_text(self, image: np.ndarray, lang: str = 'chi_sim+eng') -> str:
        """
        识别图像中的文本

        Args:
            image: 图像数组
            lang: 语言

        Returns:
            识别的文本
        """
        try:
            # 图像预处理
            processed = self._preprocess_image(image)

            # OCR识别
            text = pytesseract.image_to_string(processed, lang=lang)

            # 后处理
            text = self._postprocess_text(text)

            return text
        except Exception as e:
            self.logger.error(f"OCR识别失败: {str(e)}")
            return ""

    def _preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """图像预处理"""
        # 转灰度
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image

        # 二值化
        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        # 去噪
        denoised = cv2.medianBlur(binary, 3)

        # 形态学操作
        kernel = np.ones((2, 2), np.uint8)
        morph = cv2.morphologyEx(denoised, cv2.MORPH_CLOSE, kernel)

        return morph

    def _postprocess_text(self, text: str) -> str:
        """文本后处理"""
        # 去除多余空白
        text = re.sub(r'\s+', ' ', text)

        # 修正常见OCR错误
        corrections = {
            '0': 'O',  # 数字0误识别为字母O
            '1': 'l',  # 数字1误识别为字母l
            # 添加更多纠错规则
        }

        # 这里简化处理，实际应该使用更复杂的纠错算法
        return text.strip()
```

## 9. 异常处理

```python
class DocumentParseError(Exception):
    """文档解析异常"""
    pass

class OCRError(Exception):
    """OCR识别异常"""
    pass

class RequirementExtractionError(Exception):
    """需求提取异常"""
    pass
```

## 10. 使用示例

```python
async def main():
    """主程序入口"""
    # 初始化解析器
    pdf_parser = PDFParser()
    word_parser = WordParser()
    req_extractor = RequirementExtractor()

    # 解析选项
    options = {
        'extract_tables': True,
        'extract_images': False,
        'ocr_enabled': True
    }

    # 解析PDF文档
    try:
        pdf_doc = pdf_parser.parse('bidding_document.pdf', options)
        print(f"PDF文档解析成功: {pdf_doc.title}")
        print(f"章节数: {len(pdf_doc.sections)}")
        print(f"表格数: {len(pdf_doc.tables)}")

        # 提取需求
        requirements = req_extractor.extract_requirements(pdf_doc)
        print(f"提取需求数: {len(requirements)}")

        # 输出需求
        for req in requirements[:10]:
            print(f"- [{req.priority}] {req.description[:50]}...")

    except DocumentParseError as e:
        print(f"文档解析失败: {e}")

    # 解析Word文档
    try:
        word_doc = word_parser.parse('bidding_document.docx', options)
        print(f"\nWord文档解析成功: {word_doc.title}")

    except DocumentParseError as e:
        print(f"Word文档解析失败: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 修改历史

| 日期 | 版本 | 修改者 | 修改内容概要 |
|------|------|--------|-------------|
| 2025-11-30 13:20 | 1.0 | claude-opus-4-1-20250805 | 创建文档智能解析算法实现代码 |