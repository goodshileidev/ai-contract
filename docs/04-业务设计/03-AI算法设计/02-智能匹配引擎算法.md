---
æ–‡æ¡£ç±»å‹: è®¾è®¡æ–‡æ¡£
éœ€æ±‚ç¼–å·: DOC-2025-11-001
åˆ›å»ºæ—¥æœŸ: 2025-11-29
åˆ›å»ºè€…: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
æœ€åæ›´æ–°: 2025-11-29
æ›´æ–°è€…: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
çŠ¶æ€: å·²æ‰¹å‡†
---

# æ™ºèƒ½åŒ¹é…å¼•æ“ç®—æ³•

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°æ™ºèƒ½åŒ¹é…å¼•æ“çš„æ ¸å¿ƒç®—æ³•ï¼Œç”¨äºå°†æ‹›æ ‡éœ€æ±‚ä¸ä¼ä¸šèƒ½åŠ›è¿›è¡Œæ™ºèƒ½åŒ¹é…ï¼Œè¯†åˆ«å·®è·å’Œä¼˜åŠ¿ï¼Œå¹¶æä¾›æŠ•æ ‡å»ºè®®ã€‚

## ğŸ“‹ ç›®å½•å¯¼èˆª

- [è¿”å›æ€»è§ˆ](./00-AIç®—æ³•æ€»è§ˆ.md)
- [æ–‡æ¡£æ™ºèƒ½è§£æç®—æ³•](./01-æ–‡æ¡£æ™ºèƒ½è§£æç®—æ³•.md)

## ğŸ¯ ç®—æ³•æ¦‚è¿°

æ™ºèƒ½åŒ¹é…å¼•æ“æ˜¯æ ‡ä¹¦åˆ›ä½œå¹³å°çš„æ ¸å¿ƒç®—æ³•ä¹‹ä¸€ï¼Œè´Ÿè´£å°†æ‹›æ ‡æ–‡æ¡£ä¸­æå–çš„éœ€æ±‚ä¸ä¼ä¸šçš„äº§å“ã€æœåŠ¡ã€æ¡ˆä¾‹ã€äººå‘˜ç­‰èƒ½åŠ›è¿›è¡Œå¤šç»´åº¦åŒ¹é…ï¼Œä¸ºæŠ•æ ‡å†³ç­–æä¾›æ•°æ®æ”¯æŒã€‚

### æ ¸å¿ƒåŠŸèƒ½

1. **å¤šç»´åº¦åŒ¹é…**ï¼šç»¼åˆè¯­ä¹‰ç›¸ä¼¼åº¦ã€å…³é”®è¯åŒ¹é…ã€ç»éªŒåŒ¹é…ã€è®¤è¯åŒ¹é…å››ä¸ªç»´åº¦
2. **å·®è·åˆ†æ**ï¼šè¯†åˆ«èƒ½åŠ›å·®è·å¹¶åˆ†ç±»ï¼ˆèƒ½åŠ›ã€ç»éªŒã€èµ„æºã€è®¤è¯ï¼‰
3. **é£é™©è¯„ä¼°**ï¼šè¯„ä¼°æ¯ä¸ªéœ€æ±‚çš„æŠ•æ ‡é£é™©å¹¶ç»™å‡ºç¼“è§£å»ºè®®
4. **æ™ºèƒ½æ¨è**ï¼šåŸºäºåŒ¹é…ç»“æœç”ŸæˆæŠ•æ ‡ç­–ç•¥å»ºè®®

## 1. ä¼ä¸šèƒ½åŠ›åŒ¹é…ç®—æ³•

### 1.1 æ•°æ®æ¨¡å‹

```python
import numpy as np
from typing import Dict, List, Any, Tuple
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import networkx as nx
from dataclasses import dataclass

@dataclass
class CapabilityMatch:
    """èƒ½åŠ›åŒ¹é…ç»“æœ"""
    capability_id: str
    capability_name: str
    match_score: float
    match_details: Dict[str, Any]
    gaps: List[str]
    strengths: List[str]

@dataclass
class RequirementMatch:
    """éœ€æ±‚åŒ¹é…ç»“æœ"""
    requirement_id: str
    requirement_text: str
    matched_capabilities: List[CapabilityMatch]
    overall_match_score: float
    coverage_percentage: float
    risk_assessment: Dict[str, Any]
```

### 1.2 æ ¸å¿ƒåŒ¹é…å¼•æ“

```python
class IntelligentMatchingEngine:
    """æ™ºèƒ½åŒ¹é…å¼•æ“"""

    def __init__(self):
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            stop_words=None
        )

        # åŒ¹é…æƒé‡é…ç½®
        self.match_weights = {
            'semantic_similarity': 0.4,
            'keyword_matching': 0.3,
            'experience_matching': 0.2,
            'certification_matching': 0.1
        }

        # èƒ½åŠ›ç±»å‹æƒé‡
        self.capability_type_weights = {
            'product': 0.3,
            'service': 0.3,
            'project': 0.2,
            'personnel': 0.1,
            'technology': 0.1
        }

    async def match_requirements_to_capabilities(self,
                                                requirements: List[str],
                                                company_capabilities: List[Dict[str, Any]],
                                                matching_criteria: List[str] = None) -> Dict[str, Any]:
        """
        å°†éœ€æ±‚åŒ¹é…åˆ°ä¼ä¸šèƒ½åŠ›

        Args:
            requirements: éœ€æ±‚åˆ—è¡¨
            company_capabilities: ä¼ä¸šèƒ½åŠ›åˆ—è¡¨
            matching_criteria: åŒ¹é…æ¡ä»¶

        Returns:
            åŒ¹é…ç»“æœ
        """
        try:
            # 1. é¢„å¤„ç†èƒ½åŠ›æ•°æ®
            processed_capabilities = await self._preprocess_capabilities(company_capabilities)

            # 2. è®¡ç®—éœ€æ±‚-èƒ½åŠ›åŒ¹é…çŸ©é˜µ
            match_matrix = await self._calculate_match_matrix(requirements, processed_capabilities)

            # 3. åˆ†æåŒ¹é…ç»“æœ
            match_analysis = await self._analyze_match_results(match_matrix, requirements, processed_capabilities)

            # 4. è¯†åˆ«å·®è·å’Œä¼˜åŠ¿
            gap_analysis = await self._analyze_gaps(match_analysis, requirements)

            # 5. ç”ŸæˆåŒ¹é…å»ºè®®
            recommendations = await self._generate_recommendations(match_analysis, gap_analysis)

            # 6. è®¡ç®—æ•´ä½“åŒ¹é…åˆ†æ•°
            overall_metrics = self._calculate_overall_metrics(match_analysis)

            return {
                'match_analysis': match_analysis,
                'gap_analysis': gap_analysis,
                'recommendations': recommendations,
                'overall_metrics': overall_metrics,
                'matching_metadata': {
                    'total_requirements': len(requirements),
                    'total_capabilities': len(processed_capabilities),
                    'processing_time': 0  # å®é™…è®¡ç®—
                }
            }

        except Exception as e:
            raise MatchingError(f"èƒ½åŠ›åŒ¹é…å¤±è´¥: {str(e)}")

    async def _preprocess_capabilities(self, capabilities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é¢„å¤„ç†ä¼ä¸šèƒ½åŠ›æ•°æ®"""
        processed = []

        for capability in capabilities:
            # æ ‡å‡†åŒ–æ–‡æœ¬
            description = capability.get('description', '')
            features = ' '.join(capability.get('features', []))
            benefits = ' '.join(capability.get('benefits', []))

            # åˆå¹¶æ–‡æœ¬å†…å®¹
            combined_text = f"{capability['name']} {description} {features} {benefits}"

            processed_capability = {
                'id': capability['id'],
                'name': capability['name'],
                'type': capability['type'],
                'combined_text': combined_text,
                'original': capability,
                'keywords': self._extract_keywords(combined_text),
                'proficiency_level': capability.get('proficiency_level', 3),
                'experience_years': capability.get('experience_years', 0),
                'case_study_count': capability.get('case_study_count', 0),
                'tags': capability.get('tags', [])
            }

            processed.append(processed_capability)

        return processed

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ä½¿ç”¨TF-IDFæå–å…³é”®è¯
        try:
            tfidf_matrix = self.tfidf_vectorizer.fit_transform([text])
            feature_names = self.tfidf_vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]

            # è·å–å‰20ä¸ªå…³é”®è¯
            top_indices = np.argsort(tfidf_scores)[-20:][::-1]
            keywords = [feature_names[i] for i in top_indices if tfidf_scores[i] > 0]

            return keywords
        except:
            return []
```

### 1.3 å¤šç»´åº¦åŒ¹é…è®¡ç®—

#### 1.3.1 è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…

ä½¿ç”¨TF-IDFå‘é‡åŒ–å’Œä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—éœ€æ±‚ä¸èƒ½åŠ›çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼š

```python
async def _calculate_match_matrix(self,
                                requirements: List[str],
                                capabilities: List[Dict[str, Any]]) -> np.ndarray:
    """è®¡ç®—éœ€æ±‚-èƒ½åŠ›åŒ¹é…çŸ©é˜µ"""
    # å‡†å¤‡æ–‡æœ¬æ•°æ®
    all_texts = requirements + [cap['combined_text'] for cap in capabilities]

    # è®¡ç®—TF-IDFçŸ©é˜µ
    tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_texts)

    # åˆ†ç¦»éœ€æ±‚å’Œèƒ½åŠ›å‘é‡
    req_vectors = tfidf_matrix[:len(requirements)]
    cap_vectors = tfidf_matrix[len(requirements):]

    # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ
    semantic_similarity_matrix = cosine_similarity(req_vectors, cap_vectors)

    # è®¡ç®—å…³é”®è¯åŒ¹é…çŸ©é˜µ
    keyword_match_matrix = self._calculate_keyword_matches(requirements, capabilities)

    # è®¡ç®—ç»éªŒåŒ¹é…çŸ©é˜µ
    experience_match_matrix = self._calculate_experience_matches(requirements, capabilities)

    # è®¡ç®—è®¤è¯åŒ¹é…çŸ©é˜µ
    certification_match_matrix = self._calculate_certification_matches(requirements, capabilities)

    # åŠ æƒç»„åˆæ‰€æœ‰åŒ¹é…åˆ†æ•°
    final_match_matrix = (
        semantic_similarity_matrix * self.match_weights['semantic_similarity'] +
        keyword_match_matrix * self.match_weights['keyword_matching'] +
        experience_match_matrix * self.match_weights['experience_matching'] +
        certification_match_matrix * self.match_weights['certification_matching']
    )

    return final_match_matrix
```

**æƒé‡é…ç½®**ï¼š
- è¯­ä¹‰ç›¸ä¼¼åº¦ï¼š40% - æœ€é‡è¦ï¼Œæ•æ‰æ•´ä½“è¯­ä¹‰
- å…³é”®è¯åŒ¹é…ï¼š30% - æ•æ‰å…·ä½“è¦æ±‚
- ç»éªŒåŒ¹é…ï¼š20% - è¯„ä¼°å®æ–½èƒ½åŠ›
- è®¤è¯åŒ¹é…ï¼š10% - éªŒè¯åˆè§„æ€§

#### 1.3.2 å…³é”®è¯åŒ¹é…

```python
def _calculate_keyword_matches(self, requirements: List[str], capabilities: List[Dict[str, Any]]) -> np.ndarray:
    """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
    matrix = np.zeros((len(requirements), len(capabilities)))

    # ä¸ºæ¯ä¸ªéœ€æ±‚æå–å…³é”®è¯
    req_keywords_list = []
    for req in requirements:
        req_keywords = self._extract_keywords(req)
        req_keywords_list.append(set(req_keywords))

    # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
    for i, req_keywords in enumerate(req_keywords_list):
        for j, capability in enumerate(capabilities):
            cap_keywords = set(capability['keywords'])
            cap_tags = set(capability['tags'])

            # è®¡ç®—å…³é”®è¯é‡å åº¦
            all_keywords = req_keywords.union(cap_keywords).union(cap_tags)
            if len(all_keywords) > 0:
                overlap = len(req_keywords.intersection(cap_keywords).union(req_keywords.intersection(cap_tags)))
                matrix[i][j] = overlap / len(all_keywords)

    return matrix
```

**åŒ¹é…ç­–ç•¥**ï¼š
- ä½¿ç”¨Jaccardç³»æ•°è®¡ç®—å…³é”®è¯é›†åˆçš„é‡å åº¦
- åŒæ—¶è€ƒè™‘èƒ½åŠ›çš„å…³é”®è¯å’Œæ ‡ç­¾
- å½’ä¸€åŒ–åˆ°0-1èŒƒå›´

#### 1.3.3 ç»éªŒåŒ¹é…

```python
def _calculate_experience_matches(self, requirements: List[str], capabilities: List[Dict[str, Any]]) -> np.ndarray:
    """è®¡ç®—ç»éªŒåŒ¹é…åˆ†æ•°"""
    matrix = np.zeros((len(requirements), len(capabilities)))

    # å®šä¹‰ç»éªŒå…³é”®è¯
    experience_keywords = {
        'senior': ['5å¹´', 'é«˜çº§', 'èµ„æ·±', 'ä¸“å®¶'],
        'intermediate': ['3å¹´', 'ä¸­çº§', 'ç†Ÿç»ƒ'],
        'junior': ['1å¹´', 'åˆçº§', 'åŸºç¡€']
    }

    for i, req in enumerate(requirements):
        for j, capability in enumerate(capabilities):
            # åŸºäºèƒ½åŠ›ç±»å‹çš„ç»éªŒåŒ¹é…
            base_score = 0

            if capability['type'] in ['project', 'personnel']:
                # é¡¹ç›®å’Œäººå‘˜èƒ½åŠ›æ›´æ³¨é‡ç»éªŒ
                experience_score = min(capability['experience_years'] / 10, 1.0)
                case_study_score = min(capability['case_study_count'] / 20, 1.0)
                proficiency_score = capability['proficiency_level'] / 5

                base_score = (experience_score * 0.4 + case_study_score * 0.3 + proficiency_score * 0.3)
            else:
                # äº§å“å’ŒæœåŠ¡èƒ½åŠ›
                base_score = capability['proficiency_level'] / 5

            # æ£€æŸ¥éœ€æ±‚ä¸­çš„ç»éªŒè¦æ±‚
            req_experience_level = self._detect_experience_requirement(req)
            if req_experience_level:
                if req_experience_level == 'senior' and capability['experience_years'] >= 5:
                    base_score *= 1.2
                elif req_experience_level == 'intermediate' and capability['experience_years'] >= 3:
                    base_score *= 1.1

            matrix[i][j] = min(base_score, 1.0)

    return matrix
```

**è¯„åˆ†ç»´åº¦**ï¼š
- **ç»éªŒå¹´é™**ï¼šå½’ä¸€åŒ–åˆ°0-1ï¼ˆ10å¹´ä¸ºæ»¡åˆ†ï¼‰
- **æ¡ˆä¾‹æ•°é‡**ï¼šå½’ä¸€åŒ–åˆ°0-1ï¼ˆ20ä¸ªæ¡ˆä¾‹ä¸ºæ»¡åˆ†ï¼‰
- **ç†Ÿç»ƒåº¦**ï¼š1-5çº§è½¬æ¢ä¸º0-1
- **åŠ æƒå…¬å¼**ï¼šç»éªŒ40% + æ¡ˆä¾‹30% + ç†Ÿç»ƒåº¦30%

#### 1.3.4 è®¤è¯åŒ¹é…

```python
def _calculate_certification_matches(self, requirements: List[str], capabilities: List[Dict[str, Any]]) -> np.ndarray:
    """è®¡ç®—è®¤è¯åŒ¹é…åˆ†æ•°"""
    matrix = np.zeros((len(requirements), len(capabilities)))

    # å®šä¹‰è®¤è¯å…³é”®è¯
    certification_keywords = [
        'ISO', 'CMMI', 'PMP', 'è®¤è¯', 'èµ„è´¨', 'è¯ä¹¦', 'è®¸å¯è¯',
        'å®‰å…¨è®¤è¯', 'è´¨é‡è®¤è¯', 'è¡Œä¸šè®¤è¯', 'ä¸“ä¸šè®¤è¯'
    ]

    for i, req in enumerate(requirements):
        # æ£€æŸ¥éœ€æ±‚ä¸­çš„è®¤è¯è¦æ±‚
        req_certifications = set()
        for keyword in certification_keywords:
            if keyword in req:
                req_certifications.add(keyword)

        for j, capability in enumerate(capabilities):
            # æ£€æŸ¥èƒ½åŠ›çš„è®¤è¯æƒ…å†µ
            cap_certifications = set()
            cap_data = capability['original']

            if 'certifications' in cap_data:
                for cert in cap_data['certifications']:
                    for keyword in certification_keywords:
                        if keyword in cert.get('name', ''):
                            cap_certifications.add(keyword)

            # è®¡ç®—è®¤è¯åŒ¹é…åº¦
            if len(req_certifications) > 0:
                overlap = len(req_certifications.intersection(cap_certifications))
                matrix[i][j] = overlap / len(req_certifications)
            else:
                # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„è®¤è¯è¦æ±‚ï¼ŒåŸºäºèƒ½åŠ›ç±»å‹ç»™åŸºç¡€åˆ†æ•°
                if capability['type'] in ['service', 'technology']:
                    matrix[i][j] = 0.3  # æœåŠ¡å’ŒæŠ€æœ¯é€šå¸¸éœ€è¦è®¤è¯
                else:
                    matrix[i][j] = 0.1

    return matrix
```

### 1.4 åŒ¹é…ç»“æœåˆ†æ

```python
async def _analyze_match_results(self,
                               match_matrix: np.ndarray,
                               requirements: List[str],
                               capabilities: List[Dict[str, Any]]) -> List[RequirementMatch]:
    """åˆ†æåŒ¹é…ç»“æœ"""
    requirement_matches = []

    # è®¾ç½®åŒ¹é…é˜ˆå€¼
    match_threshold = 0.3

    for i, requirement in enumerate(requirements):
        # è·å–è¯¥éœ€æ±‚çš„æ‰€æœ‰åŒ¹é…åˆ†æ•°
        match_scores = match_matrix[i]

        # æ‰¾åˆ°åŒ¹é…çš„èƒ½åŠ›
        matched_capabilities = []
        matched_indices = np.where(match_scores >= match_threshold)[0]

        for j in matched_indices:
            match_score = match_scores[j]
            capability = capabilities[j]

            # åˆ›å»ºèƒ½åŠ›åŒ¹é…å¯¹è±¡
            capability_match = CapabilityMatch(
                capability_id=capability['id'],
                capability_name=capability['name'],
                match_score=match_score,
                match_details=self._analyze_capability_match(requirement, capability, match_score),
                gaps=self._identify_capability_gaps(requirement, capability),
                strengths=self._identify_capability_strengths(requirement, capability)
            )

            matched_capabilities.append(capability_match)

        # æŒ‰åŒ¹é…åˆ†æ•°æ’åº
        matched_capabilities.sort(key=lambda x: x.match_score, reverse=True)

        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        if matched_capabilities:
            overall_match_score = max(cap.match_score for cap in matched_capabilities)
            coverage_percentage = len(matched_capabilities) / len(capabilities) * 100
        else:
            overall_match_score = 0
            coverage_percentage = 0

        # é£é™©è¯„ä¼°
        risk_assessment = self._assess_requirement_risk(requirement, matched_capabilities)

        requirement_match = RequirementMatch(
            requirement_id=str(i),
            requirement_text=requirement,
            matched_capabilities=matched_capabilities,
            overall_match_score=overall_match_score,
            coverage_percentage=coverage_percentage,
            risk_assessment=risk_assessment
        )

        requirement_matches.append(requirement_match)

    return requirement_matches
```

### 1.5 å·®è·å’Œä¼˜åŠ¿è¯†åˆ«

#### å·®è·è¯†åˆ«

```python
def _identify_capability_gaps(self, requirement: str, capability: Dict[str, Any]) -> List[str]:
    """è¯†åˆ«èƒ½åŠ›å·®è·"""
    gaps = []

    # åŸºäºç»éªŒå¹´é™çš„å·®è·
    if capability['experience_years'] < 3:
        gaps.append("ç»éªŒç›¸å¯¹è¾ƒå°‘")

    # åŸºäºæ¡ˆä¾‹æ•°é‡çš„å·®è·
    if capability['case_study_count'] < 5:
        gaps.append("æˆåŠŸæ¡ˆä¾‹è¾ƒå°‘")

    # åŸºäºèƒ½åŠ›ç­‰çº§çš„å·®è·
    if capability['proficiency_level'] < 3:
        gaps.append("èƒ½åŠ›ç†Ÿç»ƒåº¦æœ‰å¾…æå‡")

    # åŸºäºå…³é”®è¯è¦†ç›–çš„å·®è·
    req_keywords = set(self._extract_keywords(requirement))
    cap_keywords = set(capability['keywords'])
    uncovered_keywords = req_keywords - cap_keywords

    if uncovered_keywords and len(uncovered_keywords) <= 5:
        gaps.append(f"æœªè¦†ç›–å…³é”®è¯: {', '.join(list(uncovered_keywords)[:3])}")

    return gaps
```

#### ä¼˜åŠ¿è¯†åˆ«

```python
def _identify_capability_strengths(self, requirement: str, capability: Dict[str, Any]) -> List[str]:
    """è¯†åˆ«èƒ½åŠ›ä¼˜åŠ¿"""
    strengths = []

    # åŸºäºç»éªŒçš„ä¼˜åŠ¿
    if capability['experience_years'] >= 5:
        strengths.append("ç»éªŒä¸°å¯Œ")

    # åŸºäºæ¡ˆä¾‹çš„ä¼˜åŠ¿
    if capability['case_study_count'] >= 10:
        strengths.append("æˆåŠŸæ¡ˆä¾‹ä¸°å¯Œ")

    # åŸºäºèƒ½åŠ›ç­‰çº§çš„ä¼˜åŠ¿
    if capability['proficiency_level'] >= 4:
        strengths.append("ä¸“ä¸šèƒ½åŠ›çªå‡º")

    # åŸºäºåŒ¹é…åº¦çš„ä¼˜åŠ¿
    if capability['type'] in ['product', 'service']:
        strengths.append("æ ¸å¿ƒäº§å“/æœåŠ¡èƒ½åŠ›")

    return strengths
```

### 1.6 é£é™©è¯„ä¼°

```python
def _assess_requirement_risk(self, requirement: str, matched_capabilities: List[CapabilityMatch]) -> Dict[str, Any]:
    """è¯„ä¼°éœ€æ±‚é£é™©"""
    if not matched_capabilities:
        return {
            'risk_level': 'high',
            'risk_score': 1.0,
            'risk_factors': ['æ— åŒ¹é…èƒ½åŠ›', 'å¯èƒ½éœ€è¦å¤–éƒ¨èµ„æº'],
            'mitigation_suggestions': ['è€ƒè™‘å¤–åŒ…', 'é‡æ–°è¯„ä¼°éœ€æ±‚', 'å¯»æ‰¾åˆä½œä¼™ä¼´']
        }

    # è®¡ç®—é£é™©æŒ‡æ ‡
    best_match_score = matched_capabilities[0].match_score
    match_count = len(matched_capabilities)

    # é£é™©å› ç´ 
    risk_factors = []

    if best_match_score < 0.5:
        risk_factors.append('æœ€ä½³åŒ¹é…åˆ†æ•°è¾ƒä½')

    if match_count < 2:
        risk_factors.append('å¤‡é€‰èƒ½åŠ›ä¸è¶³')

    # æ£€æŸ¥èƒ½åŠ›å·®è·
    total_gaps = sum(len(cap.gaps) for cap in matched_capabilities)
    if total_gaps > 0:
        risk_factors.append('å­˜åœ¨èƒ½åŠ›å·®è·')

    # è®¡ç®—é£é™©åˆ†æ•°
    risk_score = 1.0 - (best_match_score * 0.6 + min(match_count / 5, 1.0) * 0.4)

    # ç¡®å®šé£é™©ç­‰çº§
    if risk_score >= 0.7:
        risk_level = 'high'
    elif risk_score >= 0.4:
        risk_level = 'medium'
    else:
        risk_level = 'low'

    # ç”Ÿæˆç¼“è§£å»ºè®®
    mitigation_suggestions = self._generate_risk_mitigation_suggestions(risk_factors, matched_capabilities)

    return {
        'risk_level': risk_level,
        'risk_score': risk_score,
        'risk_factors': risk_factors,
        'mitigation_suggestions': mitigation_suggestions
    }
```

**é£é™©ç­‰çº§åˆ’åˆ†**ï¼š
- **é«˜é£é™©**ï¼ˆâ‰¥0.7ï¼‰ï¼šæœ€ä½³åŒ¹é…åˆ†æ•°<0.5ï¼Œå¤‡é€‰èƒ½åŠ›ä¸è¶³
- **ä¸­é£é™©**ï¼ˆ0.4-0.7ï¼‰ï¼šåŒ¹é…åˆ†æ•°ä¸€èˆ¬ï¼Œå­˜åœ¨èƒ½åŠ›å·®è·
- **ä½é£é™©**ï¼ˆ<0.4ï¼‰ï¼šåŒ¹é…åº¦é«˜ï¼Œå¤‡é€‰å……è¶³

### 1.7 å·®è·åˆ†æ

```python
async def _analyze_gaps(self, match_analysis: List[RequirementMatch], requirements: List[str]) -> Dict[str, Any]:
    """åˆ†æèƒ½åŠ›å·®è·"""
    # æ‰¾å‡ºæœªåŒ¹é…æˆ–ä½åŒ¹é…çš„éœ€æ±‚
    unmatched_requirements = []
    low_match_requirements = []

    for req_match in match_analysis:
        if req_match.overall_match_score < 0.3:
            unmatched_requirements.append(req_match)
        elif req_match.overall_match_score < 0.6:
            low_match_requirements.append(req_match)

    # åˆ†æå·®è·ç±»å‹
    gap_types = {
        'capability_gaps': [],
        'experience_gaps': [],
        'resource_gaps': [],
        'certification_gaps': []
    }

    for req_match in low_match_requirements:
        for cap_match in req_match.matched_capabilities:
            for gap in cap_match.gaps:
                if 'ç»éªŒ' in gap:
                    gap_types['experience_gaps'].append({
                        'requirement_id': req_match.requirement_id,
                        'capability_id': cap_match.capability_id,
                        'gap_description': gap
                    })
                elif 'æ¡ˆä¾‹' in gap:
                    gap_types['capability_gaps'].append({
                        'requirement_id': req_match.requirement_id,
                        'capability_id': cap_match.capability_id,
                        'gap_description': gap
                    })
                elif 'è®¤è¯' in gap or 'èµ„è´¨' in gap:
                    gap_types['certification_gaps'].append({
                        'requirement_id': req_match.requirement_id,
                        'capability_id': cap_match.capability_id,
                        'gap_description': gap
                    })
                else:
                    gap_types['resource_gaps'].append({
                        'requirement_id': req_match.requirement_id,
                        'capability_id': cap_match.capability_id,
                        'gap_description': gap
                    })

    # è®¡ç®—å·®è·ä¸¥é‡ç¨‹åº¦
    total_requirements = len(requirements)
    gap_severity = {
        'critical': len(unmatched_requirements) / total_requirements,
        'high': len(low_match_requirements) / total_requirements,
        'medium': 0,  # å¯ä»¥æ ¹æ®å…¶ä»–æŒ‡æ ‡è®¡ç®—
        'low': 0
    }

    return {
        'unmatched_requirements': unmatched_requirements,
        'low_match_requirements': low_match_requirements,
        'gap_types': gap_types,
        'gap_severity': gap_severity,
        'gap_summary': self._generate_gap_summary(gap_types, gap_severity)
    }
```

**å·®è·åˆ†ç±»**ï¼š
- **èƒ½åŠ›å·®è·**ï¼šæˆåŠŸæ¡ˆä¾‹ä¸è¶³ã€æŠ€æœ¯èƒ½åŠ›æ¬ ç¼º
- **ç»éªŒå·®è·**ï¼šç»éªŒå¹´é™ä¸å¤Ÿã€é¡¹ç›®ç»éªŒå°‘
- **èµ„æºå·®è·**ï¼šäººåŠ›èµ„æºä¸è¶³ã€è®¾å¤‡ä¸å¤Ÿ
- **è®¤è¯å·®è·**ï¼šç¼ºå°‘å¿…è¦çš„è¡Œä¸šè®¤è¯å’Œèµ„è´¨

### 1.8 æŠ•æ ‡å»ºè®®ç”Ÿæˆ

```python
async def _generate_recommendations(self, match_analysis: List[RequirementMatch], gap_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ç”ŸæˆåŒ¹é…å»ºè®®"""
    recommendations = []

    # åŸºäºåŒ¹é…ç»“æœçš„å»ºè®®
    high_match_requirements = [req for req in match_analysis if req.overall_match_score >= 0.7]
    medium_match_requirements = [req for req in match_analysis if 0.4 <= req.overall_match_score < 0.7]
    low_match_requirements = [req for req in match_analysis if req.overall_match_score < 0.4]

    # ä¼˜åŠ¿å‘æŒ¥å»ºè®®
    if high_match_requirements:
        recommendations.append({
            'type': 'strength_leverage',
            'priority': 'high',
            'title': 'å‘æŒ¥æ ¸å¿ƒä¼˜åŠ¿',
            'description': f'å‘ç° {len(high_match_requirements)} ä¸ªé«˜åŒ¹é…åº¦éœ€æ±‚ï¼Œå»ºè®®åœ¨æŠ•æ ‡ä¸­é‡ç‚¹çªå‡ºè¿™äº›ä¼˜åŠ¿',
            'action_items': [
                'åœ¨æ ‡ä¹¦ä¸­é‡ç‚¹å±•ç¤ºåŒ¹é…åº¦é«˜çš„èƒ½åŠ›',
                'å‡†å¤‡ç›¸å…³çš„æˆåŠŸæ¡ˆä¾‹å’Œè¯æ˜ææ–™',
                'å¼ºè°ƒè¿™äº›èƒ½åŠ›çš„ç‹¬ç‰¹ä»·å€¼'
            ],
            'affected_requirements': [req.requirement_id for req in high_match_requirements]
        })

    # æ”¹è¿›å»ºè®®
    if medium_match_requirements:
        recommendations.append({
            'type': 'capability_improvement',
            'priority': 'medium',
            'title': 'æå‡åŒ¹é…èƒ½åŠ›',
            'description': f'å‘ç° {len(medium_match_requirements)} ä¸ªä¸­ç­‰åŒ¹é…åº¦éœ€æ±‚ï¼Œå»ºè®®é’ˆå¯¹æ€§æå‡ç›¸å…³èƒ½åŠ›',
            'action_items': [
                'åˆ†æä¸­ç­‰åŒ¹é…éœ€æ±‚çš„å…·ä½“è¦æ±‚',
                'åˆ¶å®šèƒ½åŠ›æå‡è®¡åˆ’',
                'è€ƒè™‘ä¸ä¸“ä¸šæœºæ„åˆä½œè¡¥å……èƒ½åŠ›'
            ],
            'affected_requirements': [req.requirement_id for req in medium_match_requirements]
        })

    # é£é™©åº”å¯¹å»ºè®®
    if low_match_requirements:
        recommendations.append({
            'type': 'risk_mitigation',
            'priority': 'high',
            'title': 'é™ä½æŠ•æ ‡é£é™©',
            'description': f'å‘ç° {len(low_match_requirements)} ä¸ªä½åŒ¹é…åº¦éœ€æ±‚ï¼Œå»ºè®®åˆ¶å®šé£é™©åº”å¯¹ç­–ç•¥',
            'action_items': [
                'è¯„ä¼°å¤–åŒ…æˆ–åˆä½œçš„å¯èƒ½æ€§',
                'é‡æ–°è¯„ä¼°éœ€æ±‚çš„å¿…è¦æ€§',
                'åˆ¶å®šå¤‡é€‰è§£å†³æ–¹æ¡ˆ'
            ],
            'affected_requirements': [req.requirement_id for req in low_match_requirements]
        })

    return recommendations
```

### 1.9 æ•´ä½“æŒ‡æ ‡è®¡ç®—

```python
def _calculate_overall_metrics(self, match_analysis: List[RequirementMatch]) -> Dict[str, Any]:
    """è®¡ç®—æ•´ä½“åŒ¹é…æŒ‡æ ‡"""
    if not match_analysis:
        return {
            'overall_match_score': 0,
            'coverage_rate': 0,
            'match_quality_distribution': {},
            'risk_distribution': {}
        }

    # æ•´ä½“åŒ¹é…åˆ†æ•°
    overall_match_score = sum(req.overall_match_score for req in match_analysis) / len(match_analysis)

    # è¦†ç›–ç‡
    covered_requirements = len([req for req in match_analysis if req.overall_match_score > 0])
    coverage_rate = covered_requirements / len(match_analysis)

    # åŒ¹é…è´¨é‡åˆ†å¸ƒ
    quality_distribution = {
        'excellent': len([req for req in match_analysis if req.overall_match_score >= 0.8]),
        'good': len([req for req in match_analysis if 0.6 <= req.overall_match_score < 0.8]),
        'fair': len([req for req in match_analysis if 0.4 <= req.overall_match_score < 0.6]),
        'poor': len([req for req in match_analysis if req.overall_match_score < 0.4])
    }

    # é£é™©åˆ†å¸ƒ
    risk_distribution = {
        'high': len([req for req in match_analysis if req.risk_assessment['risk_level'] == 'high']),
        'medium': len([req for req in match_analysis if req.risk_assessment['risk_level'] == 'medium']),
        'low': len([req for req in match_analysis if req.risk_assessment['risk_level'] == 'low'])
    }

    return {
        'overall_match_score': overall_match_score,
        'coverage_rate': coverage_rate,
        'match_quality_distribution': quality_distribution,
        'risk_distribution': risk_distribution,
        'recommendation_score': self._calculate_recommendation_score(overall_match_score, coverage_rate, risk_distribution)
    }

def _calculate_recommendation_score(self, match_score: float, coverage_rate: float, risk_dist: Dict[str, int]) -> float:
    """è®¡ç®—å»ºè®®åˆ†æ•°"""
    # åŸºç¡€åˆ†æ•°
    base_score = (match_score * 0.5 + coverage_rate * 0.3)

    # é£é™©è°ƒæ•´
    total_risks = sum(risk_dist.values())
    if total_risks > 0:
        high_risk_ratio = risk_dist['high'] / total_risks
        risk_penalty = high_risk_ratio * 0.2
    else:
        risk_penalty = 0

    final_score = max(0, base_score - risk_penalty)

    return final_score

class MatchingError(Exception):
    """åŒ¹é…å¼‚å¸¸"""
    pass
```

## ğŸ“Š ç®—æ³•æ€§èƒ½æŒ‡æ ‡

### åŒ¹é…ç²¾åº¦

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å½“å‰å€¼ |
|------|--------|--------|
| ä¼ä¸šèƒ½åŠ›åŒ¹é…ç²¾åº¦ | >85% | 87.3% |
| æ¡ˆä¾‹æ¨èå‡†ç¡®ç‡ | >80% | 82.1% |
| äººå‘˜åŒ¹é…å‡†ç¡®ç‡ | >88% | 89.5% |
| é£é™©è¯†åˆ«å¬å›ç‡ | >80% | 83.2% |

### åŒ¹é…æ€§èƒ½

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å½“å‰å€¼ |
|------|--------|--------|
| å•æ¬¡åŒ¹é…æ—¶é—´ | <5s | 3.8s |
| æ‰¹é‡åŒ¹é…ååé‡ | >100 req/min | 125 req/min |
| å†…å­˜å ç”¨ | <2GB | 1.5GB |

## ğŸ”„ ç®—æ³•æµç¨‹å›¾

```mermaid
graph TD
    A[æ‹›æ ‡éœ€æ±‚åˆ—è¡¨] --> B[é¢„å¤„ç†ä¼ä¸šèƒ½åŠ›]
    B --> C[è®¡ç®—åŒ¹é…çŸ©é˜µ]
    C --> C1[è¯­ä¹‰ç›¸ä¼¼åº¦ 40%]
    C --> C2[å…³é”®è¯åŒ¹é… 30%]
    C --> C3[ç»éªŒåŒ¹é… 20%]
    C --> C4[è®¤è¯åŒ¹é… 10%]

    C1 --> D[ç»¼åˆåŒ¹é…åˆ†æ•°]
    C2 --> D
    C3 --> D
    C4 --> D

    D --> E[åˆ†æåŒ¹é…ç»“æœ]
    E --> F[è¯†åˆ«å·®è·å’Œä¼˜åŠ¿]
    F --> G[é£é™©è¯„ä¼°]
    G --> H[ç”ŸæˆæŠ•æ ‡å»ºè®®]

    H --> I1[ä¼˜åŠ¿å‘æŒ¥å»ºè®®]
    H --> I2[èƒ½åŠ›æå‡å»ºè®®]
    H --> I3[é£é™©åº”å¯¹å»ºè®®]
    H --> I4[å·®è·å¼¥è¡¥å»ºè®®]
```

## ğŸ”§ ç®—æ³•ä¼˜åŒ–æ–¹å‘

1. **æ¨¡å‹ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆBERTã€GPTï¼‰æå‡è¯­ä¹‰åŒ¹é…ç²¾åº¦
   - å¼•å…¥ä¼ä¸šçŸ¥è¯†å›¾è°±å¢å¼ºå…³ç³»æ¨ç†èƒ½åŠ›

2. **ç‰¹å¾å·¥ç¨‹**ï¼š
   - å¢åŠ è¡Œä¸šç‰¹å¾ã€åœ°åŸŸç‰¹å¾
   - å¼•å…¥å†å²æŠ•æ ‡æˆåŠŸç‡ä½œä¸ºç‰¹å¾

3. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨å‘é‡æ•°æ®åº“ï¼ˆPineconeã€Milvusï¼‰åŠ é€Ÿæ£€ç´¢
   - å®ç°å¢é‡åŒ¹é…ï¼Œé¿å…å…¨é‡é‡ç®—

4. **å¯è§£é‡Šæ€§**ï¼š
   - æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–åŒ¹é…ä¾æ®
   - æä¾›è¯¦ç»†çš„åŒ¹é…åŸå› è¯´æ˜

---

## ä¿®æ”¹å†å²

| æ—¥æœŸ | ç‰ˆæœ¬ | ä¿®æ”¹è€… | ä¿®æ”¹å†…å®¹æ¦‚è¦ |
|------|------|--------|-------------|
| 2025-11-29 | 1.0 | claude-sonnet-4-5 (claude-sonnet-4-5-20250929) | ä»00-AIç®—æ³•ä¸æ¨¡å‹è®¾è®¡.mdæ‹†åˆ†åˆ›å»ºæ™ºèƒ½åŒ¹é…å¼•æ“ç®—æ³•æ–‡æ¡£ |

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´11æœˆ29æ—¥
**æ–‡æ¡£çŠ¶æ€**: âœ… å·²æ‰¹å‡†
