# AIæ ‡ä¹¦æ™ºèƒ½åˆ›ä½œå¹³å° - AIèƒ½åŠ›å±‚è®¾è®¡

## ğŸ“‹ AIèƒ½åŠ›æ¦‚è§ˆ

### AIèƒ½åŠ›çŸ©é˜µ

```yaml
æ ¸å¿ƒAIèƒ½åŠ›:
  æ–‡æ¡£ç†è§£:
    - æ‹›æ ‡æ–‡ä»¶è§£æ
    - éœ€æ±‚æå–
    - å…³é”®ä¿¡æ¯è¯†åˆ«
    - é£é™©ç‚¹åˆ†æ

  æ™ºèƒ½åŒ¹é…:
    - éœ€æ±‚-èƒ½åŠ›åŒ¹é…
    - æ¡ˆä¾‹æ¨è
    - äººå‘˜åŒ¹é…
    - è§£å†³æ–¹æ¡ˆæ¨è

  å†…å®¹ç”Ÿæˆ:
    - æŠ€æœ¯æ–¹æ¡ˆç”Ÿæˆ
    - å•†åŠ¡æ–¹æ¡ˆç”Ÿæˆ
    - é¡¹ç›®è®¡åˆ’ç”Ÿæˆ
    - å“åº”æ–‡æ¡£ç”Ÿæˆ

  è´¨é‡ä¿è¯:
    - å†…å®¹æŸ¥é‡
    - å®Œæ•´æ€§æ£€æŸ¥
    - åˆè§„æ€§å®¡æ ¸
    - è´¨é‡è¯„åˆ†

  çŸ¥è¯†ç®¡ç†:
    - çŸ¥è¯†æŠ½å–
    - çŸ¥è¯†å›¾è°±æ„å»º
    - è¯­ä¹‰æ£€ç´¢
    - çŸ¥è¯†æ¨ç†
```

### AIæŠ€æœ¯æ ˆ

```yaml
å¤§è¯­è¨€æ¨¡å‹:
  ä¸»æ¨¡å‹:
    - GPT-4 Turbo: å¤æ‚æ¨ç†ã€åˆ›ä½œ
    - GPT-3.5 Turbo: æ—¥å¸¸å¯¹è¯ã€ç®€å•ç”Ÿæˆ
    - Claude 3 Opus: é•¿æ–‡æœ¬ç†è§£ã€æ·±åº¦åˆ†æ

  å›½äº§æ¨¡å‹:
    - æ™ºè°±ChatGLM: ä¸­æ–‡ç†è§£ã€å¯¹è¯
    - ç™¾åº¦æ–‡å¿ƒä¸€è¨€: ä¸­æ–‡ç”Ÿæˆ
    - é˜¿é‡Œé€šä¹‰åƒé—®: ä¼ä¸šåœºæ™¯

  å¼€æºæ¨¡å‹:
    - Llama 2: æœ¬åœ°éƒ¨ç½²ã€éšç§åœºæ™¯
    - Mistral: è½»é‡çº§æ¨ç†
    - DeepSeek: ä»£ç ç†è§£

å‘é‡åµŒå…¥:
  - OpenAI text-embedding-ada-002
  - HuggingFace Sentence-BERT
  - æ™ºè°±GLM Embedding

å‘é‡æ•°æ®åº“:
  - Pinecone (äº‘æœåŠ¡)
  - Chroma (æœ¬åœ°éƒ¨ç½²)
  - Weaviate (æ··åˆæ¨¡å¼)

å›¾æ•°æ®åº“:
  - Neo4j: çŸ¥è¯†å›¾è°±å­˜å‚¨
  - ArangoDB: å¤šæ¨¡å‹æ•°æ®åº“

AIæ¡†æ¶:
  - LangChain: AIåº”ç”¨å¼€å‘
  - LangGraph: å·¥ä½œæµç¼–æ’
  - LlamaIndex: æ•°æ®ç´¢å¼•
```

## ğŸ¤– LLMæœåŠ¡æ¶æ„

### 1. LLMå®¢æˆ·ç«¯è®¾è®¡

```python
# app/services/ai/llm_client.py
from typing import Optional, List, Dict, Any
from enum import Enum
import openai
import anthropic
from langchain.chat_models import ChatOpenAI, ChatAnthropic
from langchain.schema import SystemMessage, HumanMessage, AIMessage

class ModelProvider(str, Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    ZHIPU = "zhipu"
    BAIDU = "baidu"

class LLMClient:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯"""

    def __init__(self):
        self.providers = {
            ModelProvider.OPENAI: self._init_openai(),
            ModelProvider.ANTHROPIC: self._init_anthropic(),
            ModelProvider.ZHIPU: self._init_zhipu(),
        }

    def _init_openai(self) -> ChatOpenAI:
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        return ChatOpenAI(
            model_name="gpt-4-turbo-preview",
            temperature=0.7,
            openai_api_key=settings.OPENAI_API_KEY,
        )

    def _init_anthropic(self) -> ChatAnthropic:
        """åˆå§‹åŒ–Anthropicå®¢æˆ·ç«¯"""
        return ChatAnthropic(
            model="claude-3-opus-20240229",
            anthropic_api_key=settings.ANTHROPIC_API_KEY,
        )

    async def chat(
        self,
        messages: List[Dict[str, str]],
        provider: ModelProvider = ModelProvider.OPENAI,
        model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„èŠå¤©æ¥å£

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            provider: æ¨¡å‹æä¾›å•†
            model: å…·ä½“æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°

        Returns:
            ç”Ÿæˆç»“æœ
        """
        llm = self.providers[provider]

        if model:
            llm.model_name = model
        llm.temperature = temperature
        llm.max_tokens = max_tokens

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        formatted_messages = self._format_messages(messages)

        # è°ƒç”¨LLM
        response = await llm.agenerate([formatted_messages])

        return {
            "content": response.generations[0][0].text,
            "model": llm.model_name,
            "tokens_used": {
                "prompt": response.llm_output.get("token_usage", {}).get("prompt_tokens", 0),
                "completion": response.llm_output.get("token_usage", {}).get("completion_tokens", 0),
                "total": response.llm_output.get("token_usage", {}).get("total_tokens", 0),
            },
            "finish_reason": response.generations[0][0].generation_info.get("finish_reason"),
        }

    def _format_messages(self, messages: List[Dict[str, str]]) -> List:
        """æ ¼å¼åŒ–æ¶ˆæ¯"""
        formatted = []
        for msg in messages:
            role = msg["role"]
            content = msg["content"]

            if role == "system":
                formatted.append(SystemMessage(content=content))
            elif role == "user":
                formatted.append(HumanMessage(content=content))
            elif role == "assistant":
                formatted.append(AIMessage(content=content))

        return formatted
```

### 2. Promptç®¡ç†

```python
# app/services/ai/prompt_manager.py
from typing import Dict, Any, Optional
from jinja2 import Template
from app.models.ai_prompt import AIPrompt
from app.core.cache import cache

class PromptManager:
    """Promptæ¨¡æ¿ç®¡ç†å™¨"""

    def __init__(self):
        self.cache_ttl = 3600  # 1å°æ—¶ç¼“å­˜

    async def get_prompt(
        self,
        code: str,
        variables: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        è·å–å¹¶æ¸²æŸ“Prompt

        Args:
            code: Promptä»£ç 
            variables: å˜é‡å­—å…¸

        Returns:
            æ¸²æŸ“åçš„Prompt
        """
        # ä»ç¼“å­˜è·å–
        cache_key = f"prompt:{code}"
        prompt_template = await cache.get(cache_key)

        if not prompt_template:
            # ä»æ•°æ®åº“è·å–
            prompt = await AIPrompt.get_by_code(code)
            if not prompt:
                raise ValueError(f"Prompt not found: {code}")

            prompt_template = prompt.prompt_template
            # ç¼“å­˜
            await cache.set(cache_key, prompt_template, self.cache_ttl)

        # æ¸²æŸ“æ¨¡æ¿
        if variables:
            template = Template(prompt_template)
            return template.render(**variables)

        return prompt_template

    async def create_prompt(
        self,
        name: str,
        code: str,
        category: str,
        prompt_template: str,
        system_prompt: Optional[str] = None,
        variables: Optional[List[str]] = None,
        model_params: Optional[Dict[str, Any]] = None,
    ) -> AIPrompt:
        """åˆ›å»ºPromptæ¨¡æ¿"""
        prompt = await AIPrompt.create(
            name=name,
            code=code,
            category=category,
            prompt_template=prompt_template,
            system_prompt=system_prompt,
            variables=variables or [],
            model_params=model_params or {},
        )

        # æ¸…é™¤ç¼“å­˜
        await cache.delete(f"prompt:{code}")

        return prompt


# Promptç¤ºä¾‹
REQUIREMENT_ANALYSIS_PROMPT = """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ‹›æŠ•æ ‡ä¸“å®¶ï¼Œæ“…é•¿åˆ†ææ‹›æ ‡æ–‡ä»¶å¹¶æå–å…³é”®éœ€æ±‚ã€‚

è¯·åˆ†æä»¥ä¸‹æ‹›æ ‡æ–‡ä»¶å†…å®¹ï¼Œæå–å…³é”®éœ€æ±‚ä¿¡æ¯ï¼š

æ‹›æ ‡æ–‡ä»¶å†…å®¹ï¼š
{bidding_document_content}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š

1. é¡¹ç›®åŸºæœ¬ä¿¡æ¯
- é¡¹ç›®åç§°
- æ‹›æ ‡å•ä½
- é¡¹ç›®é¢„ç®—
- æäº¤æˆªæ­¢æ—¶é—´

2. æŠ€æœ¯éœ€æ±‚
- æ ¸å¿ƒåŠŸèƒ½éœ€æ±‚
- æŠ€æœ¯æŒ‡æ ‡è¦æ±‚
- æ€§èƒ½è¦æ±‚
- å®‰å…¨è¦æ±‚

3. å•†åŠ¡éœ€æ±‚
- èµ„è´¨è¦æ±‚
- ä¸šç»©è¦æ±‚
- äººå‘˜è¦æ±‚
- å…¶ä»–å•†åŠ¡æ¡ä»¶

4. è¯„åˆ†æ ‡å‡†
- æŠ€æœ¯è¯„åˆ†ï¼ˆæƒé‡å’Œæ ‡å‡†ï¼‰
- å•†åŠ¡è¯„åˆ†ï¼ˆæƒé‡å’Œæ ‡å‡†ï¼‰

5. é£é™©ç‚¹
- è¯†åˆ«æ½œåœ¨çš„é£é™©ç‚¹
- ç»™å‡ºé£é™©ç­‰çº§ï¼ˆé«˜/ä¸­/ä½ï¼‰

6. å»ºè®®
- æŠ•æ ‡å»ºè®®
- éœ€è¦é‡ç‚¹å…³æ³¨çš„æ–¹é¢
"""

CONTENT_GENERATION_PROMPT = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ ‡ä¹¦æ’°å†™ä¸“å®¶ï¼Œæ“…é•¿æ’°å†™æŠ€æœ¯æ–¹æ¡ˆå’Œå•†åŠ¡æ–¹æ¡ˆã€‚

é¡¹ç›®èƒŒæ™¯ï¼š
{project_background}

éœ€æ±‚å†…å®¹ï¼š
{requirements}

ä¼ä¸šèƒ½åŠ›ï¼š
{capabilities}

å†å²æ¡ˆä¾‹ï¼š
{cases}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œæ’°å†™{section_type}éƒ¨åˆ†çš„å†…å®¹ã€‚

è¦æ±‚ï¼š
1. å†…å®¹ä¸“ä¸šã€å‡†ç¡®ã€æœ‰è¯´æœåŠ›
2. çªå‡ºä¼ä¸šä¼˜åŠ¿å’Œç«äº‰åŠ›
3. ç´§å¯†è´´åˆæ‹›æ ‡éœ€æ±‚
4. ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥å¯†
5. å­—æ•°çº¦{word_count}å­—

è¯·ç›´æ¥è¾“å‡ºå†…å®¹ï¼Œä¸è¦åŒ…å«é¢å¤–çš„è§£é‡Šã€‚
"""
```

### 3. AIä»»åŠ¡é˜Ÿåˆ—

```python
# app/services/ai/task_queue.py
from celery import Task
from typing import Dict, Any
from app.tasks.celery_app import celery_app
from app.models.ai_task import AITask, TaskStatus
from app.services.ai.llm_client import LLMClient
from app.core.logging import logger

class AITaskQueue:
    """AIä»»åŠ¡é˜Ÿåˆ—ç®¡ç†"""

    @staticmethod
    @celery_app.task(bind=True, max_retries=3)
    async def process_ai_task(self: Task, task_id: str) -> Dict[str, Any]:
        """
        å¤„ç†AIä»»åŠ¡

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            å¤„ç†ç»“æœ
        """
        # è·å–ä»»åŠ¡
        task = await AITask.get(task_id)
        if not task:
            raise ValueError(f"Task not found: {task_id}")

        try:
            # æ›´æ–°çŠ¶æ€ä¸ºè¿è¡Œä¸­
            await task.update(
                status=TaskStatus.RUNNING,
                started_at=datetime.utcnow()
            )

            # è·å–LLMå®¢æˆ·ç«¯
            llm_client = LLMClient()

            # æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œ
            if task.task_type == "parse":
                result = await self._process_parse_task(task, llm_client)
            elif task.task_type == "analyze":
                result = await self._process_analyze_task(task, llm_client)
            elif task.task_type == "match":
                result = await self._process_match_task(task, llm_client)
            elif task.task_type == "generate":
                result = await self._process_generate_task(task, llm_client)
            elif task.task_type == "review":
                result = await self._process_review_task(task, llm_client)
            else:
                raise ValueError(f"Unknown task type: {task.task_type}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºæˆåŠŸ
            await task.update(
                status=TaskStatus.SUCCESS,
                output_data=result,
                completed_at=datetime.utcnow(),
                duration_seconds=(datetime.utcnow() - task.started_at).total_seconds()
            )

            return result

        except Exception as e:
            logger.error(f"AI task failed: {task_id}, error: {str(e)}")

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            await task.update(
                status=TaskStatus.FAILED,
                error_message=str(e),
                completed_at=datetime.utcnow(),
                retry_count=task.retry_count + 1
            )

            # é‡è¯•
            if task.retry_count < task.max_retries:
                raise self.retry(exc=e, countdown=60 * (task.retry_count + 1))

            raise

    @staticmethod
    async def _process_generate_task(
        task: AITask,
        llm_client: LLMClient
    ) -> Dict[str, Any]:
        """å¤„ç†å†…å®¹ç”Ÿæˆä»»åŠ¡"""
        input_data = task.input_data

        # è·å–Prompt
        prompt_manager = PromptManager()
        prompt = await prompt_manager.get_prompt(
            code="content_generation",
            variables=input_data.get("context", {})
        )

        # è°ƒç”¨LLM
        response = await llm_client.chat(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ ‡ä¹¦æ’°å†™ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            model=input_data.get("model", "gpt-4-turbo-preview"),
            temperature=input_data.get("temperature", 0.7),
            max_tokens=input_data.get("max_tokens", 2000)
        )

        return {
            "content": response["content"],
            "tokens_used": response["tokens_used"],
            "model": response["model"]
        }
```

## ğŸ” å‘é‡æ£€ç´¢æœåŠ¡

### 1. å‘é‡åµŒå…¥æœåŠ¡

```python
# app/services/ai/embedding_service.py
from typing import List, Dict, Any
import openai
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings

class EmbeddingService:
    """å‘é‡åµŒå…¥æœåŠ¡"""

    def __init__(self):
        self.openai_embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            openai_api_key=settings.OPENAI_API_KEY
        )

        self.huggingface_embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )

    async def embed_text(
        self,
        text: str,
        provider: str = "openai"
    ) -> List[float]:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡

        Args:
            text: æ–‡æœ¬å†…å®¹
            provider: åµŒå…¥æ¨¡å‹æä¾›å•†

        Returns:
            å‘é‡æ•°ç»„
        """
        if provider == "openai":
            return await self.openai_embeddings.aembed_query(text)
        elif provider == "huggingface":
            return self.huggingface_embeddings.embed_query(text)
        else:
            raise ValueError(f"Unknown provider: {provider}")

    async def embed_documents(
        self,
        documents: List[str],
        provider: str = "openai"
    ) -> List[List[float]]:
        """
        æ‰¹é‡åµŒå…¥æ–‡æ¡£

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            provider: åµŒå…¥æ¨¡å‹æä¾›å•†

        Returns:
            å‘é‡æ•°ç»„åˆ—è¡¨
        """
        if provider == "openai":
            return await self.openai_embeddings.aembed_documents(documents)
        elif provider == "huggingface":
            return self.huggingface_embeddings.embed_documents(documents)
        else:
            raise ValueError(f"Unknown provider: {provider}")
```

### 2. å‘é‡å­˜å‚¨æœåŠ¡

```python
# app/services/ai/vector_store.py
from typing import List, Dict, Any, Optional
import pinecone
from langchain.vectorstores import Pinecone, Chroma

class VectorStore:
    """å‘é‡å­˜å‚¨æœåŠ¡"""

    def __init__(self):
        # åˆå§‹åŒ–Pinecone
        pinecone.init(
            api_key=settings.PINECONE_API_KEY,
            environment=settings.PINECONE_ENVIRONMENT
        )

        self.index_name = "aibidcomposer"

        # åˆ›å»ºæˆ–è·å–ç´¢å¼•
        if self.index_name not in pinecone.list_indexes():
            pinecone.create_index(
                name=self.index_name,
                dimension=1536,  # OpenAI embeddingç»´åº¦
                metric="cosine"
            )

        self.index = pinecone.Index(self.index_name)

    async def add_documents(
        self,
        documents: List[Dict[str, Any]],
        embeddings: List[List[float]],
        namespace: str = "default"
    ) -> None:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            embeddings: å‘é‡åˆ—è¡¨
            namespace: å‘½åç©ºé—´
        """
        vectors = []
        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):
            vectors.append({
                "id": doc.get("id", f"doc_{i}"),
                "values": embedding,
                "metadata": {
                    "text": doc.get("text", ""),
                    "source": doc.get("source", ""),
                    "created_at": doc.get("created_at", ""),
                    **doc.get("metadata", {})
                }
            })

        # æ‰¹é‡ä¸Šä¼ 
        self.index.upsert(vectors=vectors, namespace=namespace)

    async def search(
        self,
        query_embedding: List[float],
        top_k: int = 10,
        namespace: str = "default",
        filter_dict: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        å‘é‡ç›¸ä¼¼åº¦æœç´¢

        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›ç»“æœæ•°é‡
            namespace: å‘½åç©ºé—´
            filter_dict: è¿‡æ»¤æ¡ä»¶

        Returns:
            æœç´¢ç»“æœ
        """
        results = self.index.query(
            vector=query_embedding,
            top_k=top_k,
            namespace=namespace,
            filter=filter_dict,
            include_metadata=True
        )

        return [
            {
                "id": match.id,
                "score": match.score,
                "metadata": match.metadata
            }
            for match in results.matches
        ]

    async def semantic_search(
        self,
        query_text: str,
        top_k: int = 10,
        namespace: str = "default",
        filter_dict: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢ï¼ˆæ–‡æœ¬æŸ¥è¯¢ï¼‰

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            namespace: å‘½åç©ºé—´
            filter_dict: è¿‡æ»¤æ¡ä»¶

        Returns:
            æœç´¢ç»“æœ
        """
        # è·å–åµŒå…¥æœåŠ¡
        embedding_service = EmbeddingService()

        # å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        query_embedding = await embedding_service.embed_text(query_text)

        # æ‰§è¡Œæœç´¢
        return await self.search(
            query_embedding=query_embedding,
            top_k=top_k,
            namespace=namespace,
            filter_dict=filter_dict
        )
```

## ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±æœåŠ¡

### 1. çŸ¥è¯†å›¾è°±ç®¡ç†

```python
# app/services/ai/knowledge_graph.py
from typing import List, Dict, Any, Optional
from neo4j import AsyncGraphDatabase
from app.core.config import settings

class KnowledgeGraph:
    """çŸ¥è¯†å›¾è°±æœåŠ¡"""

    def __init__(self):
        self.driver = AsyncGraphDatabase.driver(
            settings.NEO4J_URI,
            auth=(settings.NEO4J_USER, settings.NEO4J_PASSWORD)
        )

    async def close(self):
        """å…³é—­è¿æ¥"""
        await self.driver.close()

    async def create_entity(
        self,
        entity_type: str,
        properties: Dict[str, Any]
    ) -> str:
        """
        åˆ›å»ºå®ä½“èŠ‚ç‚¹

        Args:
            entity_type: å®ä½“ç±»å‹ï¼ˆRequirement, Capability, Project, Person, Caseï¼‰
            properties: å®ä½“å±æ€§

        Returns:
            å®ä½“ID
        """
        async with self.driver.session() as session:
            result = await session.run(
                f"""
                CREATE (n:{entity_type} $properties)
                RETURN id(n) as entity_id
                """,
                properties=properties
            )
            record = await result.single()
            return record["entity_id"]

    async def create_relationship(
        self,
        from_id: str,
        to_id: str,
        relationship_type: str,
        properties: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        åˆ›å»ºå…³ç³»

        Args:
            from_id: èµ·å§‹èŠ‚ç‚¹ID
            to_id: ç›®æ ‡èŠ‚ç‚¹ID
            relationship_type: å…³ç³»ç±»å‹
            properties: å…³ç³»å±æ€§
        """
        async with self.driver.session() as session:
            await session.run(
                f"""
                MATCH (a), (b)
                WHERE id(a) = $from_id AND id(b) = $to_id
                CREATE (a)-[r:{relationship_type} $properties]->(b)
                """,
                from_id=from_id,
                to_id=to_id,
                properties=properties or {}
            )

    async def find_matching_capabilities(
        self,
        requirement_id: str,
        min_score: float = 0.7
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾åŒ¹é…çš„èƒ½åŠ›

        Args:
            requirement_id: éœ€æ±‚ID
            min_score: æœ€å°åŒ¹é…åˆ†æ•°

        Returns:
            åŒ¹é…çš„èƒ½åŠ›åˆ—è¡¨
        """
        async with self.driver.session() as session:
            result = await session.run(
                """
                MATCH (req:Requirement)-[r:MATCHES]->(cap:Capability)
                WHERE id(req) = $requirement_id AND r.score >= $min_score
                RETURN cap, r.score as score
                ORDER BY r.score DESC
                """,
                requirement_id=requirement_id,
                min_score=min_score
            )

            capabilities = []
            async for record in result:
                cap_node = record["cap"]
                capabilities.append({
                    "id": cap_node.id,
                    "name": cap_node.get("name"),
                    "description": cap_node.get("description"),
                    "type": cap_node.get("type"),
                    "score": record["score"]
                })

            return capabilities

    async def find_similar_cases(
        self,
        requirement_text: str,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼æ¡ˆä¾‹

        Args:
            requirement_text: éœ€æ±‚æ–‡æœ¬
            top_k: è¿”å›æ•°é‡

        Returns:
            ç›¸ä¼¼æ¡ˆä¾‹åˆ—è¡¨
        """
        # 1. ä½¿ç”¨å‘é‡æ£€ç´¢æ‰¾åˆ°ç›¸ä¼¼æ¡ˆä¾‹
        vector_store = VectorStore()
        similar_cases = await vector_store.semantic_search(
            query_text=requirement_text,
            top_k=top_k,
            namespace="cases"
        )

        # 2. ä»çŸ¥è¯†å›¾è°±è·å–æ¡ˆä¾‹è¯¦ç»†ä¿¡æ¯å’Œå…³è”å®ä½“
        case_ids = [case["id"] for case in similar_cases]

        async with self.driver.session() as session:
            result = await session.run(
                """
                MATCH (c:Case)
                WHERE c.case_id IN $case_ids
                OPTIONAL MATCH (c)-[:USES_TECHNOLOGY]->(tech:Technology)
                OPTIONAL MATCH (c)-[:BELONGS_TO_INDUSTRY]->(ind:Industry)
                OPTIONAL MATCH (c)-[:INVOLVES_PERSON]->(per:Person)
                RETURN c,
                       collect(DISTINCT tech.name) as technologies,
                       collect(DISTINCT ind.name) as industries,
                       collect(DISTINCT per.name) as persons
                """,
                case_ids=case_ids
            )

            cases = []
            async for record in result:
                case_node = record["c"]
                cases.append({
                    "id": case_node.get("case_id"),
                    "name": case_node.get("name"),
                    "description": case_node.get("description"),
                    "client": case_node.get("client"),
                    "technologies": record["technologies"],
                    "industries": record["industries"],
                    "persons": record["persons"]
                })

            return cases
```

### 2. æ™ºèƒ½åŒ¹é…å¼•æ“

```python
# app/services/ai/matching_engine.py
from typing import List, Dict, Any
from app.services.ai.embedding_service import EmbeddingService
from app.services.ai/vector_store import VectorStore
from app.services.ai.knowledge_graph import KnowledgeGraph

class MatchingEngine:
    """æ™ºèƒ½åŒ¹é…å¼•æ“"""

    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.vector_store = VectorStore()
        self.knowledge_graph = KnowledgeGraph()

    async def match_requirements_to_capabilities(
        self,
        requirements: List[Dict[str, Any]],
        organization_id: str
    ) -> Dict[str, Any]:
        """
        éœ€æ±‚ä¸èƒ½åŠ›åŒ¹é…

        Args:
            requirements: éœ€æ±‚åˆ—è¡¨
            organization_id: ç»„ç»‡ID

        Returns:
            åŒ¹é…ç»“æœ
        """
        matches = []

        for req in requirements:
            # 1. å‘é‡ç›¸ä¼¼åº¦æœç´¢
            vector_results = await self.vector_store.semantic_search(
                query_text=req["description"],
                top_k=10,
                namespace=f"capabilities_{organization_id}"
            )

            # 2. çŸ¥è¯†å›¾è°±å…³ç³»æ¨ç†
            graph_results = await self.knowledge_graph.find_matching_capabilities(
                requirement_id=req["id"],
                min_score=0.6
            )

            # 3. ç»¼åˆè¯„åˆ†
            combined_results = self._combine_match_results(
                vector_results=vector_results,
                graph_results=graph_results
            )

            matches.append({
                "requirement": req,
                "matched_capabilities": combined_results[:5],
                "match_score": combined_results[0]["score"] if combined_results else 0
            })

        return {
            "matches": matches,
            "overall_match_rate": sum(m["match_score"] for m in matches) / len(matches) if matches else 0
        }

    def _combine_match_results(
        self,
        vector_results: List[Dict[str, Any]],
        graph_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ç»¼åˆå‘é‡æœç´¢å’Œå›¾è°±æ¨ç†çš„ç»“æœ

        Args:
            vector_results: å‘é‡æœç´¢ç»“æœ
            graph_results: å›¾è°±æ¨ç†ç»“æœ

        Returns:
            ç»¼åˆç»“æœ
        """
        # åˆ›å»ºIDåˆ°ç»“æœçš„æ˜ å°„
        vector_map = {r["id"]: r for r in vector_results}
        graph_map = {r["id"]: r for r in graph_results}

        # åˆå¹¶ç»“æœ
        combined = {}
        all_ids = set(vector_map.keys()) | set(graph_map.keys())

        for cap_id in all_ids:
            vector_score = vector_map.get(cap_id, {}).get("score", 0)
            graph_score = graph_map.get(cap_id, {}).get("score", 0)

            # åŠ æƒå¹³å‡ï¼ˆå‘é‡60%ï¼Œå›¾è°±40%ï¼‰
            final_score = vector_score * 0.6 + graph_score * 0.4

            combined[cap_id] = {
                "id": cap_id,
                "score": final_score,
                "vector_score": vector_score,
                "graph_score": graph_score,
                "metadata": vector_map.get(cap_id, {}).get("metadata", {})
            }

        # æŒ‰åˆ†æ•°æ’åº
        sorted_results = sorted(
            combined.values(),
            key=lambda x: x["score"],
            reverse=True
        )

        return sorted_results
```

## ğŸ”„ å·¥ä½œæµç¼–æ’

### 1. LangGraphå·¥ä½œæµ

```python
# app/services/ai/workflow_engine.py
from typing import Dict, Any, List
from langgraph.graph import StateGraph, END
from langchain.schema import HumanMessage, AIMessage

class BidDocumentWorkflow:
    """æ ‡ä¹¦åˆ›ä½œå·¥ä½œæµ"""

    def __init__(self):
        self.llm_client = LLMClient()
        self.matching_engine = MatchingEngine()

    def create_workflow(self) -> StateGraph:
        """åˆ›å»ºæ ‡ä¹¦åˆ›ä½œå·¥ä½œæµ"""

        # å®šä¹‰çŠ¶æ€
        workflow = StateGraph()

        # å®šä¹‰èŠ‚ç‚¹
        workflow.add_node("analyze_requirements", self.analyze_requirements)
        workflow.add_node("match_capabilities", self.match_capabilities)
        workflow.add_node("select_cases", self.select_cases)
        workflow.add_node("generate_technical_solution", self.generate_technical_solution)
        workflow.add_node("generate_commercial_solution", self.generate_commercial_solution)
        workflow.add_node("review_quality", self.review_quality)

        # å®šä¹‰è¾¹
        workflow.add_edge("analyze_requirements", "match_capabilities")
        workflow.add_edge("match_capabilities", "select_cases")
        workflow.add_edge("select_cases", "generate_technical_solution")
        workflow.add_edge("generate_technical_solution", "generate_commercial_solution")
        workflow.add_edge("generate_commercial_solution", "review_quality")
        workflow.add_edge("review_quality", END)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("analyze_requirements")

        return workflow.compile()

    async def analyze_requirements(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æéœ€æ±‚"""
        bidding_document = state["bidding_document"]

        # è°ƒç”¨LLMåˆ†æéœ€æ±‚
        prompt_manager = PromptManager()
        prompt = await prompt_manager.get_prompt(
            code="requirement_analysis",
            variables={"bidding_document_content": bidding_document}
        )

        response = await self.llm_client.chat(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯æ‹›æŠ•æ ‡éœ€æ±‚åˆ†æä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )

        state["requirements"] = self._parse_requirements(response["content"])
        return state

    async def match_capabilities(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """åŒ¹é…èƒ½åŠ›"""
        requirements = state["requirements"]
        organization_id = state["organization_id"]

        # è°ƒç”¨åŒ¹é…å¼•æ“
        match_results = await self.matching_engine.match_requirements_to_capabilities(
            requirements=requirements,
            organization_id=organization_id
        )

        state["match_results"] = match_results
        return state

    async def generate_technical_solution(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆ"""
        requirements = state["requirements"]
        capabilities = state["match_results"]
        cases = state["selected_cases"]

        # ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆ
        prompt_manager = PromptManager()
        prompt = await prompt_manager.get_prompt(
            code="technical_solution_generation",
            variables={
                "requirements": requirements,
                "capabilities": capabilities,
                "cases": cases
            }
        )

        response = await self.llm_client.chat(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯æŠ€æœ¯æ–¹æ¡ˆæ’°å†™ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            model="gpt-4-turbo-preview",
            temperature=0.7,
            max_tokens=4000
        )

        state["technical_solution"] = response["content"]
        return state

    def _parse_requirements(self, content: str) -> List[Dict[str, Any]]:
        """è§£æéœ€æ±‚"""
        # å®ç°éœ€æ±‚è§£æé€»è¾‘
        pass
```

### 2. AIåŠ©æ‰‹çŸ©é˜µ

```python
# app/services/ai/assistant_matrix.py
from typing import Dict, Any, List
from enum import Enum

class AssistantType(str, Enum):
    REQUIREMENT_ANALYST = "requirement_analyst"
    SOLUTION_WRITER = "solution_writer"
    COMMERCIAL_ASSISTANT = "commercial_assistant"
    COMPLIANCE_REVIEWER = "compliance_reviewer"
    QUALITY_ASSURER = "quality_assurer"

class AIAssistantMatrix:
    """AIåŠ©æ‰‹çŸ©é˜µ"""

    def __init__(self):
        self.llm_client = LLMClient()
        self.assistants = {
            AssistantType.REQUIREMENT_ANALYST: {
                "model": "gpt-4-turbo-preview",
                "temperature": 0.2,
                "system_prompt": "ä½ æ˜¯éœ€æ±‚åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»æ‹›æ ‡æ–‡ä»¶ä¸­ç²¾å‡†è¯†åˆ«éœ€æ±‚ã€‚"
            },
            AssistantType.SOLUTION_WRITER: {
                "model": "gpt-4-turbo-preview",
                "temperature": 0.7,
                "system_prompt": "ä½ æ˜¯æŠ€æœ¯æ–¹æ¡ˆæ’°å†™ä¸“å®¶ï¼Œæ“…é•¿æ’°å†™ä¸“ä¸šçš„æŠ€æœ¯æ–¹æ¡ˆã€‚"
            },
            AssistantType.COMMERCIAL_ASSISTANT: {
                "model": "gpt-3.5-turbo",
                "temperature": 0.5,
                "system_prompt": "ä½ æ˜¯å•†åŠ¡æ–¹æ¡ˆä¸“å®¶ï¼Œæ“…é•¿å•†åŠ¡æ¡æ¬¾å’ŒæŠ¥ä»·ç­–ç•¥ã€‚"
            },
            AssistantType.COMPLIANCE_REVIEWER: {
                "model": "gpt-4-turbo-preview",
                "temperature": 0.1,
                "system_prompt": "ä½ æ˜¯åˆè§„å®¡æ ¸ä¸“å®¶ï¼Œæ“…é•¿æ£€æŸ¥æ ‡ä¹¦çš„åˆè§„æ€§ã€‚"
            },
            AssistantType.QUALITY_ASSURER: {
                "model": "gpt-4-turbo-preview",
                "temperature": 0.3,
                "system_prompt": "ä½ æ˜¯è´¨é‡ä¿è¯ä¸“å®¶ï¼Œæ“…é•¿æ£€æŸ¥å†…å®¹è´¨é‡å’Œå®Œæ•´æ€§ã€‚"
            }
        }

    async def invoke_assistant(
        self,
        assistant_type: AssistantType,
        prompt: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨åŠ©æ‰‹

        Args:
            assistant_type: åŠ©æ‰‹ç±»å‹
            prompt: æç¤ºè¯
            context: ä¸Šä¸‹æ–‡

        Returns:
            åŠ©æ‰‹å“åº”
        """
        config = self.assistants[assistant_type]

        messages = [
            {"role": "system", "content": config["system_prompt"]},
        ]

        if context:
            messages.append({
                "role": "user",
                "content": f"ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n{json.dumps(context, ensure_ascii=False)}"
            })

        messages.append({"role": "user", "content": prompt})

        response = await self.llm_client.chat(
            messages=messages,
            model=config["model"],
            temperature=config["temperature"]
        )

        return response
```

## ğŸ“Š æˆæœ¬å’Œæ€§èƒ½ä¼˜åŒ–

### 1. Tokenä¼˜åŒ–ç­–ç•¥

```python
# app/services/ai/token_optimizer.py
from typing import List, Dict, Any
import tiktoken

class TokenOptimizer:
    """Tokenä¼˜åŒ–å™¨"""

    def __init__(self):
        self.encoding = tiktoken.get_encoding("cl100k_base")

    def count_tokens(self, text: str) -> int:
        """è®¡ç®—Tokenæ•°é‡"""
        return len(self.encoding.encode(text))

    def truncate_to_token_limit(
        self,
        text: str,
        max_tokens: int,
        preserve_end: bool = False
    ) -> str:
        """
        æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®šTokenæ•°

        Args:
            text: æ–‡æœ¬
            max_tokens: æœ€å¤§Tokenæ•°
            preserve_end: æ˜¯å¦ä¿ç•™æœ«å°¾

        Returns:
            æˆªæ–­åçš„æ–‡æœ¬
        """
        tokens = self.encoding.encode(text)

        if len(tokens) <= max_tokens:
            return text

        if preserve_end:
            tokens = tokens[-max_tokens:]
        else:
            tokens = tokens[:max_tokens]

        return self.encoding.decode(tokens)

    def optimize_messages(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 4000
    ) -> List[Dict[str, str]]:
        """
        ä¼˜åŒ–æ¶ˆæ¯åˆ—è¡¨çš„Tokenä½¿ç”¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_tokens: æœ€å¤§Tokenæ•°

        Returns:
            ä¼˜åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        total_tokens = sum(self.count_tokens(msg["content"]) for msg in messages)

        if total_tokens <= max_tokens:
            return messages

        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        system_messages = [msg for msg in messages if msg["role"] == "system"]
        user_messages = [msg for msg in messages if msg["role"] == "user"]
        last_user_message = user_messages[-1] if user_messages else None

        # è®¡ç®—å‰©ä½™Tokenæ•°
        reserved_tokens = (
            sum(self.count_tokens(msg["content"]) for msg in system_messages) +
            (self.count_tokens(last_user_message["content"]) if last_user_message else 0)
        )

        remaining_tokens = max_tokens - reserved_tokens

        # æˆªæ–­ä¸­é—´æ¶ˆæ¯
        optimized = system_messages.copy()
        if len(user_messages) > 1:
            for msg in user_messages[:-1]:
                msg_tokens = self.count_tokens(msg["content"])
                if msg_tokens <= remaining_tokens:
                    optimized.append(msg)
                    remaining_tokens -= msg_tokens
                else:
                    # æˆªæ–­æ­¤æ¶ˆæ¯
                    truncated_content = self.truncate_to_token_limit(
                        msg["content"],
                        remaining_tokens
                    )
                    optimized.append({"role": msg["role"], "content": truncated_content})
                    break

        if last_user_message:
            optimized.append(last_user_message)

        return optimized
```

### 2. ç¼“å­˜ç­–ç•¥

```python
# app/services/ai/cache_strategy.py
from typing import Optional, Any
from app.core.cache import cache
import hashlib
import json

class AICache:
    """AIç»“æœç¼“å­˜"""

    @staticmethod
    def generate_cache_key(
        prompt: str,
        model: str,
        temperature: float,
        **kwargs
    ) -> str:
        """ç”Ÿæˆç¼“å­˜key"""
        cache_data = {
            "prompt": prompt,
            "model": model,
            "temperature": temperature,
            **kwargs
        }

        cache_str = json.dumps(cache_data, sort_keys=True)
        return f"ai:cache:{hashlib.md5(cache_str.encode()).hexdigest()}"

    @staticmethod
    async def get_cached_result(cache_key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜ç»“æœ"""
        return await cache.get(cache_key)

    @staticmethod
    async def set_cached_result(
        cache_key: str,
        result: Any,
        ttl: int = 3600
    ) -> None:
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        await cache.set(cache_key, result, ttl)
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´11æœˆ14æ—¥
**æ›´æ–°æ—¶é—´**: 2025å¹´11æœˆ14æ—¥
