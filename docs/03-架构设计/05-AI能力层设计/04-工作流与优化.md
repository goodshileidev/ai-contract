---
æ–‡æ¡£ç±»å‹: æ¶æ„æ–‡æ¡£
éœ€æ±‚ç¼–å·: DOC-2025-11-001
created_at: 2025-11-29
author: claude-sonnet-4-5 (claude-sonnet-4-5-20250929)
updated_at: 2025-11-30 00:25
updater: gemini-pro
status: å·²æ‰¹å‡†
---

# AIæ ‡ä¹¦æ™ºèƒ½åˆ›ä½œå¹³å° - å·¥ä½œæµä¸ä¼˜åŒ–

> **å·¥ä½œæµæ¡†æ¶**: LangGraph (å¤æ‚å·¥ä½œæµ) + Celery (å¼‚æ­¥ä»»åŠ¡)
> **ä¼˜åŒ–ç­–ç•¥**: Tokenä¼˜åŒ–ã€ç»“æœç¼“å­˜ã€æˆæœ¬æ§åˆ¶

## ğŸ“‹ æ–‡æ¡£å¯¼èˆª

æœ¬æ–‡æ¡£æ˜¯AIèƒ½åŠ›å±‚è®¾è®¡çš„ä¸€éƒ¨åˆ†,å…¶ä»–ç›¸å…³æ–‡æ¡£:

1. [00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md](./00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md) - AIèƒ½åŠ›çŸ©é˜µå’ŒæŠ€æœ¯æ ˆæ¦‚è§ˆ
2. [01-LLMæœåŠ¡æ¶æ„.md](./01-LLMæœåŠ¡æ¶æ„.md) - LLMå®¢æˆ·ç«¯ã€Promptç®¡ç†ã€ä»»åŠ¡é˜Ÿåˆ—
3. [02-å‘é‡æ£€ç´¢æœåŠ¡.md](./02-å‘é‡æ£€ç´¢æœåŠ¡.md) - å‘é‡åµŒå…¥æœåŠ¡ã€Elasticsearchã€Pineconeå­˜å‚¨
4. [03-çŸ¥è¯†å›¾è°±æœåŠ¡.md](./03-çŸ¥è¯†å›¾è°±æœåŠ¡.md) - çŸ¥è¯†å›¾è°±ç®¡ç†ã€æ™ºèƒ½åŒ¹é…å¼•æ“
5. **04-å·¥ä½œæµä¸ä¼˜åŒ–.md**(æœ¬æ–‡æ¡£)- å·¥ä½œæµç¼–æ’ã€æˆæœ¬ä¼˜åŒ–ã€æ€§èƒ½ä¼˜åŒ–

## ğŸ”„ å·¥ä½œæµç¼–æ’

### 1. LangGraphå·¥ä½œæµ

```python
# app/services/ai/workflow_engine.py
from typing import Dict, Any, List
from langgraph.graph import StateGraph, END
from langchain.schema import HumanMessage, AIMessage

class BidDocumentWorkflow:
    """æ ‡ä¹¦åˆ›ä½œå·¥ä½œæµ"""

    def __init__(self):
        self.llm_client = LLMClient()
        self.matching_engine = MatchingEngine()

    def create_workflow(self) -> StateGraph:
        """åˆ›å»ºæ ‡ä¹¦åˆ›ä½œå·¥ä½œæµ"""

        # å®šä¹‰çŠ¶æ€
        workflow = StateGraph()

        # å®šä¹‰èŠ‚ç‚¹
        workflow.add_node("analyze_requirements", self.analyze_requirements)
        workflow.add_node("match_capabilities", self.match_capabilities)
        workflow.add_node("select_cases", self.select_cases)
        workflow.add_node("generate_technical_solution", self.generate_technical_solution)
        workflow.add_node("generate_commercial_solution", self.generate_commercial_solution)
        workflow.add_node("review_quality", self.review_quality)

        # å®šä¹‰è¾¹
        workflow.add_edge("analyze_requirements", "match_capabilities")
        workflow.add_edge("match_capabilities", "select_cases")
        workflow.add_edge("select_cases", "generate_technical_solution")
        workflow.add_edge("generate_technical_solution", "generate_commercial_solution")
        workflow.add_edge("generate_commercial_solution", "review_quality")
        workflow.add_edge("review_quality", END)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("analyze_requirements")

        return workflow.compile()

    async def analyze_requirements(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æéœ€æ±‚"""
        bidding_document = state["bidding_document"]

        # è°ƒç”¨LLMåˆ†æéœ€æ±‚
        prompt_manager = PromptManager()
        prompt = await prompt_manager.get_prompt(
            code="requirement_analysis",
            variables={"bidding_document_content": bidding_document}
        )

        response = await self.llm_client.chat(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯æ‹›æŠ•æ ‡éœ€æ±‚åˆ†æä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )

        state["requirements"] = self._parse_requirements(response["content"])
        return state

    async def match_capabilities(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """åŒ¹é…èƒ½åŠ›"""
        requirements = state["requirements"]
        organization_id = state["organization_id"]

        # è°ƒç”¨åŒ¹é…å¼•æ“
        match_results = await self.matching_engine.match_requirements_to_capabilities(
            requirements=requirements,
            organization_id=organization_id
        )

        state["match_results"] = match_results
        return state

    async def generate_technical_solution(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆ"""
        requirements = state["requirements"]
        capabilities = state["match_results"]
        cases = state["selected_cases"]

        # ç”ŸæˆæŠ€æœ¯æ–¹æ¡ˆ
        prompt_manager = PromptManager()
        prompt = await prompt_manager.get_prompt(
            code="technical_solution_generation",
            variables={
                "requirements": requirements,
                "capabilities": capabilities,
                "cases": cases
            }
        )

        response = await self.llm_client.chat(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯æŠ€æœ¯æ–¹æ¡ˆæ’°å†™ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            model="gpt-4-turbo-preview",
            temperature=0.7,
            max_tokens=4000
        )

        state["technical_solution"] = response["content"]
        return state

    def _parse_requirements(self, content: str) -> List[Dict[str, Any]]:
        """è§£æéœ€æ±‚"""
        # å®ç°éœ€æ±‚è§£æé€»è¾‘
        pass
```

### 2. AIåŠ©æ‰‹çŸ©é˜µ

```python
# app/services/ai/assistant_matrix.py
from typing import Dict, Any, List
from enum import Enum

class AssistantType(str, Enum):
    REQUIREMENT_ANALYST = "requirement_analyst"
    SOLUTION_WRITER = "solution_writer"
    COMMERCIAL_ASSISTANT = "commercial_assistant"
    COMPLIANCE_REVIEWER = "compliance_reviewer"
    QUALITY_ASSURER = "quality_assurer"

class AIAssistantMatrix:
    """AIåŠ©æ‰‹çŸ©é˜µ"""

    def __init__(self):
        self.llm_client = LLMClient()
        self.assistants = {
            AssistantType.REQUIREMENT_ANALYST: {
                "model": "gpt-4-turbo-preview",
                "temperature": 0.2,
                "system_prompt": "ä½ æ˜¯éœ€æ±‚åˆ†æä¸“å®¶,æ“…é•¿ä»æ‹›æ ‡æ–‡ä»¶ä¸­ç²¾å‡†è¯†åˆ«éœ€æ±‚ã€‚"
            },
            AssistantType.SOLUTION_WRITER: {
                "model": "gpt-4-turbo-preview",
                "temperature": 0.7,
                "system_prompt": "ä½ æ˜¯æŠ€æœ¯æ–¹æ¡ˆæ’°å†™ä¸“å®¶,æ“…é•¿æ’°å†™ä¸“ä¸šçš„æŠ€æœ¯æ–¹æ¡ˆã€‚"
            },
            AssistantType.COMMERCIAL_ASSISTANT: {
                "model": "gpt-3.5-turbo",
                "temperature": 0.5,
                "system_prompt": "ä½ æ˜¯å•†åŠ¡æ–¹æ¡ˆä¸“å®¶,æ“…é•¿å•†åŠ¡æ¡æ¬¾å’ŒæŠ¥ä»·ç­–ç•¥ã€‚"
            },
            AssistantType.COMPLIANCE_REVIEWER: {
                "model": "gpt-4-turbo-preview",
                "temperature": 0.1,
                "system_prompt": "ä½ æ˜¯åˆè§„å®¡æ ¸ä¸“å®¶,æ“…é•¿æ£€æŸ¥æ ‡ä¹¦çš„åˆè§„æ€§ã€‚"
            },
            AssistantType.QUALITY_ASSURER: {
                "model": "gpt-4-turbo-preview",
                "temperature": 0.3,
                "system_prompt": "ä½ æ˜¯è´¨é‡ä¿è¯ä¸“å®¶,æ“…é•¿æ£€æŸ¥å†…å®¹è´¨é‡å’Œå®Œæ•´æ€§ã€‚"
            }
        }

    async def invoke_assistant(
        self,
        assistant_type: AssistantType,
        prompt: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨åŠ©æ‰‹

        Args:
            assistant_type: åŠ©æ‰‹ç±»å‹
            prompt: æç¤ºè¯
            context: ä¸Šä¸‹æ–‡

        Returns:
            åŠ©æ‰‹å“åº”
        """
        config = self.assistants[assistant_type]

        messages = [
            {"role": "system", "content": config["system_prompt"]},
        ]

        if context:
            messages.append({
                "role": "user",
                "content": f"ä¸Šä¸‹æ–‡ä¿¡æ¯:\n{json.dumps(context, ensure_ascii=False)}"
            })

        messages.append({"role": "user", "content": prompt})

        response = await self.llm_client.chat(
            messages=messages,
            model=config["model"],
            temperature=config["temperature"]
        )

        return response
```

## ğŸ“Š æˆæœ¬å’Œæ€§èƒ½ä¼˜åŒ–

### 1. Tokenä¼˜åŒ–ç­–ç•¥

```python
# app/services/ai/token_optimizer.py
from typing import List, Dict, Any
import tiktoken

class TokenOptimizer:
    """Tokenä¼˜åŒ–å™¨"""

    def __init__(self):
        self.encoding = tiktoken.get_encoding("cl100k_base")

    def count_tokens(self, text: str) -> int:
        """è®¡ç®—Tokenæ•°é‡"""
        return len(self.encoding.encode(text))

    def truncate_to_token_limit(
        self,
        text: str,
        max_tokens: int,
        preserve_end: bool = False
    ) -> str:
        """
        æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®šTokenæ•°

        Args:
            text: æ–‡æœ¬
            max_tokens: æœ€å¤§Tokenæ•°
            preserve_end: æ˜¯å¦ä¿ç•™æœ«å°¾

        Returns:
            æˆªæ–­åçš„æ–‡æœ¬
        """
        tokens = self.encoding.encode(text)

        if len(tokens) <= max_tokens:
            return text

        if preserve_end:
            tokens = tokens[-max_tokens:]
        else:
            tokens = tokens[:max_tokens]

        return self.encoding.decode(tokens)

    def optimize_messages(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 4000
    ) -> List[Dict[str, str]]:
        """
        ä¼˜åŒ–æ¶ˆæ¯åˆ—è¡¨çš„Tokenä½¿ç”¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            max_tokens: æœ€å¤§Tokenæ•°

        Returns:
            ä¼˜åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        total_tokens = sum(self.count_tokens(msg["content"]) for msg in messages)

        if total_tokens <= max_tokens:
            return messages

        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        system_messages = [msg for msg in messages if msg["role"] == "system"]
        user_messages = [msg for msg in messages if msg["role"] == "user"]
        last_user_message = user_messages[-1] if user_messages else None

        # è®¡ç®—å‰©ä½™Tokenæ•°
        reserved_tokens = (
            sum(self.count_tokens(msg["content"]) for msg in system_messages) +
            (self.count_tokens(last_user_message["content"]) if last_user_message else 0)
        )

        remaining_tokens = max_tokens - reserved_tokens

        # æˆªæ–­ä¸­é—´æ¶ˆæ¯
        optimized = system_messages.copy()
        if len(user_messages) > 1:
            for msg in user_messages[:-1]:
                msg_tokens = self.count_tokens(msg["content"])
                if msg_tokens <= remaining_tokens:
                    optimized.append(msg)
                    remaining_tokens -= msg_tokens
                else:
                    # æˆªæ–­æ­¤æ¶ˆæ¯
                    truncated_content = self.truncate_to_token_limit(
                        msg["content"],
                        remaining_tokens
                    )
                    optimized.append({"role": msg["role"], "content": truncated_content})
                    break

        if last_user_message:
            optimized.append(last_user_message)

        return optimized
```

### 2. ç¼“å­˜ç­–ç•¥

```python
# app/services/ai/cache_strategy.py
from typing import Optional, Any
from app.core.cache import cache
import hashlib
import json

class AICache:
    """AIç»“æœç¼“å­˜"""

    @staticmethod
    def generate_cache_key(
        prompt: str,
        model: str,
        temperature: float,
        **kwargs
    ) -> str:
        """ç”Ÿæˆç¼“å­˜key"""
        cache_data = {
            "prompt": prompt,
            "model": model,
            "temperature": temperature,
            **kwargs
        }

        cache_str = json.dumps(cache_data, sort_keys=True)
        return f"ai:cache:{hashlib.md5(cache_str.encode()).hexdigest()}"

    @staticmethod
    async def get_cached_result(cache_key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜ç»“æœ"""
        return await cache.get(cache_key)

    @staticmethod
    async def set_cached_result(
        cache_key: str,
        result: Any,
        ttl: int = 3600
    ) -> None:
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        await cache.set(cache_key, result, ttl)
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **AIèƒ½åŠ›å±‚æ€»è§ˆ**: [00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md](./00-AIèƒ½åŠ›å±‚æ€»è§ˆ.md)
- **LLMæœåŠ¡æ¶æ„**: [01-LLMæœåŠ¡æ¶æ„.md](./01-LLMæœåŠ¡æ¶æ„.md)
- **å‘é‡æ£€ç´¢æœåŠ¡**: [02-å‘é‡æ£€ç´¢æœåŠ¡.md](./02-å‘é‡æ£€ç´¢æœåŠ¡.md)
- **çŸ¥è¯†å›¾è°±æœåŠ¡**: [03-çŸ¥è¯†å›¾è°±æœåŠ¡.md](./03-çŸ¥è¯†å›¾è°±æœåŠ¡.md)

---

## ä¿®æ”¹å†å²

| æ—¥æœŸ | ç‰ˆæœ¬ | ä¿®æ”¹è€… | ä¿®æ”¹å†…å®¹æ¦‚è¦ |
|------|------|--------|-------------|
| 2025-11-30 00:25 | 1.1 | gemini-pro | YAMLå¤´éƒ¨æ—¶é—´æˆ³æ›´æ–°ã€‚ |
| 2025-11-29 | 1.0 | claude-sonnet-4-5 (claude-sonnet-4-5-20250929) | ä»05-AIèƒ½åŠ›å±‚è®¾è®¡.mdæ‹†åˆ†åˆ›å»ºå·¥ä½œæµä¸ä¼˜åŒ–æ–‡æ¡£ |

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025å¹´11æœˆ29æ—¥
**æ–‡æ¡£çŠ¶æ€**: âœ… å·²æ‰¹å‡†
